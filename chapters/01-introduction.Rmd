# Introduction: Understanding identity data matching in transnational security infrastructures {#ch-intro}

\chaptermark{Introduction}

---

> “The message is that there are no “knowns.” There are things we know that we know. There are known unknowns. That is to say there are things that we now know we don't know. But there are also unknown unknowns. There are things we don't know we don't know. So when we do the best we can and we pull all this information together, and we then say well that's basically what we see as the situation, that is really only the known knowns and the known unknowns. And each year, we discover a few more of those unknown unknowns.” [@rumsfeldPressConferenceFormer2002]

---

## Information gaps and blind spots in identification

Former US Secretary of Defense Donald Rumsfeld made the above-quoted observation in a 2002 press conference, which has since captivated academic and non-academic audiences alike. In his observation, he distinguished between what he called "known knowns" (i.e., facts that authorities are confident they have) and "known unknowns" (i.e., facts that authorities are aware they do not yet have). But he also pointed out that there are "unknown unknowns" or blind spots that authorities don't know about and don't even realize they don't know. In the context of this chapter, Rumsfeld's idea of "known unknowns" and "unknown unknowns" offers an insightful lens through which to introduce the challenges of identifying people in the context of border security and migration control. Identifying and tracking individuals across national borders is not always straightforward, as data is only sometimes complete or readily available. This raises the question of how authorities can effectually perform this task despite the presence of incomplete data sets, aliases, or even false identities.

Rumsfeld's concept of "known unknowns" can be applied to situations where an individual's data is present in a particular database but is not directly linked to their identity data in other systems. These "known unknowns" or "blind spots" can hinder the ability of authorities to identify people fully due to legal, organizational, or technical challenges. One such example of a "known unknown" in identifying people at borders could be an individual with multiple identity data in different databases or systems that have not been linked. For instance, international watch lists contain information on individuals suspected or known to be involved in criminal activities. Individuals on these lists are often listed with multiple known aliases, making it challenging to link all of their identity data together to establish that they may be the same person. To put it differently, authorities already recognize these individuals, but there remain uncertainties regarding their identification. As such, the ambiguity of personal identity data creates a "known unknown" regarding individuals' identities, which can have implications for security and law enforcement purposes.

Similarly, "unknown unknowns" can apply to information that is not only unknown but also unidentifiable through traditional means. In these cases, technology can play a role in detecting — or should we say enacting? — such blind spots by enabling the analysis and correlation of large amounts of identity data. For instance, advanced algorithms and machine learning techniques can detect previously unknown connections and patterns in the data. In their book "Algorithmic Reason", @aradauAlgorithmicReasonNew2022 offers an intriguing case in which two journalists were potentially flagged as persons of interest by a United States security agency using algorithms that detected anomalies in regular data patterns. In the previous case of "known unknowns", individuals may be placed on a watchlist, and it is known that these persons may use different names or identity documents. On the other hand, "unknown unknowns" involve entirely unknown connections and patterns that cannot be readily identified. By connecting various identities and other data and finding patterns, it becomes possible to identify or re-identify someone who is not yet known or of interest to authorities, thus potentially uncovering "unknown unknowns".

The problem of identifying and connecting identity data is not new and has been a challenge for many domains. However, in recent decades, the collection, storage, and analysis of data have been significantly impacted by technological advancements, resulting in vast amounts of personal information being processed [e.g., @borgmanBigDataLittle2015; @kitchinDataRevolutionBig2014]. Hence, one critical development in this context has been the emergence of technology designed to identify and link data from different sources [e.g., @harronChallengesAdministrativeData2017]. Such technology has wide applications in various fields, including statistics, marketing, fraud detection, and law enforcement. Thus, it is reasonable to assume that as the collection of personal data continues to increase is concomitant with a need to connect these data points within data infrastructures. As a result, it is necessary to begin by recalling the history and applications of matching and linking data and how it has been applied in the specific context of identity data in border security and migration management.

## Connecting the dots: The development of data matching techniques

The use of computing technology to connect personal identity data has a long past that dates back to the early days of punch card technology, even before the existence of database technology, as evident from research such as @dunnRecordLinkage1946's "Record linkage" and @newcombeAutomaticLinkageVital1959's "Automatic linkage of vital records". The term "record linkage" is often used in areas such as public health, epidemiology, and demography to identify and link records that refer to the same real-world person across different datasets. The process of linking data can be beneficial to states in terms of delivering better services to their citizens and facilitating research, such as in the areas of public health [e.g., @jutteAdministrativeRecordLinkage2011] and demographics [e.g., @abbottLargescaleLinkageTotal2015]. Record linkage typically involves matching records based on common attributes such as name, address, date of birth, or social security number to create a comprehensive view of an individual across multiple datasets. Yet, since its inception, record linkage has had to address the challenges posed by fragmented, incomplete, and duplicated information across multiple sources [@newcombeRecordLinkageMaking1962].

The emergence of electronic computers and database technology enabled more sophisticated matching algorithms to be developed, leading to increased adoption in other fields [@batiniObjectIdentification2016; @christenDataMatchingConcepts2012]. As a result, the process of matching data sets and linking records is now referred to by various names depending on the field and context, such as data matching, data linking, data merging, data integration, record linkage, deduplication, or entity resolution, depending on the specific context and application [@christenDataMatchingConcepts2012]. This dissertation will use the term data matching as it is a more general term referring to identifying records in data sets that refer to the same real-world persons (or other entities) and reconciling duplicates or inconsistencies between data sets.

Over the years, numerous data matching methods and techniques for classifying matches have been devised [@batiniObjectIdentification2016; @christenDataMatchingConcepts2012; @fellegiTheoryRecordLinkage1969; @winklerMatchingRecordLinkage2014]. The following standard data matching methods can be distinguished based on this literature. One of the most basic techniques for identifying matching records is deterministic matching, which employs predefined rules or criteria. An example rule could be that if two records have the same first name, last name, and date of birth, they are considered a match. Another approach is probabilistic matching which uses statistical algorithms to calculate the probability that two records are a match based on the similarity of their categories of data. If, as in the previous example, two data records have similar but not identical names or dates of birth, the records may still be considered a match based on the probability calculation. Another approach is rule-based matching, which can combine deterministic and probabilistic methods to find matches and incorporates expert knowledge or domain-specific rules to increase accuracy. Last but not least, matching techniques based on machine learning are gaining popularity. Such methods employ algorithms that can learn from data to improve accuracy and more easily adapt to new data sources.

While data matching may seem like a technical process, its increasing use and impact on society and individuals mean that it has significant consequences that should not be overlooked. With the growth of the internet and the digitalization of many aspects of modern life, data matching has become even more prevalent, with a wide range of actors using these techniques to link data from different sources and gain insights into individuals, their behaviour, and preferences [@clarkeHumanIdentificationInformation1994; @gandySurveillanceSocietyInformation1989; @zuboffBigOtherSurveillance2015]. Furthermore, using data matching algorithms in automated systems can introduce errors and biases, making some people disproportionately the target of surveillance and control [examples: @benjaminRaceTechnologyAbolitionist2019; @eubanksAutomatingInequalityHow2018]. For instance, the German-Lebanese citizen Khalid al-Masri was imprisoned and tortured by the CIA in 2003 after being mistakenly identified as a suspected terrorist with a similar name. Data matching technologies play a crucial role in these processes by allowing for the analysis and correlation of vast amounts of data and determining previously unknown connections in identity data.

Identity data matching is a critical component of migration and border control systems worldwide, and data matching errors can lead to severe consequences. Yet, there still needs to be a greater understanding of how identity data matching systems are developed and how they work in practice. Many factors can contribute to this lack of knowledge, including a lack of transparency in developing such systems, limited access to information on their design and operation, and the complexity of the underlying technical processes. Therefore, further research into the design and operation of identity data matching systems must address this knowledge gap. With this in mind, the following section will highlight the significance of examining the actors involved in developing data matching technology as we explore a contemporary example of data matching in migration and border control within the European Union.

## Leveraging data matching for border control

> “In light of the recent terrorist attacks in Europe and the increase in irregular migration in recent years, action needs to be taken to address this risk of information gaps and blind spots. The measures in this proposal [Interoperability of EU information systems for security, border and migration management] will ensure the various systems can exchange data and share information so that authorized bodies and officers have the information they need to strengthen our borders and better protect Europe.” (European Commission, 2017, MEMO_17_5241)

---

> ”Establishing a common repository of data would overcome the current fragmentation in the EU’s architecture of data management for border control and security. This fragmentation is contrary to the data minimization principle, as it results in the same data being stored several times. Where necessary, the common repository would allow for the recognition of connections and provide an overall picture by combining individual data elements stored in different information systems. It would thus address the current knowledge gaps and shed light on blind spots for border guards and police officers.“ [@europeancommissionCommunicationCommissionEuropean2016, p. 18]

---

> “[One of] the four technical components of the proposal [is] a multiple identity detector — this will verify whether the biographical data that is being searched exists in multiple systems, helping to detect multiple identities. It has the dual purpose of ensuring the correct identification of bona fide persons and combating identity fraud.” [@ecFrequentlyAskedQuestions2017]

---

The quotes mentioned above refer to a project linking identity data of EU information systems for security, border, and migration management. Presently, each of these EU information systems operates independently from its database and serves a distinct purpose, such as managing asylum requests, processing visa applications, or supporting law enforcement activities. The proposal explicitly identifies a potential risk of information gaps and blind spots because data is not connected. It proposes to address this by connecting and sharing information from those multiple systems. Furthermore, the EC communication stresses the importance of having the information required to strengthen borders and identify potential threats.

The second quote describes the need for "establishing a common repository of data", which would "address the current knowledge gaps and shed light on blind spots for border guards and police officers" (p.18). Finally, the third quote describes a component for finding multiple identities that refer to the same person. Fragmentation in the EU's architecture of data management for border control and security is thus portrayed as causing duplicate data storage and leaving border guards and police officers with knowledge gaps and blind spots. According to this logic, the common repository and multiple identity detector components would enable the recognition of connections and provide an overall picture by combining individual data elements stored in different information systems.

The new components in implementing the EU interoperability project are just one example of how the importance of data matching technologies has only increased. The project aims to improve the efficiency and accuracy of EU information systems (including Eurodac, SIS, and VIS) by adding new components that enable the matching of biometric data, visa data, and other identity-related information. However, the use of these new components is not just limited to improving the functioning of these systems. They will also be pivotal in putting into practice how to identify individuals who are deemed suspicious based on the links between data sets based on probabilistic, rule-based, or machine learning-based data matching. It is, therefore, crucial to consider how efforts to close information gaps and blind spots in transnational data infrastructure functioning and design may significantly impact identification processes. Furthermore, the European Commission is not building these systems by itself but increasingly relies on global information technology suppliers and integrators [@lemberg-pedersenPoliticalEconomyEntry2020; @valdiviaNeitherOpaqueTransparent2022].

A research gap exists in understanding how data matching technologies, which are increasingly developed by commercial entities for global use [e.g., @leeseStandardizingSecurityBusiness2018; @lemberg-pedersenPoliticalEconomyEntry2020; @valdiviaNeitherOpaqueTransparent2022; @zureikGovernanceSecurityTechnology2004], operate and influence processes of identification. The international and commercial dimensions of identification technology mean that there is a growing need to examine how the private sector is involved in developing and implementing these standardized technologies. As @pollockSoftwareOrganisationsBiography2009 have noted, STS researchers tend to be critical of standardized software and concentrate on how poorly such software adapts to different settings. However, the widespread use of standardized identification software emphasizes how crucial it is to comprehend how such software is created to operate in various contexts and its broader implications. Therefore, it is important to consider how the technology's functioning and design impact data matching and identification processes to enable the identification and tracking of individuals across different systems and jurisdictions.

The use of data matching technologies in transnational security infrastructures has become a powerful tool for identifying and tracking individuals across different systems and jurisdictions. However, the development and commercialization of these technologies pose a challenge in understanding how they work and how they impact identification processes. In light of this challenge, the following section will outline the research problem, aims, and objectives of this study, which seek to unpack the complexities of analyzing data matching in transnational security infrastructure.

## Unpacking the challenge of analyzing data matching in transnational security infrastructure

## Research problem, aims, and objectives

To address the problem of analyzing data matching in transnational security infrastructure, this research aims to unpack the complex dynamics surrounding the development, deployment, and impact of data matching technologies. The study aims to comprehensively investigate the different dimensions of identity data matching within transnational security infrastructures, using various theoretical and methodological approaches to provide a nuanced understanding of this complex phenomenon. In particular, the dissertation will look into how infrastructure development, security, and internationalization in identification are all intertwined with identity data matching. To accomplish this task, the research needs to contribute to developing effective methodological strategies for analyzing data matching in transnational infrastructures. To achieve these aims, the study identifies the following objectives:

* To map the theoretical landscape related to internationalization, securitization, and infrastructuring of identification and apply this understanding to data matching in transnational security infrastructures (Chapter 2).
* To develop a conceptual framework for analyzing data matching in transnational infrastructures using "infrastructural inversion" as a methodological strategy (Chapter 3).
* To introduce a new method and software tool to analyze data models underpinning information systems in population management (Chapter 4).
* To examine the relationship between identity data matching technologies and routine identification practices (Chapter 5).
* To investigate the long-term development of identification systems and building of transnational security infrastructures by identifying crucial moments in their lifecycle to explore how data matching expertise travels and circulates (Chapter 6).

A mixed-methods approach, including case studies, interviews, and document analysis, will be employed to achieve these objectives. Different "infrastructural inversions" will focus on specific aspects of data matching. In addition, a novel methodology will be used to analyze the data models that underlie information systems and the relationships between identity data stored by various authorities. The study uses fieldwork data collected from a software company that develops data matching technology to analyze how their technology is used in the EU and member state systems of border control and migration management. Specifically, the study will explore how routine identification is intertwined with data matching technology and how the software package has evolved to be used in identity and security settings. Through the research objectives outlined above, this dissertation seeks to comprehensively analyze the challenges and opportunities presented by data matching in transnational contexts. The following section provides an overview of the dissertation's structure, outlining the chapters and their focus.

## Structure of the dissertation

This dissertation is organized progressively, beginning with a mapping of relevant theoretical concepts, then developing a conceptual framework that will guide the analysis, and followed with more empirical chapters. Chapter 2 starts by mapping theoretical concepts on identification and matching identity data, drawing on literature related to the internationalization and commercialization of identification, the securitization of identification, and the infrastructuring of identification. This chapter lays the groundwork for the subsequent chapters by discussing various theoretical perspectives on matching identity data and the implications of matching identity data for transnational security infrastructures.

Chapter 3 introduces the conceptual framework for analyzing data matching in transnational infrastructures. The framework uses three methodological strategies, "infrastructural inversions," to direct attention to specific facets of data matching. These three strategies are based on comparing data models, analyzing data practices and tracing sociotechnical change. Comparing data models can reveal information collected by various organizations and systems; data practices can show the searching and matching of identity data within and across organizations; sociotechnical change can shed light on the circulation of data matching knowledge, technologies and practices over time and across organizations. The chapter explains how these strategies were applied to the dissertation's fieldwork at a software company developing data matching technology. The chapter also describes the methods of data collection and the techniques of data analysis used in the dissertation.

Chapter 4 introduces the "Ontology Explorer" (OE) methodology, a semantic approach and an open-source tool to analyze the data models underlying information systems. The method is specifically designed to compare data models in different formats used by various systems. This chapter explains how it is applied in the dissertation to reveal less visible assumptions and patterns in information systems. Unlike other methods, the OE allows for the systematic comparison of non-homogeneous data formats and enables comparisons of data models across diverse information systems. Therefore, the OE makes it possible to observe how identity data properties influence the creation and circulation of data and the relations between different authorities' data models.

Chapter 5 examines the relationship between technologies for searching and matching identity data and routine bureaucratic identification practices in migration management. The chapter focuses on how a government migration agency searches and matches client data using a specific data matching software package. The chapter introduces the concept of "re-identification" to refer to the process by which clients of bureaucratic procedures are re-identified in data infrastructures at various points in those procedures. The chapter demonstrates the implications of data matching for designing and using information systems in bureaucratic settings in two ways. First, the chapter shifts the usual focus from first registration to re-identification practices across data infrastructures. Second, the chapter's findings demonstrate how the software mediates (re-)identification practices and redistributes competencies through the software's affordances to mitigate data uncertainties.

Chapter 6 looks at the long-term development of identification systems and infrastructures. The chapter proposes two heuristics for selecting crucial moments in the lifecycle of identification technologies. First, it demonstrates how a software package's changing interpretive flexibility allows us to see actors' varying problematizations of identification, such as those related to the securitization of identification. Second, the chapter demonstrates how "gateway moments" make it possible to see the compromises necessary when building identification infrastructures and adapting globally honed technologies to new settings. Together, these chapter's findings shed light on the activities of under-the-radar actors, such as commercial software vendors, whose distribution and reuse of software packages have long-term implications on identification practices and infrastructures in various contexts.

The dissertation concludes in Chapter 7 with a summary of empirical findings, literature contributions, and reflections on the research process. The study contributes to the literature by mapping the theoretical landscape of identity data matching, introducing a conceptual framework for analyzing data matching in transnational infrastructures, proposing new methods for analyzing data matching and using these to examine the relationship between data matching technologies and bureaucratic practices. It also explores the long-term development of identification systems and infrastructures. Finally, the chapter acknowledges the study's limitations and suggests areas for future research. Overall, the dissertation contends that matching identity data is a complex and multifaceted phenomenon that requires a nuanced and interdisciplinary approach to understand how it shapes and is shaped by transnational security infrastructures.
