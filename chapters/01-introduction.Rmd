# Introduction {#ch-intro}

\chaptermark{Introduction}

---

> ‘Establishing a common repository of data would overcome the current fragmentation in the EU’s architecture of data management for border control and security. This fragmentation is contrary to the data minimisation principle, as it results in the same data being stored several times. Where necessary, the common repository would allow for the recognition of connections and provide an overall picture by combining individual data elements stored in different information systems. It would thus address the current knowledge gaps and shed light on _blind spots_ for border guards and police officers.’ [@europeancommissionCommunicationCommissionEuropean2016, p. 18, emphasis mine]

In April 2016 the European Commission (EC) announced a new strategy for the EU information systems for security, border and migration control with the communication titled ‘Stronger and Smarter Information Systems for Borders and Security‘ [@europeancommissionCommunicationCommissionEuropean2016]. In this proposal, the EC Directorate-General for Migration and Home Affairs provided a new vision that aspires to improve the EU’s information systems to make better use of data that is already available so that relevant authorities can work more effective and efficient. One way the report envisions the objectives of this proposal to be achieved is through ’establishing a common repository of data’ which eventually would ‘address the current knowledge gaps and shed light on blind spots for border guards and police officers‘ (p.18). My interest in this area developed while I was reading this communication and the subsequent proposals, feasibility studies, and other reports on interoperability for the EU information systems of Justice and Home Affairs. Early on in my research, I decided to delve deeper into these aspects of linking personal data and related problems of data quality. For this reason, I decided to title this dissertation ‘finding blind spots’, to capture this alluring and contradictory proposition of a visual metaphor of finding something that one knows is there but cannot see.

Technologies and digital infrastructures for making people at the border visible have already been a frequent concern of studies on migration and border control [see e.g. @tazzioliSightMigrationGovernmentality2016; @dijstelbloemSurveillanceSeaTransactional2017; @follisVisionTransterritoryBorders2017; @plajasKnowingRomaVisual2019]. On account of the fact that these technologies aim to make such phenomena visible, they are frequently understood as ’technologies of vision’. This definition draws on the work by Donna Haraway [-@harawaySituatedKnowledgesScience1988], according to whom vision is always particular, partial, embodied. Situated vision at the border is consequently not passive, but actively constitutive of the meanings given to border crossers’ identities. In the same way, technologies proposed to address the blind spots, such as through the use of technologies to find matching identities, will not be passive but will actively shape the meanings given to security, borders, migration.

On one hand, studies have investigated how the situated vision of technologies deployed at the border are constitutive of the meanings the phenomena they aim to represent. The imageries of maps, flows of people and border crossings that underpin the infrastructures for monitoring and controlling do not just make these phenomena visible, but are in fact co-constitutive of what borders and border crossings mean [@vanreekumDrawingLinesEnacting2017]. Moreover, these visions shape the immediate and future responses of authorities [@tazzioliSpyTrackArchive2018]. In the Eurosur and Jura systems analysed by Tazzioli, for example, the systems do not just monitor and detect migrant movement; collected data are also used to prevent future movements along those routes. Technologies deployed at the border can then enact migrants in different ways: as people that need that to be stopped or as people to be saved in order to prevent new disasters in the Mediterranean.

On the other hand, literature on the securitization of border control showed that technologies of vision are also constitutive of the meanings of borders and border control. For example, Bigo [-@bigoSecuritizationPracticesThree2014] pieced together three kinds of meanings actors gave to the border practices of border control: as ‘solid’, something to be defended and linked to sovereignty; as ‘liquid’, relating to how borders can be controlled through filtering and identifying populations; as ‘cloudy’, only visible through the use of digital databases and analytics. In the three meanings described by Bigo, the practices and uses of visual technologies by the actors, in turn, informed the meanings they attribute to border control. So on the one hand, new technologies of vision ‘alter the nature of national borders’ [@follisVisionTransterritoryBorders2017]. On the other, meanings attributed to borders and border control shape the kinds of technologies that are developed and employed. A look at developments of the EU technical systems can further show how the technologies of vision for border, migration control, and security are shaping and have been shaped by their social and technical contexts.

## Datafication and dataveillance in EU systems

> ‘We need to know who is crossing our borders... By November, we will propose a European travel information system --- an automated system to determine who will be allowed to travel to Europe. This way we will know who is travelling to Europe before they even get here.‘ [President of the European Commission J.C. @junckerStateUnion20162016]

Over the past decades, European states have aimed to define and control their wanted and unwanted migration through various means[@balchDevelopmentEUMigration2011; @broedersNewDigitalBorders2007]. @balchDevelopmentEUMigration2011 have traced some of these important developments of EU policy for a migration and asylum system and build up for technical system. From their analysis, they argue that it is difficult to speak of a common EU policy in this area. Rather, the evolving institutional setting has been decisive in shaping the kind of migration and asylum policies. They note that, generally, it has been easier for EU Member States to cooperate on reducing irregular migration than on creating a framework for legal migration. According to the authors, this trend can also be seen in the EU agencies and information systems that have been designed to fight cross-border crime and irregular migration. As the authors write, there has been a ’political appetite for technological solutions to the perceived problems of irregular migration at the EU level’ (p. 28). A major focus of European authorities has therefore been to identify irregular migration through the use of new technological solutions such as databases and linking data.

Different authors have analysed these developments of new information technology as a sort of ‘datafication’ of migration and border management [@dijstelbloemMigrationNewTechnological2011; @broedersDataficationMobilityMigration2016; @leursDataficationDiscrimination2017; @bestersGreedyInformationTechnology2010]. The creation of databases in Europe to store data about people and their movements in this view are seen as an important element for a kind of ‘social sorting’ of populations for different treatments [@lyonSurveillanceSecuritySocial2007]. Consequently, surveillance of borders and the people who cross them has thus been analysed through mechanisms of data collection and processing, frequently described under the portmanteau ‘dataveillance’.[^surveillance] However, there risk is that such a view portrays databases as mere tools to obtain information on people and their movements.

[^surveillance]: This theorisation of surveilling populations by using database technologies is furthermore linked to work in studies on surveillance and policing of borders [@clarkeInformationTechnologyDataveillance1988; @lyonSurveillanceSocialSorting2003; @zureikGlobalSurveillancePolicing2005].

Theories of ‘datafication’ and ‘dataveillance’ have been productive to understand the data collection practices and technologies for border control. The database technologies and concepts however often assume that there exists such a priori categories of persons such as irregular migrant that can be identified and sorted for different treatment. As others have highlighted [@potzschEmergenceIBorderBordering2015; @pelizzaProcessingAlterityEnacting2019], such approaches do not take into account how such practices are constitutive of types of subjectivities they purport to process:

> ‘[D]istinctions between citizen and migrant or between trusted traveller and terrorist threat do not emerge as a priori givens that necessitate certain practices, procedures, and regimes of profiling, tracking, sorting, and filtering, but the division itself becomes conceivable as the contingent result of these formalized operations’[@potzschEmergenceIBorderBordering2015, p. 103]

The technologies and practices of matching and linking identities is one important aspect in constituting these categories. For instance, it may only be through the process of detecting multiple similar identities that someone becomes an irregular migrant or risky traveller. It is thus crucial to examine the latest technological developments related to the matching of identity data in the EU systems.

## EU systems and interoperability

Currently, there are three EU information systems for security, migration and border management in operation: the Schengen Information (SIS), Eurodac, and the Visa Information System (VIS). Even though they all capture some data about non-EU citizens, there are still individual purposes for which the systems exist. First, SIS supports external border control and law enforcement cooperation in the European Union. It supports this task by storing alerts which contain information on persons and objects, and what that do when they are found. Second, Eurodac is concerned with identifying asylum seekers and determining the responsibility between member states for processing asylum applications. For doing this identification it stores fingerprint data of asylum seekers and third-country nationals. Third, the VIS allows for exchange of visa information data (including personal data and biometrics), and this way aims to maintain a common EU visa policy.  

A high-level architectural overview of how these current systems are structured is useful to understand how the systems generally work. The VIS for example consists of a central system with a database containing biographic and biometric data. Each member state has its own National Interface (NI) that they connect to the central system through a formal interface that is specified in an Interface Control Document (ICD). An interface in this way defines how the data flows between the systems. For example, by defining what types of data needs to be sent to do a lookup of a person for their visa data. This type of architecture is approximately used for each different information systems. The picture becomes more complicated when we also take into account the differences between how a member states systems work and what data they capture. The national systems might be very different for the different members states, but each has to send data to the central system through the interface. Many of the difficulties with data matching and data quality are therefore related to the tensions between these national and central systems. Hence, the proposed interoperability framework aims to make all three information systems work together.

The communication from the European Commission [@europeancommissionCommunicationCommissionEuropean2016] put the topic of interoperability firmly on the agenda as a proposal for improving border and migration management and for internal security inside the Union. Later in June 2016 a ’high-level expert group on information systems and interoperability’ (HLEG) was set up by the Commission[^hleg-setup] with a mission to help with ‘developing a joint strategy to make data management in the Union more effective and efficient, in full respect of data protection requirements, to better protect its external borders and to enhance its internal security’.[^hleg-meeting] In May 2017, this HLEG concluded with a publication of their final report that includes recommendations for changes to the information systems and achieving interoperability between them.[^hleg-report] These recommendations were further taken up by the Commission who began work on a proposal for a legislation on interoperability.[^interop-proposal]. In 2019, After a long back and forth between the European Commission, Council, and Parliament a political consensus on the regulations establishing a framework for interoperability between EU information systems.[^interop-final]

<!-- The importance of having this in regulation is emphasised by Dijstelbloem and Meijer: -->

<!-- > ‘Whether or not governments connect and combine different bodies of information will increasingly become a matter of legal constraints, as the technological constraints are losing their relevance quickly. However, the main effect of technology will be a matter of changing methods rather than goals and intentions.’ [@dijstelbloemMigrationNewTechnological2011, p. 77]. -->

[^hleg-setup]: Commission Decision of 17 June 2016 setting up the high-level expert group on information systems and interoperability — 2016/C 257/03

[^hleg-meeting]: Full details of this HLEG with meetings notes http://ec.europa.eu/transparency/regexpert/index.cfm?do=groupDetail.groupDetail&groupID=3435

[^hleg-report]: European Commission, High-level expert group on information systems and interoperability, Final report May 2017.

[^interop-proposal]: COM(2017) 794 final, 12.12.2017

[^interop-final]: <https://www.consilium.europa.eu/en/press/press-releases/2019/05/14/interoperability-between-eu-information-systems-council-adopts-regulations/>

Essentially, the previously mentioned HLEG proposed several additional new components to the current architecture which they consider to make it possible to achieve the interoperability of these systems: a European Search Portal (ESP), a Shared Biometric Matching Service (sBMS), a Common Identity Repository (CIR), and a Multiple-Identity Detector (MID). In short, we can say that the ESP would be an intermediary between the different information systems that allows for querying off all the different information systems through a single interface, the sBMS is a technical component that would allow automated comparing of biometric data from different systems, the CIR would change the storing of data from the individual systems to a shared database, and the MID would allow for automatic detection of multiple identities with the same identity data. The CIR and MID are therefore two large-scale cases of advanced data matching for linking identities from the three databases into a common repository.

Next to these existing systems there are also new systems that have been proposed in new regulations and are at the time of writing in different states of development, or still being discussed. The forthcoming European Travel Information and Authorisation System (ETIAS) will require people who currently benefit from visa free travel to the Schengen area to pre-register and undergo additional security checks prior to their travel. Another upcoming information system, the Entry-Exit System (EES), is being developed by European agencies to register entry, exit and refusal of entry data of all third country nationals crossing the external borders of the Schengen area. All these developments to the systems show a preoccupation of European policymakers to tackle presumed invisible phenomena of border crossing at the external and internal borders of Europe. To this end, the most recurrent solutions put in place by authorities tend to make border crossers visible on information systems before they reach the border[@pelizzaIdentificationTranslationArt2021], and by collecting and connecting more data.

## Data quality and data matching

Generally speaking, the integration, interoperation, or interoperability of data is a commonly sought-after goal of organizations and governments, to ensure that data can be shared, used, processed by different parties and information systems. Achieving such data interoperability, however, is a complicated endeavour that includes both changes to technical and organizational elements [@gil-garciaCollaborativeEGovernmentImpediments2007; @kubicekInteroperabilityGovernment2011; @pelizzaDiscipliningChangeDisplacing2016; @schollEgovernmentIntegrationInteroperability2007]. Technical changes might include introducing new standards that formalize, on one hand, the formats, or syntactic structures, for exchanging data. On the other hand, semantic standards and vocabularies can guarantee that the meaning of the content of exchanged data can be interpreted by other systems. In effect, data will need to of high quality so that they are both fit for the purposes for which they are elaborated, and re-usable for new purposes and by other actors [@floridiPhilosophyInformationQuality2014].

Unfortunately, data quality often remains a poorly defined term. This shows a need to be more explicit about exactly what is meant by data quality. Often data quality is used interchangeably and without precision as accuracy. That is, how the data adheres to something in the world. However, this is just one aspect of data quality. A useful demarcation are the data quality dimensions defined by Batini et al. [@batiniDataQualityDimensions2016]. Based on their extensive literature reviews on various aspects of data quality, Batini et al. define the main dimensions as follows (p. 23):

1. Accuracy: how does the information adhere to something in the world.
2. Completeness: the extent to which it actually adhered to this real world entity.
3. Redundancy: is the information concise, minimum required.
4. Readability: how easy is it to understand the information by users.
5. Accessibility: related to the ability of the user to access information.
6. Consistency: does it comply with different constraints and rules.
7. Usefulness: what are benefits of use of information.
8. Trust: does it arrive from authoritative source that can be trusted.

Connecting multiple data sources creates additional difficulties and uncertainties related to the quality of data, as well as opportunities to resolve such complications. In general, databases are never without mistakes. For example, the database of a webshop may have duplicates records for a customer which was mistakenly created, or a customer’s address may no longer up to date because they moved. Organizations therefore need many data management activities for keeping their data current. Such data management activities include processes of detecting, correcting, and removing records that are identified as inaccurate, incomplete, or incorrect. Generally speaking, by integrating multiple data sources, new uncertainties may arise. For example, to identify if records of different databases refer to the same customer. Conversely, integrating multiple data sources can in many cases be used to complete and correct data from one source by combining and comparing it to others. In both cases the difficulty remains of identifying if two entities represented by data records from on or more sources actually refer to the same real world entities or not, and how they should be merged [@batiniObjectIdentification2016].

Data deduplication or duplicate identification is commonly used term to refer to this problem of object identification when dealing with duplicate records from a data source. Generally, such a process would compare every record with every other record to score its likelihood of matching that record. The complexity of deduplication in such cases is therefore equal to the Cartesian product of records in the database, i.e., resulting in a computational complexity of $O(n^{2})$, where $n$ is the cardinality or number of elements the data set. If data sets are too big — and this becomes computationally expensive — the search space might need to be reduced [@batiniObjectIdentification2016]. After determining the set of possible matches, a domain expert generally needs to be involved to decide on the possible matches.

## Thesis aims and objectives

## Research questions

<!-- ```{r research-questions, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "research-questions.csv")
data = read.csv(path, header = TRUE)

# block_table(data, header = TRUE, properties = pt)

knitr::kable(data, format = "latex", longtable = TRUE, booktabs = TRUE)
``` -->

1. Which types of knowledge about people-on-the-move are inscribed in data models of national and transnational security infrastructures? What does a typology of data for people-on-the-move tell us about how organizations search and match their data?
2. In which ways do actual information about people-on-the-move differ from those expected by the data models? What do such differences tell us how the organizations that record and use the data and how they deal with differences to match identities?
3. How do organizations that collect information about people-on-the-move search and match for identity data in their systems? How do such mechanisms shape expectations on the quality of the data?
4. How is data about people-on-the-move matched across different agencies and organizations? And how do such knowledge and technologies for matching identity data travel and circulate?

## Outline of the thesis

<!-- Yet the scholarship shows the tendency to primarily refer to the (in)visibility of humans only. Less attention has been given to the invisibility of those data infrastructures that allow making human mobility, migration and borders visible. This is a key aspect, though. People on the move become visible through infrastructures that are not neutral, but enact them in certain ways. Therefore, discussing the invisibility of humans cannot avoid discussing the invisibility of those data infrastructures that define how they become visible. -->
