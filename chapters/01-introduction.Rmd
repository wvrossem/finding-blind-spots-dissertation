\clearpage
\pagenumbering{arabic}

# Introduction: Understanding identity data matching in transnational security infrastructures {#ch-intro}

\chaptermark{Introduction}

> The message is that there are no “knowns.” There are things we know that we know. There are known unknowns. That is to say there are things that we now know we don’t know. But there are also unknown unknowns. There are things we don’t know we don’t know. So when we do the best we can and we pull all this information together, and we then say well that’s basically what we see as the situation, that is really only the known knowns and the known unknowns. And each year, we discover a few more of those unknown unknowns. [@rumsfeldPressConferenceFormer2002]

## Information gaps and blind spots in identification

Former US Secretary of Defense Donald Rumsfeld made this observation in a 2002 press conference, which has since captivated academic and lay audiences alike. In his observation, he distinguished between what he called “known knowns” (i.e., facts that authorities are confident they know) and “known unknowns” (i.e., facts that authorities are aware they do not yet know). However, he also pointed out that there are “unknown unknowns” or peculiar blind spots that authorities don’t know about and don’t even realize they don’t know. In the context of this thesis, Rumsfeld’s idea of “known unknowns” and “unknown unknowns” offers an insightful lens through which to introduce the challenges of identifying people in border security and migration control. Identifying and tracking individuals across national borders is rarely straightforward, as data are only sometimes complete or readily available.[^data] This raises the question of how authorities can effectively identify individuals despite incomplete data sets, aliases, or even false identities, as well as how authorities can acknowledge and address the incompleteness of information about something they are not aware of.

[^data]: The word data is often treated as a mass noun, and hence, something that cannot be counted or divided (e.g., “the data is available”). In contrast, this dissertation uses data in its countable plural noun form (“data are”). I follow the convention of using this form to highlight that data are multiple and “arise from and are used in varied circumstances worth acknowledging” [@loukissasAllDataAre2019, p. 13].

Rumsfeld’s concept of “known unknowns” can be applied to situations where an individual’s data are present in a database but is not directly linked to their identity data in other systems. These “known unknowns” or “blind spots” can hinder the ability of authorities to identify people fully because of legal, organizational, or technical challenges. One example of a “known unknown” in identifying people at borders could be an individual with multiple identity data in different databases or systems that have not been linked. For instance, international watch lists contain information on individuals suspected or known to be involved in criminal activities. Individuals on these lists are often listed with multiple known aliases to address the challenge of linking all their identity data together to establish that they may be the same person. In other words, authorities already recognize these individuals, but there remain uncertainties regarding their identification. As such, the ambiguity of personal identity data creates a “known unknown” regarding individuals’ identities, which can have implications for security and law enforcement purposes.

“Unknown unknowns” can apply to information that is not only unknown but also unidentifiable through traditional means. Technology can play a role in detecting — or should we say enacting? — such blind spots by enabling the analysis and correlation of large amounts of identity data. For instance, advanced algorithms and machine learning techniques can detect previously unknown connections and patterns in the data. In their book “Algorithmic Reason,” @aradauAlgorithmicReasonNew2022 offer an intriguing case in which two journalists were potentially flagged as persons of interest by a United States security agency using algorithms that detected anomalies in regular data patterns. In the previous case of “known unknowns,” individuals may be placed on a watchlist, and it is known that these persons may use different names or identity documents. Conversely, “unknown unknowns” involve entirely unknown connections and patterns that have yet to be discovered. By connecting various identities and other data and finding patterns, it becomes possible to identify or re-identify someone who is not yet known or of interest to authorities, thus potentially uncovering “unknown unknowns.”

The problem of identifying and connecting identity data is not new and has been a challenge for various domains of knowledge. However, in recent decades, data collection, storage, and analysis have been significantly impacted by processes of datafication, resulting in vast amounts of personal information being processed [e.g., @borgmanBigDataLittle2015; @kitchinDataRevolutionBig2014]. Hence, one critical development in this context has been the growth of _data matching technology_ to identify individuals across multiple sources [e.g., @christenDataMatchingConcepts2012; @harronChallengesAdministrativeData2017; @talburtSpecialIssueEntity2013]. By utilizing data matching technology, individuals can be identified even when the information is incomplete or inconsistent, thanks to the comparison and reconciliation of data, such as name, address, and identification numbers. As a result, data matching tools are widely deployed in fields where up-to-date information is vital, such as healthcare, finance, and law enforcement [@talburtSpecialIssueEntity2013].

In the healthcare sector, data matching technology is used to match patient data across different systems to ensure accurate patient identification and prevent medical errors [e.g., @leeNaturalisticPatientMatching2016; @mccoyMatchingIdentifiersElectronic2013; @sauleauMedicalRecordLinkage2005]. By utilizing patient information like names, birthdates, and social security numbers, medical records can be effectively matched and organized across diverse healthcare systems and databases [@zechMeasuringDegreeUnmatched2016]. In financial intelligence, data matching is used, among others, to detect fraudulent activities and for sanctions compliance. For instance, SWIFT, a worldwide provider of secure financial messaging services, employs data matching algorithms to aid financial institutions in complying with sanctions regulations by accurately identifying individuals on sanction lists who may use aliases or fraudulent identities to avoid detection [@swiftSimplifyComplexWorld2018; @swiftNameScreeningFulfil2021]. In law enforcement, data matching technology is used to analyze data and aid investigations, such as identifying individuals involved in organized crime networks by linking their biographical information across databases [@fergusonRiseBigData2017; @steinbockDataMatchingData2005]. For example, data matching techniques are employed to analyze flight passenger data to identify patterns and potential threats by linking and analyzing individuals’ travel histories across different flights and airlines [@bellanovaDifferentViewMaking2012; @hobbingTracingTerroristsEuropean2010].

The previous examples underscore how data matching technology has become crucial in linking and reconciling personal data across multiple sources, given the increasing collection of information, such as electronic medical records, financial transactions, and online purchases. The technology enables organizations to create more comprehensive profiles of individuals, contributing to detecting fraudulent activities, ensuring regulatory compliance, and dealing with the siloed nature of data sources. Nevertheless, despite the increasing use of data matching technology in various sectors, there is still a lack of understanding of how it shapes the meaning of the things it connects, including identifying data as suspicious and shaping relations between organizations whose data are being matched and connected. This research seeks to contribute to a more performative understanding of the role of data matching technology by investigating how it shapes the meaning of data, practices, and the organizations that use it. As a result, it is necessary to begin by recalling the history and applications of matching and linking data.

## Connecting the dots: The development of data matching techniques

\sectionmark{Connecting the dots}

The use of computing technology to connect personal identity data has a long past that dates to the early days of punch card technology, even predating database technology, as evident from research such as @dunnRecordLinkage1946’s “Record linkage” and @newcombeAutomaticLinkageVital1959’s “Automatic linkage of vital records.” The term “record linkage” is often used in public health, epidemiology, and demography to describe the practice of matching and linking records pertaining to the same individual across multiple data sets. In the fields of public health [(e.g., @jutteAdministrativeRecordLinkage2011] and demographics [e.g., @abbottLargescaleLinkageTotal2015], for example, linking data proved beneficial to states seeking to improve services for their citizens and facilitate research. Through the use of identifiers and shared attributes such as name, address, date of birth, or social security number, states could establish a more detailed profile of individuals by connecting data records [@newcombeRecordLinkageMaking1962]. Difficulties arose when attempting to link personal data because of data quality issues, such as inconsistencies in name spellings or missing information, prompting the development of new technologies and techniques to tackle these challenges.

The emergence of electronic computers and database technology enabled more sophisticated matching algorithms to be developed, leading to increased adoption in other fields [@batiniObjectIdentification2016; @christenDataMatchingConcepts2012]. As a result, the process of matching data sets and linking records is now referred to by various names, such as data matching, data linking, data merging, data integration, record linkage, deduplication, or entity resolution, depending on the context and application [@christenDataMatchingConcepts2012]. This dissertation will use the term _data matching_ as it is a more general term referring to identifying records in data sets that refer to the same real-world persons (or other entities) and reconciling duplicates or inconsistencies between data sets.

Another concept related to data matching is _schema matching_ [@bellahseneSchemaMatchingMapping2011; @kementsietsidisSchemaMatching2009]. Schema matching addresses the challenge of integrating data from multiple sources that have different schema structures or data models. Schema matching’s importance stems from its capacity to facilitate data integration across disparate data sources, which is frequently required for effective data matching. For example, identifying records referring to the same person may be challenging without knowledge of the underlying data models, as the same person may be represented differently across different data sets. Therefore, in fields where data are fragmented and dispersed across multiple sources, data matching and schema matching are crucial components of successful data management and integration. While the term “schema matching” will not be used in the dissertation, the question of how to investigate correspondences and differences between different data models (i.e., schemas) that underpin the data will be explored in greater depth.

Over the years, various data matching methods and techniques for classifying matches have been devised [@batiniObjectIdentification2016; @christenDataMatchingConcepts2012; @fellegiTheoryRecordLinkage1969; @winklerMatchingRecordLinkage2014]. The following standard data matching methods can be distinguished based on the literature. One of the most basic techniques for identifying matching records is _deterministic matching_, which employs predefined rules or criteria. For example, when two records have the same first name, last name, and date of birth, they are considered a match. Another approach is _probabilistic matching_, which uses statistical algorithms to calculate the probability that two records are a match based on the similarity of their categories of data. If, as in the previous example, two data records have similar but not identical names or dates of birth, the records may still be considered a match based on the probability calculation. Another approach is _rule-based matching_, which can combine deterministic and probabilistic methods to find matches and incorporate expert knowledge or domain-specific rules to increase accuracy. Finally, matching techniques based on _machine learning_ are gaining popularity. Such methods employ algorithms that can learn from data to improve accuracy and more easily adapt to new data sources.

While data matching may seem like a technical process, its increasing use and impact on society and individuals mean that it has significant consequences that should not be overlooked. With the growth of the internet and the digitalization of many aspects of contemporary life, data matching has become even more ubiquitous, with many actors using these techniques to link data from different sources and gain insights into individuals, their behavior, and preferences [@clarkeHumanIdentificationInformation1994; @gandySurveillanceSocietyInformation1989; @zuboffBigOtherSurveillance2015]. Furthermore, using data matching algorithms in automated systems can introduce errors and biases, making some people disproportionately the target of surveillance and control [e.g., @aradauAlgorithmicSurveillancePolitical2021; @benjaminRaceTechnologyAbolitionist2019; @eubanksAutomatingInequalityHow2018]. For instance, the German-Lebanese citizen Khalid al-Masri was imprisoned and tortured by the CIA in 2003 after being mistakenly identified as a suspected terrorist with a similar name [@priestWrongfulImprisonmentAnatomy2005]. Data matching technologies play a crucial role in these processes by allowing for the analysis and correlation of vast amounts of data and determining previously unknown connections in identity data.

Data matching has a long history of addressing the challenges posed by fragmented, incomplete, and duplicated information across multiple sources, with the development of various techniques. However, data matching is not just a technical process that can potentially discover previously unknown connections, but can alter the things being connected. These connections can affect the meaning of the data, practices, and the organizations that use it. For instance, one could argue that by matching flight passenger data to terrorist watch lists, the identification of a match alters the original meaning of the passenger data, and changes the role of organizations such as airline carriers [see also, @amooreGovernanceRiskDataveillance2005; @bellanovaDifferentViewMaking2012]. Over time, passenger data has evolved from simple travel information to a powerful tool that connects data, allowing for the identification of suspicious travel patterns and the detection of individuals who may be considered security risks.

As such, it is crucial to understand how data matching technology shapes the meaning of data and practices, including identifying data as suspicious and shaping relationships between organizations whose data are being matched and connected. This research seeks to contribute to a more performative understanding of the role of data matching technology by investigating how it shapes the meaning of data, practices, and organizations. The choice of exploring data matching in border security and migration control is linked to the overarching _Processing Citizenship_ (PC) project, which aims to understand how data infrastructures for processing migrants and refugees co-produce individuals and Europe [@pcProcessingCitizenshipDigital2017].[^funding] The purpose of investigating the use of matching and linking data in the context of identity data in border security and migration management in this dissertation is thus closely connected to the PC project’s aim of exploring how the production, evaluation and circulation of data about third-country nationals are reshaping European governance [see also, @pelizzaProcessingAlterityEnacting2019; @pelizzaTellingMoreComplex2023]. Specifically, this research aims to examine the use of matching and linking data in the context of identity data in border security and migration management. The following section takes a closer look at how data matching is used in this context by exploring a contemporary example of data matching in migration and border control within the European Union.

[^funding]: The Processing Citizenship project, including this PhD research, was funded by the European Research Council in the context of the European Framework Program for Research and Innovation Horizon 2020, grant agreement No 714463, principal investigator Annalisa Pelizza.

## Leveraging data matching for border control

> In light of the recent terrorist attacks in Europe and the increase in irregular migration in recent years, action needs to be taken to address this risk of information gaps and blind spots. The measures in this proposal [Interoperability of EU information systems for security, border and migration management] will ensure the various systems can exchange data and share information so that authorized bodies and officers have the information they need to strengthen our borders and better protect Europe. [@europeancommissionFrequentlyAskedQuestions2017]

---

> Establishing a common repository of data would overcome the current fragmentation in the EU’s architecture of data management for border control and security. This fragmentation is contrary to the data minimization principle, as it results in the same data being stored several times. Where necessary, the common repository would allow for the recognition of connections and provide an overall picture by combining individual data elements stored in different information systems. It would thus address the current knowledge gaps and shed light on blind spots for border guards and police officers. [@europeancommissionCommunicationCommissionEuropean2016, p. 18]

---

> [One of] the four technical components of the proposal [is] a multiple identity detector — this will verify whether the biographical data that is being searched exists in multiple systems, helping to detect multiple identities. It has the dual purpose of ensuring the correct identification of bona fide persons and combating identity fraud. [@europeancommissionFrequentlyAskedQuestions2017]

---

These quotes reveal how in the European Union (EU) context data matching is regarded as a critical component in addressing identity issues in migration and border control systems, including identifying multiple identities. The quotes above refer to a project linking identity data of different EU information systems for security, border, and migration management.[^interop] Presently, each of these EU information systems operates independently of its database and serves a distinct purpose, such as managing asylum requests, processing visa applications, or supporting law enforcement activities. The proposal explicitly identifies a potential risk of information gaps and blind spots because data are not connected. It proposes to address this risk by connecting and sharing information from those multiple systems. Furthermore, the European Commission (EC) communication underscores the importance of having the necessary information to strengthen borders and identify potential threats.

[^interop]: The interoperability initiative follows recommendations from the “High Level Expert Group on Information Systems and Interoperability” [@europeancommissionCommissionDecision172016] and a new legislative mandate [@europeanunionRegulationEU20182018] for the European Agency for the operational management of large-scale IT systems in the area of freedom, security and justice (eu-LISA). It was split into two proposals due to different legal provisions for regulating (a) borders and visas and (b) police and judicial cooperation, asylum, and migration [@europeanunionRegulationEU20192019; @europeanunionRegulationEU20192019a].

The second quote describes the need for “establishing a common repository of data,” which would “address the current knowledge gaps and shed light on blind spots for border guards and police officers” (p. 18). Finally, the third quote describes a component for finding multiple identities that refer to the same person. Fragmentation of the EU’s data management architecture for border control and security is thus portrayed as causing duplicate data storage and leaving border guards and police officers with knowledge gaps and blind spots. According to this logic, the common repository and multiple identity detector components would enable the recognition of connections and provide a holistic view by combining data elements stored in different information systems.

The EU interoperability initiative introduces new components that emphasize the growing significance of data matching technologies. By allowing the matching of biometric data, visa data, and other identity-related information, the initiative aims to enhance the accuracy and efficiency of EU information systems for mobility and border control. However, the use of these new components is not just limited to improving the functioning of these systems. They will also be pivotal in implementing new, interlinked forms of identification of individuals deemed suspicious based on the links between data sets [@quintelConnectingPersonalData2018] based on probabilistic, rule-based, or machine learning-based data matching. Understanding the performative nature of data matching technology is essential, as it shapes the perceptions and treatment of individuals in the context of border security and migration management.

Note that the European Commission is not building these systems by itself; it increasingly relies on global information technology suppliers and integrators [@lemberg-pedersenPoliticalEconomyEntry2020; @valdiviaNeitherOpaqueTransparent2022]. A research gap exists in understanding how data matching technologies, which are increasingly developed by commercial entities for global use [see, for example, @leeseStandardizingSecurityBusiness2018; @lemberg-pedersenPoliticalEconomyEntry2020; @valdiviaNeitherOpaqueTransparent2022; @zureikGovernanceSecurityTechnology2004], operate and influence processes of identification in a sensitive domain. The international and commercial dimensions of identification technology mean that there is a growing need to examine how the private sector is involved in developing and implementing these standardizing technologies. As @pollockSoftwareOrganisationsBiography2009 have noted, criticism of standardized software and focus on how poorly such software adapts to different settings is not a sufficient research perspective. The widespread use of standardized identification software requires comprehending how such software is produced and adapted to operate in various contexts, as well.

## Unpacking the challenge of analyzing data matching in transnational data infrastructures

\sectionmark{Analyzing data matching in transnational data infrastructures}

The previous examples show how identity data matching has developed into an integral part of border and migration control systems to enable the identification and tracking of individuals across different systems and jurisdictions. As such, matching data from national and international sources indicates that identification practices extend beyond the borders of nation-states, highlighting the internationalization of identification. Yet, research on identification has typically concentrated on how states identify people. Data matching technology, however, is illustrative of the global dimensions of identification, which become apparent only when looking beyond the borders of individual nations.

A transnational perspective can thus shift the focus away from the nation-state and onto the various other actors involved in identification. The shift from state authorities creating and implementing identification technology to the state purchasing systems created by commercial organizations can be seen in various programmes involving global information technology companies. For example, the United States Automated Biometric Identification System relied on Cogent/Thales’ automated fingerprint identification technology [@thalesgroupDHSAutomatedBiometric2021], while India’s Aadhaar biometric ID system utilized Accenture and Daon’s technology for combining different biometric modalities [@accentureUniqueIdentificationAuthority2010]. Meanwhile, the upcoming European Entry/Exit System will utilize IDEMIA’s biometric matching systems [@accentureEuropeanCommissionSelects2012]. In this way, identification technology is increasingly becoming a commercial product rather than a creation of the state. Yet, little is known about how these actors developed identity data matching systems and put them to work. This lack of knowledge can be attributed to various factors, including the lack of transparency in developing these systems, limited access to information on their design and operation, and the complexity of the underlying technical and trans-organizational processes.

Data matching technology for identification can also be seen as a component of a broader _data infrastructure_ [e.g., @flyverbomDatastructuringOrganizingCurating2018; @kitchinDataRevolutionBig2014].[^info-infra] Infrastructures are those things we depend on to make other things work [@edwardsIntroductionAgendaInfrastructure2009; @starStepsEcologyInfrastructure1996]. Hence, data infrastructure includes the technologies, protocols, regulations, habits, procedures, and agreements to handle and utilize data. In the case of data matching, the technology is an essential component of the infrastructure that enables the sharing, linking, and matching of identity data across various systems and organizations. The development and maintenance of this infrastructure require collaboration and coordination among different actors, including government agencies, private companies, and international organizations. Understanding data matching as part of data infrastructure provides a more comprehensive view of the interconnected systems that enable local, national, and international identification practices.

[^info-infra]: Earlier studies have employed alternative terminologies such as “information infrastructure” [e.g., @bowkerInformationInfrastructureStudies2009; @hansethDevelopingInformationInfrastructure1996] or “e-infrastructure” [e.g., @pollockEInfrastructuresHowWe2010] to refer to these assemblages of technological and social components that enable the flow and management of data across different settings and contexts.

Considering these challenges, the following section will outline this study’s research problem, aims, and objectives, which seek to unpack the complexities of analyzing data matching in transnational data infrastructure.

### Research problem, aims, and objectives

This dissertation aims to contribute to a more performative understanding of the role of data matching technology by investigating how it shapes the meaning of data, practices, and organizations in transnational contexts, particularly in the securitization of the European border. Recognizing a gap in research regarding the performative effects of data matching technology in transnational security infrastructures, this study aims to empirically investigate its involvement in infrastructure development, security, and internationalization within the realm of identification. To address this research problem and achieve the overarching research aims, the dissertation outlines specific research objectives:

* To map the theoretical landscape related to internationalization, securitization, and infrastructuring of identification and derive the dissertation’s research question for investigating data matching in transnational data infrastructures (Chapter 2).
* To develop a methodological framework for analyzing data matching in transnational infrastructures using methodological strategies to uncover the embedded and less obvious technical details of matching and linking identity data in infrastructures (Chapter 3).
* To introduce a new method and software tool for analyzing the schemas that underpin information systems in population management (Chapter 4).
* To examine the relationship between identity data matching technologies and routine identification practices (Chapter 5).
* To investigate the long-term development of identification systems and building of transnational data infrastructures by identifying contingent moments in their evolution to explore how data matching expertise travels and circulates (Chapter 6).

The research aim to map the theoretical landscape aims to help understand how the meanings, practices, and technologies of identification have changed as it has become more international, commercial, linked to security issues, and part of broader infrastructures. The methodological framework and methodological strategy aim to provide an overarching framework for analyzing data matching in transnational infrastructures. The objective of introducing a new method and software tool to analyze the schemas underpinning information systems in population management is to examine the expectations and imaginaries of the schemas underpinning information systems. The objective of examining routine identification practices is to understand the relationship with identity data matching technologies, shedding light on how these technologies shape the utilization and meanings of data. Finally, the objective of investigating the long-term development of identification systems and the building of transnational data infrastructures is to explore how data matching expertise and technologies travel and circulate. The following section provides an overview of the dissertation’s structure, outlining the chapters and their focus.

## Structure of the dissertation

The organization of this dissertation involves an initial mapping of theoretical concepts, followed by the development of a methodological framework to direct the analysis, and follows with empirical chapters. Chapter 2 starts by mapping theoretical concepts on identification and matching identity data, drawing on literature related to the internationalization and commercialization of identification, the securitization of identification, and the infrastructuring of identification. This chapter lays the groundwork for the subsequent chapters by discussing various theoretical perspectives on matching identity data and the implications of matching identity data for transnational data infrastructures.

Chapter 3 introduces the methodological framework for analyzing data matching in transnational infrastructures. The framework proposes three methodological strategies, wherein data matching serves as both a research topic and a methodological resource. These three strategies are based on comparing data models, analyzing data practices and tracing sociotechnical change. Comparing data models can reveal information collected by various organizations and systems; data practices can show the searching and matching of identity data within and across organizations; sociotechnical change can shed light on the circulation of data matching knowledge, technologies, and practices over time and across organizations. The chapter explains how these strategies were used in the dissertation’s fieldwork at a software company developing data matching technology. The chapter also describes the methods of data collection and the techniques of data analysis used in the dissertation.

Chapter 4 introduces the “Ontology Explorer” (OE) methodology, a semantic approach and an open-source tool to analyze the data models’ underlying information systems. The method draws inspiration from schema matching and is designed to compare data models in different formats used by various systems. This chapter explains how it is applied in the dissertation to reveal less visible assumptions and patterns in information systems design. Unlike other methods, the OE allows for the systematic comparison of non-homogeneous data formats and enables comparisons of data models across information systems run by diverse organizations and authorities. Therefore, the OE makes it possible to observe how identity data properties influence the production and circulation of data and the relations between different authorities’ data models.

Chapter 5 examines the relationship between technologies for searching and matching identity data and routine bureaucratic identification practices in migration management. The chapter focuses on how a government migration agency searches and matches applicants’ data using a data matching system. The chapter introduces the concept of “re-identification” to refer to the process by which subjects of bureaucratic procedures are re-identified in data infrastructures at various points in those procedures. The chapter demonstrates the implications of data matching in bureaucratic settings in two ways. First, the chapter shifts the usual focus from first registration to re-identification practices across data infrastructures. Secondly, the findings underscore that, while integrating data matching tools for re-identification alleviates data friction, it inadvertently also comes with certain costs.

Chapter 6 looks at the long-term development of identification systems and infrastructures. The chapter proposes two heuristics for detecting contingent moments in the evolution of identification technologies. First, it demonstrates how a data matching system’s changing “interpretative flexibility” allows discerning actors’ varying problematizations of identification, such as those related to the securitization of identification. Second, the chapter demonstrates how “gateway moments” make it possible to see the compromises necessary when building identification infrastructures and adapting globally honed technologies to new settings. Together, the chapter’s findings shed light on the activities of under-the-radar actors, such as commercial software vendors, whose distribution and reuse of systems have long-term implications for identification practices and infrastructures in various contexts.

The dissertation concludes in Chapter 7 with a summary of empirical findings, literature contributions, and reflections on the research process. Contributions include mapping the theoretical landscape of identity data matching, introducing a methodological framework for analyzing data matching in transnational infrastructures, proposing new methods for analyzing data matching and using these to examine the relationship between data matching technologies and bureaucratic practices. The study makes an additional contribution by delving into the long-term evolution of identification systems and infrastructures. Finally, the chapter acknowledges the study’s limitations and suggests areas for future research. The dissertation aims to advance our understanding of identity data matching by putting it into the STS and critical data studies agendas. It contends that matching identity data is a multifaceted phenomenon that requires a nuanced and interdisciplinary approach to understand how it shapes and is shaped by transnational data infrastructures.