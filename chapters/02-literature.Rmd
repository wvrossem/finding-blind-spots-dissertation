# Matching identity data in transnational security infrastructures and beyond {#ch-literature}

\chaptermark{Matching identity data}

<!-- ## Literature 1 — data and classification studies -->

<!-- ## Literature 2 — collaborative work in information infrastructures -->

<!-- ## Literature 3 — nexus of user-designer-technology -->

## A conceptual framework for analysing data matching in distributed settings

Following the overview of existing work and the gaps in their methods, I now put forward a conceptual framework for tracing data matching practices and technologies in transnational security data infrastructures. Such a framework is needed since no single method or theory can be adapted to examine the multiple facets of data matching practices and technologies within and across organizations. In this section, I therefore propose a new conceptual framework that amalgamates established methods and theories into one framework that can guide the research in examining those multiple facets. The conceptual framework, in essence, integrates more computer science views on linking and matching data models and records, with social science views from fields associated with Science and Technology Studies (STS).

As previously said, for government agencies, business, and other organizations the tasks of finding records in databases that refer to the same person can be important in many situations. As organizations and agencies have been collecting ever more data about people, they may perceive performance benefits of having complete and accurate information about people, or even see it as beneficial to national security. The techniques for data matching — such as the mathematical models for linking records — are therefore well established. However, the creation, development, and consequences of deploying technologies for data matching are less well understood and researched. With the framework that I outline in this section I aim to address this gap in the research. I propose to trace data matching practices and technologies as sociotechnical phenomena, and from four different angles: (1) how the kinds of data that are collected about people can be compared and what this tells us about how organizations can search, match, and use this data, (2) how within organizations search and match for data about people that may have data quality issues, (3) how such identity data is matched between organizations, and (4) how, simultaneously with matching identity data, data matching technologies and practices travel and evolve across organizations.

To discuss these different aspects of analysing the matching of identity data, I propose to visualize the framework through several axes (Figure \@ref(fig:method-axes-visualization)). This visualization roughly follows a relational model to represent data as tables that model relationships between data. The three axes capture different dimensions of data, which are: _data models_, _categories of data_, and _data values_. Next, I will describe how each of these three axes and how different relations between the axes correspond to specific aspects of data matching within and across organizations.

<!-- ```{r method-axes-visualization, echo=FALSE, fig.cap="A visualization of the different aspects of data matching examined through the framework.", out.width="100%", fig.align="center"}
knitr::include_graphics("figures/method-axes-visualization.pdf")
``` -->

To begin, the axis for _data models_ (axis `z`) represents the abstract models that standardize the kinds of information about people collected by different organizations. An example of such a data model could be the schemas specifying data collected about migrants by a government agency. Each organization generally defines its own data model. However, some data models might even be shared to make interfacing between systems possible. Next, each data model specifies _categories of data_ (axis `x`) which are collected about people. For example, the categories of data of the example asylum agency's data model may include "name", "place of birth", "date of birth". Finally, there are the actual _data values_ (axis `y`) in systems' databases for a data model and its categories of data. For example, a value for the category of data "place of birth" may be "Brussels".

That being so, I propose to schematize different facets of data matching as combinations of pairs of these axes. In Figure \@ref(fig:method-axes-visualization), three axes each zoom in on specific facets of data matching within and across organizations.

### Matching data models

The first pair _(data models `z`, categories of data `x`)_ represents the identification of semantic similarities and differences between different data models and their categories of data. Such a process will also be used in the research approach to support discursive analyses of data models. In this way, the analysis intends to compare the expectations and imaginaries of diverse actors concerning data collected about people and the ways data can be searched, matched, used. For example, the data model of a government agency may contain the category of data "family name" while another agency uses "surname" as category of data. Even though the name of the categories of data are distinct, they clearly refer to a similar concept. In this case, a part of someone's personal name that indicates their family. To connect information about people from these two agencies'systems should therefore identify similarities between their data models and these two categories of data.

This first aspect of matching people's identity data that is addressed by the conceptual framework is therefore the relations between different information systems' data models. That is, how different data models for collecting information about people-on-the-move are situated in relation to one another, and what this tell about the organizations which developed and make use of these models to collect, search, and match identities.

To do this analysis, the method draws, first, on studies of classification and its consequences which have addressed the question of how to retrace the ethical and political work of otherwise mundane devices of representations — i.e., data models. Researchers have previously used and combined various makeshift methods — from narrative interviews [@gazanImposingStructuresNarrative2005] to discursive textual analysis [@caswellUsingClassificationConvict2012], from participant observation [@meershoekConstructionEthnicDifferences2011] to archival and genealogical research [@gassonGenealogicalStudyBoundaryspanning2006] — for the ethnographic and historical studies of information systems [@starStepsEcologyInfrastructure1996] and their classifications. Furthermore, it is altogether necessary to take a more interdisciplinary approach and also pay close attention to the technical details of classification, as to avoid only considering the effects of classification [@kitchinCodeSpaceSoftware2011].

Following Geiger and Ribes [-@geigerTraceEthnographyFollowing2011], data models can be considered 'thin' traces: being rather standardized schemas made of categories and values, they are hardly meaningful in themselves. The method to analyse them therefore should consider how to turn them into 'thick' data. Nevertheless, given their implementation in diverse infrastructures, data models can be an excellent starting point to understand how geographically distributed sites are connected [@latourReassemblingSocialIntroduction2005]. Burns and Wark [@burnsWhereDatabaseDigital2020], for example, have dubbed such approach "database ethnography" and used traces left behind of a database as a site to analyse how social meanings of phenomena change over time — but, only using their own ad-hoc mix of methods.

On the other hand, matching data models shares concerns with methods developed in computer science fields interested in the use of semantic technologies, such as knowledge engineering, linked data, and natural language processing. For example, the relationship between the different types of categories and their variations is closely linked to the outcome of processes known as schema or ontology matching, which finds correspondences between concepts of schemas or ontologies [@euzenatOntologyMatching2007; @kementsietsidisSchemaMatching2009]. Another linked technique called ontology learning aims to automate processes for creating ontologies. These (semi-)automatic mechanisms then include extracting concepts and relationships between concepts from a domain of discourse by analysing a corpus of documents in that domain. There are thus shared concerns about how to examine the way different knowledge representations interrelate, and about how to recover such representations from a discursive domain. Yet, there are fundamental differences which make the methods hard to effectively support a discursive analysis.

Methods of knowledge engineering are generally more concerned with creating technologies that help machines understand better means for the systems, and with integrating different sources of information work together. By contrast, the method for this research needs to support discursive analysis by taking differences in representations as a starting point to analyse imaginaries of diverse organizations and authorities and to combine the analysis with ethnographic observation. To address this aspect of data matching there is therefore a need for approaches that can support analyses of formalized data models in two ways. First, to support analyses of information systems which define their own data models, even if these systems are not immediately comparable. Second, to systematically and quantitatively support discursive analysis of 'thin' data models, also by detecting differences and absences between systems.

### Data matching within organizations

The second pair _(categories of data `x`, data values `y`)_ relates to the categories of data and their actual values in databases, which may not always correspond to the expectations of the data models. For example, when values for the categories "first name" and "last name" are mistakenly switched in the database. Practices and technologies may therefore be employed to remediate such data quality issues — i.e., data matching. Overall, two applications of data matching within organizations and their databases can be distinguished: real-time data matching and deduplication. Real-time data matching mainly addresses the issue of retrieving information about a person through search queries. For example, police officers may need query databases based on the categories of data "name", "nationality", "date of birth" and to see if approximate matches exist for those personal details. Data matching techniques will therefore be used to find similar matching identities' data, rather than only exact matches. Deduplication, on the other hand, is the use of data matching technologies for identifying records in a database that refer to the same person and fusing the multiple records.

On one hand, it is crucial to understand the moments when actual data are not aligned with the expected data model. For example, when multiple records were created for a person due to name variations. In this way, the relation between data expected by data models and actual data in databases can draw attention to how organizations collect and use knowledge about people-on-the-move. For example, how they choose to take up technologies for searching and matching to help deal with data uncertainties deriving from frictions between the moments of collection and use of data. On the other hand, the design and use of these tools for dealing with such data matching need scrutiny. The design and use of such data matching tools can be compared to understand the relations between users, designers, and technologies. Such comparisons can identify expectations and imaginaries of designers that are inscribed into technological artefacts and how they are all configured together.

Scholars in fields of STS and beyond have devised different approaches to examine and understand the relations between users and technologies [@oudshoornHowUsersMatter2003], and how expectations and imaginaries of designers can be inscribed into technological artefacts. For example, the approach of script analysis was advanced to examine the assumed competences of users and affordances that are embedded in artefacts [@akrichSummaryConvenientVocabulary1992; @latourWhereAreMissing1992]. The script of an artefact then requires users to adopt designers' envisaged behaviours and actions to interact with an artefact. This approach also allows accounting for situations when those assumptions by artefact designers don't match the actual uses and practices. @woolgarConfiguringUserCase1990 furthermore noted how system designers might attempt to "configure the user" of the system by incorporating the user into the sociotechnical system. The method to understand such configuration is by treating the computer systems as a text that is read by users and can be interpreted differently. Designers may thus attempt to anticipate and delimit this flexibility. In these views, a sociotechnical system functions well when users and the system are successfully configured together.

Suchman [-@suchmanHumanmachineReconfigurationsPlans2007] furthermore argued that the inscriptions of users and uses are never that coherent, and that a more open-ended and indeterminate approach towards artefacts is required. She proposes configuration as a "method assemblage" that pays particular attention to technologies and "the imaginaries and materialities that they join together" [@suchmanConfiguration2014, p. 48]. Configuration as a methodological device can thus be, in accordance with Suchman's view, useful to unpack the material and discursive elements of software for dealing with data uncertainties.

This axis in the conceptual framework therefore aims to answer the question of what kinds of imaginaries and materialities are joined together in data matching technologies. Analysing and comparing the technical designs with actual uses, and thus also where they mismatch, can give evidence of the various imaginaries, expectations, assumptions at play. Yet, the approach also needs be both specific to the situatedness of the design, and keep an open-ended and indeterminate view of how such technologies are put to use [@suchmanConfiguration2014].

### Data matching across organizations

Finally, the pair _(data models `z`, data values `y`)_ addresses data matching across organizations and agencies. Identity data records that refer to the same person may exist in systems and databases of different organizations and government agencies. Data matching in this case refers to the tasks related to identifying and potentially linking or merging such identity data spread across various databases. A crucial challenge for data matching techniques is due to the fact that unique identifiers are not always available and, hence, that matching relies on personal information which is not always complete or accurate. Personal information such as names, dates of birth frequently contain typographical errors and variations. For example, a woman may have used her maiden name as surname in one system while information collected in another database uses her husband's surname after marriage. However, the question for us here is how — by bringing such data in relation with each other — data matching practices and technologies shape the relations between different actors.

Furthermore, these data matching practices and technologies cannot be studied independently of the infrastructures they are embedded in. Data matching technologies can be crucial to make certain collaborative work possible, such as the processing of an asylum application based on information collected about a person by different organizations and agencies. At the same time, the work of cleaning and managing data often can become invisible. Infrastructural studies have therefore provided different methodological strategies to make visible the distributed interconnections between technical minutiae and the politics of knowledge production [e.g. @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Investigating distributed infrastructures has thus been addressed by approaching infrastructures as a 'relational concept' [@starStepsEcologyInfrastructure1996]. Such relational approaches maintain that infrastructures are not objects of study by itself, but show preference for ethnographic methods to study how infrastructures emerge through interactive processes and practices [@starEthnographyInfrastructure1999; @karastiStudyingInfrastructuringEthnographically2018]. Methods include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999; @latourReassemblingSocialIntroduction2005], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019].

Based on the insights from infrastructural studies, data matching technologies in fields of security can therefore become an inconspicuous element in transnational data infrastructures. This in line with current debates in fields of International Relations (IR) and Security Studies who have started to acknowledge the role technology international politics [@amicelleQuestioningSecurityDevices2015; @hoijtinkTechnologyAgencyInternational2019; @bellanovaAlgorithmicRegulationSecurity2020]. Crucially, these scholars extend agency to the technologies and artefacts used in security practices. Glouftsios has, for example, shown how the often-invisible maintenance for the large-scale information systems for border security in the EU has a crucial role in sustaining the governance of international mobility [@glouftsiosGoverningBorderSecurity2020]. (more examples)

In the case of data matching technologies, the underlying assumption is that such technologies can trigger or even settle controversies about which actors produce more reliable data. For example, when matching identity data from different agencies these tools will bring certain records in relation with each other. If there are uncertainties about the data, they may then play a role in disputes about which agency's data are correct. The hypothesis is that data matching technologies can thus inscribe certain forms of agency that can reconfigure practices of security and migration in transnational data infrastructures. Infrastructural inversions are therefore key to calling attention to the background or invisible work of mundane data management practices.

### Travelling data matching software

Furthermore, and related to the last pair, is the question of how not only identity data are matched across systems, but also how technologies and practices for data matching travel across organizations. As companies develop tools to support these tasks and deploy them at multiple locations and times, the concomitant knowledge of how to manage typographic errors, name variations, missing data, and so forth may also travel across organizations. This axis is consequently related to how data are not only matched between different organizations and agencies in security contexts. Rather, the hypothesis is that, at the same time, knowledge and technologies for such data practices travel between organizations.

Although the notions of "script" and "configuring the user" have been generative in understand the relations between users and technologies, they tend to overemphasize the role of designers [@oudshoornHowUsersMatter2003]. More recent approaches have therefore suggested to, for example, take a more comprehensive approach to examine important moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. An alternative approach has therefore been developed by scholars called the 'Biography of Artefacts and Practices' that can detail the complexity of development of such technologies and related practices.

Following this approach, the emphasis of this aspect in the conceptual framework is to understand the biography of a data matching software packages. This approach is thought to highlight the wide range of actors involved in the development of such security technology and how the software travels and evolves over extended periods of time and between in different locales. This expands the narrower focus of the user-design-technology nexus used in the second axis to take a more comprehensive view of how the technology has been shaped over time, and by a broader number of actors at multiple sites.

## Old sections {-}

### Data matching {-}

In security settings the potential to link and enhance people’s identity data is seen by policy-makers as crucial to deliver expected political objectives and operational benefits such as creating trust between states, ensuring freedom of movement, and internal security (eu-LISA conference 2020). From this view, data quality and data matching can be seen as ways to "connect the dots" — i.e., connecting identity records from different databases that refer to the same real-world persons serves as a way to find patterns and irregularities. Yet at the same time, such technologies play a role in both settling or triggering controversies about which actors produce more reliable data (Pelizza 2016).

Scholars in the field of data and classification studies have extensively shown that data are never "raw," but always constituted by different choices and constraints. It is therefore crucial to begin by looking at the data models used by organizations collecting information for population management. Following this, techniques and practices of managing and cleaning data can then be understood as an further part of this “cooking”. Yet, scant attention has been paid to actual practices and techniques used to accomplish matching and linking data — such as probabilistic record linkage, fuzzy matching. This is surprising because by bringing different data into relation with each other, the tools can recontextualize data and can have organizational consequences such as redistributing of tasks and responsibilities between different actors. There is therefore a clear need to understand how such technologies for matching identity data are designed and put into practice.

However, such data matching practices and technologies cannot be studied independently of the larger transnational security infrastructures they are embedded in. Such tools are furthermore crucial to make certain collaborative work possible, such as the processing of an asylum application based on information collected about a person by different organizations and agencies. At the same time, the work of cleaning and managing data often can become invisible. Infrastructural studies have therefore proposed kinds of “infrastructural inversions “(Bowker) that can be used to foreground such backstage elements as work practices of managing and cleaning data.

STS scholars have further addressed these issues by proposing different approaches to examine and understand the nexus between users, technologies, and designers. Notions such as ‘script’ have been introduced to account for the gaps between designed intended uses and actual uses. While concepts such as ‘configuration’ emphasize how expectations and imaginaries of designers are inscribed into technological artefacts, and how they users and artefacts are figured together — delimiting and making possible certain forms of agency. Other scholars have however emphasized that such designs should not be studies as a single snapshot in time, but as a kind of “biography of artefacts and practices” (Pollock Williams). Following these approaches, different moments in the life of the data matching technology and practices should addressed.

In summary, and drawing on these literatures, there is a need to investigate data matching technologies and practices in a way which allows taking at same time a situated and a more longitudinal approach to investigate how they are shaping and shaped by transnational security infrastructures.

### The visualization of infrastructures at the border {-}

Data models and ontologies are entwined in digital infrastructures and practices, and institute certain ways of knowing and working [see e.g. @bowkerSortingThingsOut1999; @lamplandStandardsTheirStories2009; @timmermansWorldStandardsNot2010]. Data models designed to represent phenomena can be considered as a technology that enact particular kinds of knowledge, organisations and practices [@bloomfieldVisionsOrganizationOrganizations1997]. Applied to the field of border management, this implies that data models do not only represent people at the border: they also make them knowable in specific ways. For example, family composition fields that can count up to five members assume the Western parental family model. Data models that foresee only female and male gender options assume migrants who are binarily sexualized. Even the choice of identifying people through fingerprints instead of study degree or family tree shrink the possibilities in the way they can be known [@pelizzaIdentificationTranslationArt2021a].

Data models' capability to shape ways of knowing goes hand in hand with their ability to hide. The politics of data model design shapes how information is represented, and affects what becomes visible and, consequently, what becomes invisible [@hansethInscribingBehaviourInformation1997; @bowkerSortingThingsOut1999]. Yet data models themselves can easily become invisible. Bowker and Star [-@bowkerSortingThingsOut1999] for example have pointed out how choices made during design processes easily become invisible, as only what is in the systems is usually (and at best) documented.

Studies on infrastructures have therefore provided different strategies to invert this tendency of infrastructures to disappear, and to make visible the interconnections between technical minutiae and the politics of knowledge production [e.g. @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Methods proposed by these authors to invert the tendency of infrastructure to disappear include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019]. Since the 1990s, the question of how to make visible ways of knowing that have sunken into databases has primarily been addressed by approaching infrastructures as a "relational concept" [@starStepsEcologyInfrastructure1996]. Such relational approaches maintain that infrastructures are not objects of study by itself, but show preference for ethnographic methods to study how infrastructures emerge through interactive processes and practices [@starEthnographyInfrastructure1999; @karastiStudyingInfrastructuringEthnographically2018]. We argue that there is, however, still room for other kinds of methodological contributions to further the analysis of information systems' ontologies and data models.

More recently, the rise of Big Data (and its effects on the volume, variety, and uses of data) has prompted the field of Critical Data Studies (CDS) to renew the call to make visible the ways in which data is generated, circulated and deployed [@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. While the previous researchers mainly take a relational approach to study infrastructures and social order, researchers associated with CDS prefer different "critical frameworks in order to foreground data's power structures" (ibid., p. 2). That is, they emphasise how data are never "raw," but always constituted by different choices and constraints [@gitelmanRawDataOxymoron2013]. The two strands of research are therefore in many ways complementary. Rather, important questions have been raised by CDS on how the links between different mechanisms and elements that constitute data shape power relations. For example, which aspects of everyday experience are datafied, and which ones are made invisible?

In the case of border management, such relations are loaded with asymmetries that can have serious long-term consequences for the life of people on the move [@amelinaTransnationalizingInequalitiesEurope2016]. Understanding the ways in which data is constituted therefore becomes especially important for critical approach that aim to intervene into the "data politics" [@ruppertDataPolitics2017] of how subjectivities come into being. For example, analyses of information systems and their data models and standards in other fields have shown how such systems can enact young people as young offender through various relations [@ruppertNotJustAnother2013], or how different systems and standards enact family relations in distinct ways [@cornfordRepresentingFamilyHow2013]. However, much of the research up to now has employed rather ad hoc methods which are difficult to replicate. In addition, to our knowledge, no research has been found that applied this type of approach in the field of migration and border control.

This paper therefore aims to contribute to these two strands of research by developing an analytical method to make visible data models underpinning the information systems used for border management. In so doing, it answers the call by `r anonymize("the “Alterity Processing” research framework")` to analyse how people are enacted at the border by discursively comparing the data models used in diverse systems `r anonymize("[@pelizzaProcessingCitizenshipDigital2017; @pelizzaProcessingAlterityEnacting2019]")`. As a matter of fact, data models' comparison is expected to highlight not only specific ways of knowing, but also missing ones. The design choices we took in developing our analytical method and visualization tools are thus closely informed by such need to make silences hearable.

To accomplish the goal of making visible the politics of data modelling, we identified an exploratory set of criteria as benchmarks to evaluate the method. These criteria represent different areas of importance and tensions in the way information is represented in data models; tensions that may otherwise be less visible. The set of criteria is not aimed to be comprehensive. Rather, criteria guided and served as tests during the development of the method. In this way, the set compiles an eclectic mix of insights ranging from the work of @bowkerSortingThingsOut1999 and their insights on making visible the politics of data model design, to @dourishStuffBitsEssay2017 his insights on the material properties of information.

* **Comparability**: the method should allow for homogeneous comparison of data models used in different settings and for different purposes. For example, it should be possible to compare data models used in information systems of different Member States. Similarly, it should be possible to compare a data model used in policing against a data model used by asylum services.
* **Visibility**: the method should make visible categories that are made invisible. This can be achieved through breakdowns or comparisons in space and time `r anonymize("[@latourReassemblingSocialIntroduction2005; @pelizzaCommunityTextBack2010]")`. In line with the type of research questions this method is expected to answer,[^above] we choose comparison between data models used by different authorities in different countries. For example, through a comparison of two data models run by different Member States it may become clear that one stores information about family ties while the other does not.
* **Granularity**: the granularity of categories of data and their attributes should be made visible by the method. For example, one data model may have a category "nationality," while another may be more specific and distinguish between "nationality at birth" and "current nationality." Or, there may be differences in the set of values allowed for a category such as "education level."
* **Associativity**: the method should make it possible to see how things are linked, which may allow them to be treated as a group. For example, the members of a family may be registered separately, but bound together as a group by a common identifier. Or, flight details may be stored for a person which would make it possible to implicitly associate people arriving by the same flight.
* **Multiplicity and convergence**: there may exist multiple instances of the same entity. The method should make these instances visible to allow further analysis on how they are kept consistent and if/how they converge. For example, in the course of an asylum registration process a person may be registered in multiple systems of different organisations. Making this kind of information visible would allow further investigation into how multiple instances are linked and managed.
* **Alignment and separation**: alignment means to make visible how things are arranged in relation to each other. For example, there may be a temporal alignment between the categories of data "date of entry" and "date of exit." Separation on the other hand means making visible how entities maintain separate identities. For example, a travel document and a person's name may be stored as two separate entities in an information system.
* **Control**: due to the constraints of data models, the full complexity of an entity can usually not be captured. However, some control may be allowed for people doing the data entry to adapt to their specific local conditions. For example, a simple category of "additional information" may allow for some flexibility to keep track of information which cannot be added into other categories.

These criteria have been used in the development of the method, and will be utilized in Section \@ref(potential-uses) to assess the effectiveness of our method in making visible information systems' ontologies and data models. The next section elaborates on the specific information systems and data models examined during the development and testing of the method.

[^above]: See note 1 above.


### The visualization of people at the border {-}

Technologies and digital infrastructures for making people at the border visible are a frequent concern of studies on migration and border control [see e.g. @tazzioliSightMigrationGovernmentality2016; @dijstelbloemSurveillanceSeaTransactional2017; @follisVisionTransterritoryBorders2017; @plajasKnowingRomaVisual2019]. On account of the fact that these technologies aim to make such phenomena visible, they are frequently understood as "technologies of vision." This definition draws on the work by Donna Haraway [-@harawaySituatedKnowledgesScience1988], according to whom vision is always particular, partial, embodied. Situated vision at the border is consequently not passive, but actively constitutive of the meanings given to border crossers' identities.

The situated vision of technologies deployed at the border therefore enact the things it aims to represent. For example, the imageries of maps, flows of people and border crossings that underpin the infrastructures for monitoring and controlling do not just make these things visible, but are in fact co-constitutive of what borders and border crossings mean [@vanreekumDrawingLinesEnacting2017]. Furthermore, this vision shapes the immediate and future responses of authorities [@tazzioliSpyTrackArchive2018]. In the Eurosur and Jura systems analysed by Tazzioli, for example, the systems do not just monitor and detect migrant movement; collected data are also used to prevent future movements along those routes. These technologies enact migrants in different ways: as people that need that to be stopped or as people to be saved in order to prevent new disasters in the Mediterranean.

Literature on the securitization of border control further shows that technologies of vision are constitutive of the meanings of borders and border control. For example, Bigo [-@bigoSecuritizationPracticesThree2014] pieced together three kinds of meanings actors gave to the border practices of border control: as "solid," something to be defended and linked to sovereignty; as "liquid," relating to how borders can be controlled through filtering and identifying populations; as "cloudy," only visible through the use of digital databases and analytics. In the three meanings described by Bigo, the practices and uses of visual technologies by the actors, in turn, informed the meanings they attribute to border control. So on the one hand, new technologies of vision "alter the nature of national borders" [@follisVisionTransterritoryBorders2017]. On the other, meanings attributed to borders and border control shape the kinds of technologies that are developed and employed.

Database technologies in particular are one technology in the digital infrastructures often declared to make people at the border visible, by allowing states to collect different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g. @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. However, these examples of research on technologies on the alleged invisibility of people have a tendency to adopt authorities' perspective. Less attention has been given to the invisibility of those same infrastructures that allow the informational management of mobility, migration and border control.

First and foremost, these features correspond to the semantic classifications used to identify people, the ontologies or data models implemented in the databases.[^ontology] Databases utilized at the border support authorities in collecting different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g. @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. In this way, databases institute certain kinds of visions; yet the embedded politics and design choices can easily become invisible. We might thus ask how we can retrace knowledge representation choices, in order to comparatively understand how diverse systems — run by different authorities — enact people.

Despite their topicality and insightful conclusions, research on sociotechnologies of bordering and securitization has a tendency to focus mainly on the (in)visibility of people. Less attention has been given to the invisibility of those same infrastructures that allow the informational management of mobility, migration and borders. Yet if we take the situatedness of vision seriously, then we cannot fully understand how people are enacted if we do not consider the features of the systems that enact them.

Yet, visualizing the infrastructure itself is needed, as infrastructure has a major role in how people are not only represented, but enacted. We cannot fully understand how people are enacted if we do not take into account the features of the systems that enact them. The data models in these databases institute certain kinds of visions, yet the politics and design choices can easily become invisible. We might thus ask how we can retrace the choices made in how specific forms of knowledge are represented, in order to better understand the systems and how they enact people.

### Classification systems {-}

> "[R]epresentational practices such as information requirements analysis, data modelling and the like, are conceived and employed as technologies of control. They are to be understood as efforts at 'worldmaking' ... as attempts to institute particular versions of the organization, its members and their activities. Such representational practices create a 'presence' for a particular set of 'relevant' facts, defining their range of possibilities and rendering them visible to the participants in the organizing process ..." [@bloomfieldVisionsOrganizationOrganizations1997, p. 641].

<!-- > "One answer is to think about the category as a device that travels and mediates the relationship between individuals and states, between everyday practices of classification and authoritative state classifications." [@ruppertCategory2014, p. 36]. -->

<!-- They term the choices made during the development as the practical politics for looking at categories and standards as technologies. -->

The origins and effects of standards and data models that are entwined in technologies and practices have been studied extensively [@bloomfieldVisionsOrganizationOrganizations1997; @bowkerSortingThingsOut1999; @lamplandStandardsTheirStories2009; @buschStandardsRecipesReality2011; @timmermansWorldStandardsNot2010]. As the quote from Bloomfield and Vurdubakis tells us, data models created to represent some phenomena can actually be considered as a technology that enact particular kinds of knowledge, organisations and practices. In the influential book "Sorting things out", @bowkerSortingThingsOut1999 collect a great deal of information on the importance of classification systems or standards and usage by states and other organisations to coordinate work related to the development of classification systems.  The politics in the processes of designing these conceptualisations shape how information is represented and affect what becomes visible and consequently, what turns invisible. But, as they point out, the choices that were made during this process can easily be lost since only what is in the systems is usually what is (best) documented. Studies on standards, classification systems, or data models have shown that they institute certain kinds of ways of working. The question remains how we can retrace what and how specific forms of knowledge are represented.

To retrace what might be lost or made invisible in the development of data models and standards we can compare different standards for the similar phenomena. Cornford et al. [@cornfordRepresentingFamilyHow2013] for example have compared digital standards that have been developed as public service initiatives in the United Kingdom for representing of family relationships. Their comparison highlights "the kinds of family relationships that are recorded and those that are not recorded or harder to record, any hierarchies, implicit or explicit, for family forms or relationships and the implicit and explicit assumptions that underlie the terms and classifications used" (p. 8). In our case this means comparing data models for representing persons that are embedded in information systems for migration and border control.

Data models and standards cannot always be compared separately, as often it is only through the interplay between different standards that subjects are enacted as new kinds of entities. @ruppertNotJustAnother2013 for example has studied information systems in the UK for sharing data on young persons who are at risk of (re-)offending, and for allowing interventions. In her study she shows how the "joining up" of data stored, which includes various biographical information from different agencies, produces a "subject multiple". Drawing on the work of Jane Bennet she sees the consequence of connected data as then not just from a sum of the individual parts, but that is a part a becoming of something else. In the case of border and migration control someone may for example only become an irregular migrant through connecting data from different databases.

### Database technologies and data practices in border and migration management {-}

The current, as well as the historical antecedents, of registration practices and use of database technology as tools to sort populations for controlling borders and migration has been widely researched, [@lyonSurveillanceSocialSorting2003;  @torpeyInventionPassportSurveillance2000; @ansorgeIdentifySortHow2016]. In his book on "The invention of the passport", @torpeyInventionPassportSurveillance2000 for example recounts the long historical developments of how states came to establish their imperative authority to control movements of people. He stresses the role of documents such as the passport and related practices to register, identify and track people and their movements. With people moving between different territorial authorities, authorities introduced different forms of documentation and registration to distinguish between persons that are allowed access to and rights within the territory.

Recently, European states especially have aimed to define and control their wanted and unwanted migration [@balchDevelopmentEUMigration2011; @broedersNewDigitalBorders2007]. @balchDevelopmentEUMigration2011 for example have traced development of EU policy for a migration and asylum system. From their analysis they argue that it difficult to speak of a common EU policy in this area. And that the evolving institutional setting has been decisive in shaping the kind of migration and asylum policies. They note that it has been easier for EU Member States to cooperate on reducing irregular migration than on creating a framework for legal migration. Which according to them can also be seen in the EU agencies and information systems that have been designed to fight cross-border crime and irregular migration, or, as they say, there is a "political appetite for technological solutions to the perceived problems of irregular migration at the EU level" (p. 28). A major focus of European authorities has therefore been to identify irregular migration through the use of new technological solutions such as databases.

Different authors have analysed these development of the use of new information technology as a sort of "datafication of migration management" [@dijstelbloemMigrationNewTechnological2011; @broedersDataficationMobilityMigration2016; @leursDataficationDiscrimination2017; @bestersGreedyInformationTechnology2010]. The creation of databases in Europe to store data about people and their movements in this view are seen as an important element for a kind of sorting of populations for different treatments. The risk however is that such a view portrays databases as mere tools to obtain information on people and their movements. This theorisation of surveilling populations by using database technologies is furthermore linked to work in studies on surveillance and policing  of borders [@clarkeInformationTechnologyDataveillance1988; @lyonSurveillanceSocialSorting2003; @zureikGlobalSurveillancePolicing2005]. Surveillance of borders and the people who cross them has been analysed through mechanisms of data collection and processing, frequently described under the portmanteau "dataveillance" as theories about the processes and technologies used to collect and reveal these phenomena through data.

These data collection practices and profiling of people for surveillance is sometimes figured as a panoptic system, which is not not adequate. On the one hand such a system would aim for migrants to internalise the gaze of the watcher and become a mechanism for disciplining behaviour. As @broedersDataficationMobilityMigration2016 reminds us in his article such methods of gaining knowledge about migrants are not exactly about disciplining. Rather surveillance of migrants lives as methods for gaining information on irregular migration to enable expulsion. This argument resembles theorisation of "surveillance as social sorting" [@lyonSurveillanceSocialSorting2003]. In this view profiling of migrant identities and creation of risk categories would be informed by desires to regulate and control populations.

<!-- 
and the concept of the Panopticon 
The figuration of this data collection as a panoptic surveillance system is however not adequate.
-->

Database technologies has played an important role for states to surveil and control migration through data collection practices, allowing the collecting of different kinds of biographic and biometric data to make people and bodies "legible". At the same time these technologies also have opened possibilities for ideas of sorting populations for different treatments. According to @vanreekumDrawingLinesEnacting2017 the infrastructure developed for monitoring and controlling persons and border crossings can be understood as forms of visualization that make these phenomena known or visible. In their view the schematised and abstract imageries of maps, flows, and border crossings that underpin the technologies and practices do not just make these things visible, but are in fact co-constitutive what borders and crossings are. And to make such phenomena visible van Reekum and Schinkel emphasize the role of documents and categories to render visible and infer borders and border crossings.

Theories of "datafication" and "dataveillance" have been very productive to understand the data collection practices and technologies for border control. The database technologies and concepts however often assume that there exists such a priori categories of persons such as irregular migrant that can be identified and sorted for different treatment. As others have highlighted [@potzschEmergenceIBorderBordering2015; @pelizzaProcessingAlterityEnacting2020], such approaches do not take into account how such practices are constitutive of types of subjectivities they purport to process:

> "[D]istinctions between citizen and migrant or between trusted traveller and terrorist threat do not emerge as a priori givens that necessitate certain practices, procedures, and regimes of profiling, tracking, sorting, and filtering, but the division itself becomes conceivable as the contingent result of these formalized operations"[@potzschEmergenceIBorderBordering2015, p. 103]

<!-- ## Data practices -->

Recently there is growing interest in more fine-grained analyses of knowledge production mechanisms and practices of digital technologies in border and migration management and how they enact amongst other populations and states [@mcharekTopologiesRaceDoing2014; @jeandesbozSmarteningBorderSecurity2016; @glouftsiosGoverningCirculationTechnology2018]. Contributions from two recent Special Issues [@cakiciPeoplingEuropeData2020; @scheelEnactingMigrationData2019] are for example taking a practice based approach to analyse data practices and pay attention to the mechanisms through which knowledge enacts migration. And through the concept of "alterity processing", @pelizzaProcessingAlterityEnacting2020 has addressed the multiplicity of ways in which non-European populations are enacted when reaching Europe, and their simultaneous enactment with institutions. These authors question the kind of worlds the data practices help to make and engage with ontological politics to think how they could be enacted differently [@lawEnactingSocial2004].

Moving from theories on the datafication of migration management to focusing on situated data practices using concepts from practice theory still leaves some things unresolved. Rather than technologies, people or other actors, practice theories takes social practices as the central topic of enquiry. And then look at the material arrangements in which these practices are enacted. Theories of social practices have in this way highlighted how things and practices are intertwined and constitute each other [@schatzkiMaterialitySocialLife2010]. While practice theory has been very productive to understand the situatedness of practices, there are still questions about how social practices travel, how different practices are related to each other and other "large phenomena" [@shoveMattersPractice2016; @nicoliniSmallOnlyBeautiful2016].

> "But if action is dislocal, it does not pertain to any specific site; it is distributed, variegated, multiple, dislocated and remains a puzzle for the analysts as well as for the actors." [@latourReassemblingSocialIntroduction2005, p. 60]

As Latour reminds us, action/practices are not entirely local. In this article we therefore propose to concentrate on the role of data standards and models in the infrastructures of border and migration control. We propose to analyse and compare standards from different sites that are assembled at Greek border zones. Information technology and standards in this sense have become powerful methods to trace and connect sites [@latourReassemblingSocialIntroduction2005]. Our view is rooted in studies on the world making effects of data models, standards and the like. And we see these technologies as continuously involved in processes enacting populations, territories, and borders. In the next section we review some of this work on classifications and their consequences.

### Data matching in security settings {-}

Finally, similar arguments have been raised in fields such as international relations and critical security studies to take serious materiality and performativity of devices of security and their role in international politics [see e.g., @amicelleQuestioningSecurityDevices2015; @suchmanTrackingTargetingSociotechnologies2017; @hoijtinkTechnologyAgencyInternational2019]. @bellanovaControllingSchengenInformation2020 for instance have studied the actors and practices involved in the processes of maintaining the EU Schengen Information System (SIS). The SIS system allow authorities to create and consult alerts on, among other, missing persons and on persons related to criminal offences. By looking at how these alerts "acquire the status of allegedly credible and accurate information that becomes available to end-users through the SIS II" (p. 2) they make evident its role in conditioning international mobility. In addition, @leeseConfiguringWarfareAutomation2019 has used Suchman's concept of configuration to examine how the design and engineering practices of autonomous weapons enact boundaries between humans and machines, delineating each's capacity to act. Therefore, by taking serious the role of socio-technologies in practices of identification allows us to account for the complex and often less visible organizational, and political consequences.

### The visibility of data management work

The previous subsection deliberately describes more technical details of data matching. By doing this, I attempted to emphasize how, following @dignazioDataFeminism2020, "steps like cleaning and wrangling data are presented as solely technical conundrums" and that "there is less discussion of the social context, ethics, values, or politics of data." (p. 66) It is by now well-known that data are never "raw," but always constituted by different choices and constraints [@bowkerSortingThingsOut1999; @gitelmanRawDataOxymoron2013]. Techniques and practices of managing and cleaning data are part of the cooking.

The impact of new information technologies on the workplace has been the subject of much debate [e.g. @bowkerSocialScienceTechnical1997; @leonardiMaterialityOrganizingSocial2012; @orlikowskiSociomaterialityChallengingSeparation2008; @pollockSoftwareOrganisationsBiography2009]. For instance, categories of data in technical systems that are supposed to represent organizational facts can become naturalized and reorder organizational activities [@alaimoManagingDataAlgorithmic2021]. Different theories and framework have been introduced to examine and theorize about their (in)ability of transforming the organization and the coordination of work [@pollockSoftwareOrganisationsBiography2009]. On one hand, some scholars have examined the introduction of information technology as a force of organization change [e.g., @kallinikosWorkControlComputation2009]. While others have critiqued such views and focus instead on the struggles encountered by actors of fitting global software to local circumstances [e.g., @hansethDevelopingInformationInfrastructure1996].

Social studies of data have further shed light on the ways in which data is generated, circulated and deployed [@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. In this way they have shown how data are a form of power that can be deployed to serve certain interests and goals of people and institutions. Data moreover embed particular values and conceptions of the world that play a role in structuring the social order [@bowkerSortingThingsOut1999]. So far, however, there has been little discussion about socio-technologies used for making data useable for others.

Additionally, social constructivists have emphasized how the construction of technologies is tentative and shaped by specific contexts and actors in which the artefact emerges [@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. Concepts such as interpretative flexibility [@pinchSocialConstructionFacts1984] and boundary objects [@starInstitutionalEcologyTranslations1989] sensitize scholars to how different social group attribute different meanings to artefacts and how they put them to use. This scholarship has further influenced organization studies to blur the boundaries between social and material and instead see "the constitutive entanglement of the social and the material in everyday organizational life" [@orlikowskiSociomaterialPracticesExploring2007, p. 1438]. Certainly, these concepts are crucial to analyse how different actors perceive, use, configure the tools for searching and matching data.

The processes and practices of managing data can be considered as articulation work, which CSCW scholars have shown is crucial for collaborative work. In this view, it is the background or invisible work for making other things work smoothly. Such ideas are also evident in the metaphors and imaginaries surrounding the work of managing data; further shaping how this work is valued and made (in)visible. Metaphors and work titles used for the kind data management — such as "data cleaner" or "data janitor" — underline how such metaphors even have gender and class connotations. Infrastructure scholars in particular however have shown that what counts as work depends on the context and definitions in which it is done [@starLayersSilenceArenas1999]. Data management activities are therefore not inherently visible or invisible. Rather, technologies for using and managing data embed particular imaginaries and values that can make such work more or less visible.

### Separating the work of data management {-}

Managing and cleaning the data so that they can be used by others is often part of the back office; the part of an organization where administrative and logistical work is performed to perform service or products, but who do not deal directly with customers of that service or product. This back office work of data management closely relates to what @goffmanPresentationSelfEveryday1959 calls the "back stage" of social life. This metaphor from the world of theatre highlights the impression management of actors and institutions. A database should give the impression that it is accurate and complete for the people interacting with it. Nevertheless, the backstage work of cleaning messy data should likely not be visible to those users. Moreover, automation of such tasks and processes may start and be more prominent at the back office [@snellenBlurredPartitionsThicker1992].

A similar analogy applies in software parlance, where the concepts of front and back end of a computer system are used to indicate a "separation of concerns" between the presentation of the application that is accessible to the user (front end), and the back end that performs operations invisible to the user such as data access. This design criterion of separation of concerns is a well-establish and influential. It emphasizes a modular way of designing software by separating and encapsulating different functions of a system (i.e., "concerns"), as a type of "divide and conquer" strategy to manage the complexity of software development [@laplanteWhatEveryEngineer2007]. Yet not much reflection is given by engineers on the effects of such designs.

Of course, the distinctions between front and back office/stage/end depends on positioning. One person's front can be the back of another. But what these metaphors and analogies show evidence of is the often implicit assumptions about how this kind of work should be made (in)visible. Such assumptions are furthermore materialized and made durable in the actual systems that are created to support these tasks. In order to understand these effects we therefore need to pay close attention to the design and uses of the technological artefacts.

### Data quality and interoperability {-}

The integration, interoperation, or interoperability of data is a commonly sought-after goal of organizations and governments, to ensure that data can be shared, used, processed by different parties and information systems. Achieving such data interoperability however is a complicated endeavour that includes both changes to technical and organizational elements [@gil-garciaCollaborativeEGovernmentImpediments2007; @kubicekInteroperabilityGovernment2011; @pelizzaDiscipliningChangeDisplacing2016; @schollEgovernmentIntegrationInteroperability2007]. Technical changes might include introducing new standards that formalize, on one hand, the formats, or syntactic structures, for exchanging data. On the other hand, semantic standards and vocabularies can guarantee that the meaning of the content of exchanged data can be interpreted by other systems. In effect, data will need to of high quality so that they are both fit for the purposes for which they are elaborated, and re-usable for new purposes and by other actors [@floridiPhilosophyInformationQuality2014].[^data] Yet assessing the quality of data — how complete, accurate, consistent, etc. they are — is dependent and specific on the purpose and domain in which data are used [@illariIQPurposeDimensions2014].

Connecting multiple data sources therefore creates additional difficulties and uncertainties related to the quality of data, as well as opportunities to resolve such complications. In general, databases are never without mistakes. For example, the database of a webshop may have duplicates records for a customer which was mistakenly created, or a customer's address may no longer up to date because they moved. Organizations therefore need many data management activities for keeping their data current. Such data management activities include processes of detecting, correcting, and removing records that are identified as inaccurate, incomplete, or incorrect. Generally speaking, by integrating multiple data sources, new uncertainties may arise. For example, to identify if records of different databases refer to the same customer. Conversely, integrating multiple data sources can in many cases be used to complete and correct data from one source by combining and comparing it to others. In both cases the difficulty remains of identifying if two entities represented by data records from on or more sources actually refer to the same real world entities or not, and how they should be merged [@batiniObjectIdentification2016].

Data deduplication or duplicate identification is commonly used to refer to this problem of object identification when dealing with duplicate records from a data source. Generally, such a process would compare every record with every other record to score its likelihood of matching that record. The complexity of deduplication in such cases is therefore equal to the Cartesian product of records in the database, i.e. resulting in a computational complexity of $O(n^{2})$, where $n$ is the cardinality or number of elements the data set. If data sets are too big — and this becomes computationally expensive — the search space might need to be reduced [@batiniObjectIdentification2016]. After determining the set of possible matches, a domain expert generally needs to be involved to decide on the possible matches.

### EU information systems, smart borders package

In April 2016 the European Commission announced a new strategy for the EU information systems for security, border and migration management with the communication "Stronger and Smarter Information Systems for Borders and Security".[^smart-borders] This proposal provides a new vision that aspires to improve the operation systems to make better use of data that is already available so that, if they realize this vision, relevant authorities could do their work more effective and efficient. Having more and better data available is also identified as relevant to "support evidence-based policymaking on migration and security in the Union" with the introduction of new reporting and statistics. One way they envision the objectives of this proposal to be achieved and we want to scrutinize is through "improving and harmonizing data quality requirements of the respective EU information systems".

[^smart-borders]: https://ec.europa.eu/home-affairs/what-is-new/news/news/2016/20160406_3_en

As Sergio Sismondo [-@sismondoChapter13Rhetoric2011] reminds us, arguments that are made in technical writing can be persuasive. Different choices in how it texts are rhetorically constructed can have different effect. It is therefore interesting to look at how the arguments are made to be persuasive. Simply by looking at the headlines of communications for the interoperability project we can see the significance of rhetorical framing. We can see this for example from the use of the phrase on how the proposed solutions can help to find so-called “blind spots” or “information gaps”. This is an issue that is identified in the proposals as a data quality problem of missing links between data, and for which new technical components are put forward as solutions to this problem. Data quality then seems to be put forward as a requisite for making these information systems “stronger and smarter”. The importance of how this is then represented in the media can be seen from Figure \@ref{headlines}.

The academic research field on information systems has for a long time been attentive to how information technology mutually is developed and used and mutually shapes technology and organisations. They have also paid much attention to the importance of discourses and the roles in these play in these processes. Discourse analysis has been applied in information systems research to examine the ways discourses can shape planning and decision making in the development of such technical systems [@carstenstahlInformationSystemsCritical2008].

```{r headlines, echo=FALSE, fig.cap="Collage made by the author of news headlines from digital news media.", out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("figures/headlines.jpeg")
```

In the discourses for the interoperability project we can find different important moments on the planning of the systems. After the initial above mentioned communication by the European Commission, a “high-level expert group on information systems and interoperability” (HLEG) was set up by the Commission with a mission to help with "developing a joint strategy to make data management in the Union more effective and efficient, in full respect of data protection requirements, to better protect its external borders and to enhance its internal security". In May 2017 this HLEG concluded with a publication of their final report that includes recommendations for changes to the information systems and achieving interoperability between them. These were then taken up by the Commission who began work on a proposal for a legislation on interoperability.

To understand this project, it will be helpful to look at a high-level overview of how the current information systems broadly work. These information systems are: the Schengen Information System (SIS), the Visa Information System (VIS), and Eurodac. The SIS supports external border control and law enforcement cooperation in the European Union. It supports this task by storing alerts which contain information on persons and objects, and what to do when they are found. Eurodac on the other hand is concerned with identifying asylum seekers and determining the responsibility between member states for processing asylum applications. For doing this identification it stores fingerprint data of asylum seekers and third-country nationals. Lastly, the VIS allows for exchange of visa information data (including personal data and biometrics) and in this way aims to maintain a common EU visa policy [^is-overview].

[^is-overview]: See for example Alex Balch and Andrew Geddes [-@balchDevelopmentEUMigration2011] for an overview of some major developments.

All three information systems are based a very similar architecture of national Member State systems that connect to a central database. Figure \@ref{fig:vis-architecture} gives a high-level architectural overview of how these current systems are broadly structured. The visa information system for example comprises a central system with a database containing biographic and biometric data. Each member state has its own National Interface (NI) that they connect to the central system through a formal interface that is specified in an Interface Control Document (ICD). An interface in this way defines how the data flows between the systems, for example by defining what types of data needs to be sent to do a look-up of a person for their visa data. This type of architecture is approximately used for each information systems. The picture becomes more complicated when we also take into account the differences between how a member states systems work and what data they capture. The systems might be different for the members states, but each has to send data to the central system through the same interface which can explain some difficulties with data quality.

```{r vis-architecture, echo=FALSE, fig.cap='Visa Information System High Level Architecture.', out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("figures/vis-architecture.png")
```

The interoperability framework that is proposed has the overall goal of making all the information systems work together and in this way also has consequences for the current architectures. The previously mentioned HLEG proposed several additional new components to the current architecture which they consider to make it possible to achieve the interoperability of these systems: a European Search Portal (ESP), a Shared Biometric Matching Service (sBMS), a Common Identity Repository (CIR), and a Multiple-Identity Detector (MID). In short, we can say that the ESP would be an intermediary between the different information systems that allows for querying off all the information systems at once through a single interface, the sBMS is a technical component that would allow automated comparing of biometric data from different systems, the CIR would change the storing of data from the individual systems to a shared database, and the MID would allow for automatic detection of multiple identities with the same identity data. Figure @\ref(fig:interoperability) shows the changes in this architecture.

```{r interoperability, echo=FALSE, fig.cap='Proposed Architecture for Interoperability.', out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("figures/interoperability-architecture.png")
```

These new technologies are put forward as improving data quality. The question becomes how are they discussed in relation to data quality. What kind of discursive strategies are used, on does this enact particular kinds of solutions? Traditional debates in philosophy of technology argue on the role of technologies. On one end up the spectrum we can find the instrumental position, which sees the use of technology in societies as a neutral tool. In this view technology can have no considerable impact on changes in society. While on the other end we can find substantivists who claim that technological development follows its own logic and with new developments also alters societies [@waelbersDoingGoodTechnologies2011, p. 12; @verbeekWhatThingsPhilosophical2005, p. 135]. More recent research in what is now known as the field of Science, and Technology Studies on the other hand have complicated these views through various empirical case studies that show for example the various involved and how technology and society are co-produced.

### Data Quality dimensions {-}

As data quality can be defined in many different ways, we also analyse the discourse for different aspects of data quality. These codings are based on the different well established data quality dimensions defined by Batini et al. [@batiniDataQualityDimensions2016]. The dimensions they define are the following (p. 23):

1. Accuracy: how does the information adhere to something in the world.

2. Completeness: the extend to which it actually adhered to this real world entity.

3. Redundancy: is the information concise, minimum required.

4. Readability: how easy is it to understand the information by users.

5. Accessibility: related to the ability of the user to access information.

6. Consistency: does it comply with different constraints and rules.

7. Usefulness: what are benefits of use of information.

8. Trust: does it arrive from authoritative source that can be trusted.

Using the atlas.ti tools we coded sentences in our collection of documents for mentions of those word, as they may refer to qualities of data. The auto coding function was used again together with a search using regular expression. To look for example for variations of term "accura*", such as accurate, accuracy, etc. The results of this coding can be found in the following table. The frequency for each dimension can be found, along with one example quotation from the documents. Note however that no filter has been done so far in order to remove incorrect quotations. Each quote still needs to be verified if it actually concerns about the data and or systems. There may also be different variations for the same, which still need to coded (eg. following the dimensions, accuracy and correctness in the same cluster).

<!-- \begin{table}
\begin{tabu} to \textwidth {l|l|X}
\small
\textbf{Dimension} & \textbf{Frequency}&  \textbf{Example quotation} \\
\hline
Accuracy & 117 & "Data stored in information systems may not always be accurate and therefore not always reliable." (1:50) \\
Consistency & 78  & "Therefore, the goal of such a data quality control mechanism will be for the central systems to automatically identify apparently incorrect or inconsistent data submissions so that the originating Member State is able to verify the data and carry out any necessary remedial actions." (1:68)\\
Accessibility & 53 & "A reliable, more accessible and easier identification could also contribute to ensuring that the right to asylum (Article 18 of the Charter) and the prohibition of refoulement (Article 19 of the Charter) are effectively ensured." (11:24) \\
Trust & 22 & "The biographical identity data in the proposed ECRIS-TCN system will be much more trustworthy as it will be established during thorough judicial procedures and data exchanges" (12:2)\\
Readability & 7 & "Messages that are sent through the VIS Mail are not in a human readable format; " \\
Usefulness & 6 & "The CIR would be most useful if fully populated and having a good ranking of answers, and also if the user can use the biometric data of the hit to perform further examinations." (3:112) \\
Redundancy & 5 & "The NI is redundant using a Local National Interface (LNI) and a Back-up National Interface (BLNI)" (2:34) \\
\end{tabu}
\caption{\label{ref_table_dimension}Overview of the data quality dimensions, frequency of use, and an example of a quotation.}
\end{table} -->