# Matching identity data in transnational security infrastructures and beyond {#ch-literature}

## Literature 1 — data and classification studies

## Literature 2 — collaborative work in information infrastructures

## Literature 3 — nexus of user-designer-technology

## A conceptual framework for analysing data matching in distributed settings

Following the overview of existing work and the gaps in their methods, I now put forward a conceptual framework for tracing data matching practices and technologies in transnational security data infrastructures. Such a framework is needed since no single method or theory can be adapted to examine the multiple facets of data matching practices and technologies within and across organizations. In this section, I therefore propose a new conceptual framework that amalgamates established methods and theories into one framework that can guide the research in examining those multiple facets. The conceptual framework, in essence, integrates more computer science views on linking and matching data models and records, with social science views from fields associated with Science and Technology Studies (STS).

As previously said, for government agencies, business, and other organizations the tasks of finding records in databases that refer to the same person can be important in many situations. As organizations and agencies have been collecting ever more data about people, they may perceive performance benefits of having complete and accurate information about people, or even see it as beneficial to national security. The techniques for data matching — such as the mathematical models for linking records — are therefore well established. However, the creation, development, and consequences of deploying technologies for data matching are less well understood and researched. With the framework that I outline in this section I aim to address this gap in the research. I propose to trace data matching practices and technologies as sociotechnical phenomena, and from four different angles: (1) how the kinds of data that are collected about people can be compared and what this tells us about how organizations can search, match, and use this data, (2) how within organizations search and match for data about people that may have data quality issues, (3) how such identity data is matched between organizations, and (4) how, simultaneously with matching identity data, data matching technologies and practices travel and evolve across organizations.

To discuss these different aspects of analysing the matching of identity data, I propose to visualize the framework through several axes (Figure \@ref(fig:method-axes-visualization)). This visualization roughly follows a relational model to represent data as tables that model relationships between data. The three axes capture different dimensions of data, which are: _data models_, _categories of data_, and _data values_. Next, I will describe how each of these three axes and how different relations between the axes correspond to specific aspects of data matching within and across organizations.

```{r method-axes-visualization, echo=FALSE, fig.cap="A visualization of the different aspects of data matching examined through the framework.", out.width="100%", fig.asp=.5, fig.align="center"}
knitr::include_graphics("figures/method-axes-visualization.pdf")
```

To begin, the axis for _data models_ (axis `z`) represents the abstract models that standardize the kinds of information about people collected by different organizations. An example of such a data model could be the schemas specifying data collected about migrants by a government agency. Each organization generally defines its own data model. However, some data models might even be shared to make interfacing between systems possible. Next, each data model specifies _categories of data_ (axis `x`) which are collected about people. For example, the categories of data of the example asylum agency's data model may include "name", "place of birth", "date of birth". Finally, there are the actual _data values_ (axis `y`) in systems' databases for a data model and its categories of data. For example, a value for the category of data "place of birth" may be "Brussels".

That being so, I propose to schematize different facets of data matching as combinations of pairs of these axes. In Figure \@ref(fig:method-axes-visualization), three axes each zoom in on specific facets of data matching within and across organizations.

### Matching data models

The first pair _(data models `z`, categories of data `x`)_ represents the identification of semantic similarities and differences between different data models and their categories of data. Such a process will also be used in the research approach to support discursive analyses of data models. In this way, the analysis intends to compare the expectations and imaginaries of diverse actors concerning data collected about people and the ways data can be searched, matched, used. For example, the data model of a government agency may contain the category of data "family name" while another agency uses "surname" as category of data. Even though the name of the categories of data are distinct, they clearly refer to a similar concept. In this case, a part of someone's personal name that indicates their family. To connect information about people from these two agencies'systems should therefore identify similarities between their data models and these two categories of data.

This first aspect of matching people's identity data that is addressed by the conceptual framework is therefore the relations between different information systems' data models. That is, how different data models for collecting information about people-on-the-move are situated in relation to one another, and what this tell about the organizations which developed and make use of these models to collect, search, and match identities.

To do this analysis, the method draws, first, on studies of classification and its consequences which have addressed the question of how to retrace the ethical and political work of otherwise mundane devices of representations — i.e., data models. Researchers have previously used and combined various makeshift methods — from narrative interviews [@gazanImposingStructuresNarrative2005] to discursive textual analysis [@caswellUsingClassificationConvict2012], from participant observation [@meershoekConstructionEthnicDifferences2011] to archival and genealogical research [@gassonGenealogicalStudyBoundaryspanning2006] — for the ethnographic and historical studies of information systems [@starStepsEcologyInfrastructure1996] and their classifications. Furthermore, it is altogether necessary to take a more interdisciplinary approach and also pay close attention to the technical details of classification, as to avoid only considering the effects of classification [@kitchinCodeSpaceSoftware2011].

Following Geiger and Ribes [-@geigerTraceEthnographyFollowing2011], data models can be considered 'thin' traces: being rather standardized schemas made of categories and values, they are hardly meaningful in themselves. The method to analyse them therefore should consider how to turn them into 'thick' data. Nevertheless, given their implementation in diverse infrastructures, data models can be an excellent starting point to understand how geographically distributed sites are connected [@latourReassemblingSocialIntroduction2005]. Burns and Wark [@burnsWhereDatabaseDigital2020], for example, have dubbed such approach "database ethnography" and used traces left behind of a database as a site to analyse how social meanings of phenomena change over time — but, only using their own ad-hoc mix of methods.

On the other hand, matching data models shares concerns with methods developed in computer science fields interested in the use of semantic technologies, such as knowledge engineering, linked data, and natural language processing. For example, the relationship between the different types of categories and their variations is closely linked to the outcome of processes known as schema or ontology matching, which finds correspondences between concepts of schemas or ontologies [@euzenatOntologyMatching2007; @kementsietsidisSchemaMatching2009]. Another linked technique called ontology learning aims to automate processes for creating ontologies. These (semi-)automatic mechanisms then include extracting concepts and relationships between concepts from a domain of discourse by analysing a corpus of documents in that domain. There are thus shared concerns about how to examine the way different knowledge representations interrelate, and about how to recover such representations from a discursive domain. Yet, there are fundamental differences which make the methods hard to effectively support a discursive analysis.

Methods of knowledge engineering are generally more concerned with creating technologies that help machines understand better means for the systems, and with integrating different sources of information work together. By contrast, the method for this research needs to support discursive analysis by taking differences in representations as a starting point to analyse imaginaries of diverse organizations and authorities and to combine the analysis with ethnographic observation. To address this aspect of data matching there is therefore a need for approaches that can support analyses of formalized data models in two ways. First, to support analyses of information systems which define their own data models, even if these systems are not immediately comparable. Second, to systematically and quantitatively support discursive analysis of 'thin' data models, also by detecting differences and absences between systems.

### Data matching within organizations

The second pair _(categories of data `x`, data values `y`)_ relates to the categories of data and their actual values in databases, which may not always correspond to the expectations of the data models. For example, when values for the categories "first name" and "last name" are mistakenly switched in the database. Practices and technologies may therefore be employed to remediate such data quality issues — i.e., data matching. Overall, two applications of data matching within organizations and their databases can be distinguished: real-time data matching and deduplication. Real-time data matching mainly addresses the issue of retrieving information about a person through search queries. For example, police officers may need query databases based on the categories of data "name", "nationality", "date of birth" and to see if approximate matches exist for those personal details. Data matching techniques will therefore be used to find similar matching identities' data, rather than only exact matches. Deduplication, on the other hand, is the use of data matching technologies for identifying records in a database that refer to the same person and fusing the multiple records.

On one hand, it is crucial to understand the moments when actual data are not aligned with the expected data model. For example, when multiple records were created for a person due to name variations. In this way, the relation between data expected by data models and actual data in databases can draw attention to how organizations collect and use knowledge about people-on-the-move. For example, how they choose to take up technologies for searching and matching to help deal with data uncertainties deriving from frictions between the moments of collection and use of data. On the other hand, the design and use of these tools for dealing with such data matching need scrutiny. The design and use of such data matching tools can be compared to understand the relations between users, designers, and technologies. Such comparisons can identify expectations and imaginaries of designers that are inscribed into technological artefacts and how they are all configured together.

Scholars in fields of STS and beyond have devised different approaches to examine and understand the relations between users and technologies [@oudshoornHowUsersMatter2003], and how expectations and imaginaries of designers can be inscribed into technological artefacts. For example, the approach of script analysis was advanced to examine the assumed competences of users and affordances that are embedded in artefacts [@akrichSummaryConvenientVocabulary1992; @latourWhereAreMissing1992]. The script of an artefact then requires users to adopt designers' envisaged behaviours and actions to interact with an artefact. This approach also allows accounting for situations when those assumptions by artefact designers don't match the actual uses and practices. @woolgarConfiguringUserCase1990 furthermore noted how system designers might attempt to "configure the user" of the system by incorporating the user into the sociotechnical system. The method to understand such configuration is by treating the computer systems as a text that is read by users and can be interpreted differently. Designers may thus attempt to anticipate and delimit this flexibility. In these views, a sociotechnical system functions well when users and the system are successfully configured together.

Suchman [-@suchmanHumanmachineReconfigurationsPlans2007] furthermore argued that the inscriptions of users and uses are never that coherent, and that a more open-ended and indeterminate approach towards artefacts is required. She proposes configuration as a "method assemblage" that pays particular attention to technologies and "the imaginaries and materialities that they join together" [@suchmanConfiguration2014, p. 48]. Configuration as a methodological device can thus be, in accordance with Suchman's view, useful to unpack the material and discursive elements of software for dealing with data uncertainties.

This axis in the conceptual framework therefore aims to answer the question of what kinds of imaginaries and materialities are joined together in data matching technologies. Analysing and comparing the technical designs with actual uses, and thus also where they mismatch, can give evidence of the various imaginaries, expectations, assumptions at play. Yet, the approach also needs be both specific to the situatedness of the design, and keep an open-ended and indeterminate view of how such technologies are put to use [@suchmanConfiguration2014].

### Data matching across organizations

Finally, the pair _(data models `z`, data values `y`)_ addresses data matching across organizations and agencies. Identity data records that refer to the same person may exist in systems and databases of different organizations and government agencies. Data matching in this case refers to the tasks related to identifying and potentially linking or merging such identity data spread across various databases. A crucial challenge for data matching techniques is due to the fact that unique identifiers are not always available and, hence, that matching relies on personal information which is not always complete or accurate. Personal information such as names, dates of birth frequently contain typographical errors and variations. For example, a woman may have used her maiden name as surname in one system while information collected in another database uses her husband's surname after marriage. However, the question for us here is how — by bringing such data in relation with each other — data matching practices and technologies shape the relations between different actors.

Furthermore, these data matching practices and technologies cannot be studied independently of the infrastructures they are embedded in. Data matching technologies can be crucial to make certain collaborative work possible, such as the processing of an asylum application based on information collected about a person by different organizations and agencies. At the same time, the work of cleaning and managing data often can become invisible. Infrastructural studies have therefore provided different methodological strategies to make visible the distributed interconnections between technical minutiae and the politics of knowledge production [e.g. @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Investigating distributed infrastructures has thus been addressed by approaching infrastructures as a 'relational concept' [@starStepsEcologyInfrastructure1996]. Such relational approaches maintain that infrastructures are not objects of study by itself, but show preference for ethnographic methods to study how infrastructures emerge through interactive processes and practices [@starEthnographyInfrastructure1999; @karastiStudyingInfrastructuringEthnographically2018]. Methods include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999; @latourReassemblingSocialIntroduction2005], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019].

Based on the insights from infrastructural studies, data matching technologies in fields of security can therefore become an inconspicuous element in transnational data infrastructures. This in line with current debates in fields of International Relations (IR) and Security Studies who have started to acknowledge the role technology international politics [@amicelleQuestioningSecurityDevices2015; @hoijtinkTechnologyAgencyInternational2019; @bellanovaAlgorithmicRegulationSecurity2020]. Crucially, these scholars extend agency to the technologies and artefacts used in security practices. Glouftsios has, for example, shown how the often-invisible maintenance for the large-scale information systems for border security in the EU has a crucial role in sustaining the governance of international mobility [@glouftsiosGoverningBorderSecurity2020]. (more examples)

In the case of data matching technologies, the underlying assumption is that such technologies can trigger or even settle controversies about which actors produce more reliable data. For example, when matching identity data from different agencies these tools will bring certain records in relation with each other. If there are uncertainties about the data, they may then play a role in disputes about which agency's data are correct. The hypothesis is that data matching technologies can thus inscribe certain forms of agency that can reconfigure practices of security and migration in transnational data infrastructures. Infrastructural inversions are therefore key to calling attention to the background or invisible work of mundane data management practices.

### Travelling data matching software

Furthermore, and related to the last pair, is the question of how not only identity data are matched across systems, but also how technologies and practices for data matching travel across organizations. As companies develop tools to support these tasks and deploy them at multiple locations and times, the concomitant knowledge of how to manage typographic errors, name variations, missing data, and so forth may also travel across organizations. This axis is consequently related to how data are not only matched between different organizations and agencies in security contexts. Rather, the hypothesis is that, at the same time, knowledge and technologies for such data practices travel between organizations.

Although the notions of "script" and "configuring the user" have been generative in understand the relations between users and technologies, they tend to overemphasize the role of designers [@oudshoornHowUsersMatter2003]. More recent approaches have therefore suggested to, for example, take a more comprehensive approach to examine important moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. An alternative approach has therefore been developed by scholars called the 'Biography of Artefacts and Practices' that can detail the complexity of development of such technologies and related practices.

Following this approach, the emphasis of this aspect in the conceptual framework is to understand the biography of a data matching software packages. This approach is thought to highlight the wide range of actors involved in the development of such security technology and how the software travels and evolves over extended periods of time and between in different locales. This expands the narrower focus of the user-design-technology nexus used in the second axis to take a more comprehensive view of how the technology has been shaped over time, and by a broader number of actors at multiple sites.
