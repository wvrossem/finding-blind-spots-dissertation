# Matching identity data in transnational security infrastructures and beyond {#ch-literature}

\chaptermark{Matching identity data}

In this dissertation I deal with the multifaceted phenomena of matching people’s identity data, and how the development of technologies to make this possible shape and are shaped by transnational security infrastructures. Still, how can we get to know these facets of the technologies and practices involved, and how is what we know bound up with different methods and perspectives? This chapter develops a conceptual framework which integrates perspectives from primarily three strands of research into a visual tool to comprehensively analyse the matching of identity data in transnational security infrastructures. The chapter is composed of two parts. The first part introduces conceptualizations of systems of migration, border control as infrastructures and the well-known problem that infrastructures (and the technical choices made) tend to disappear. I therefore propose three bodies of literature that can each highlight different ways for making visible diverse facets of security infrastructures. There is a need for combining methods as most studies in the field have focused on either local interactions, or broad technical overviews.

Digital infrastructures can be both topics and resources of research of social inquiries [@marresDigitalSociologyReinvention2017]. Deciding of what an infrastructure is, the difficulty of demarcating where an infrastructure begins and ends, is often approached by studying an infrastructure ethnographically [@karastiStudyingInfrastructuringEthnographically2018; @starEthnographyInfrastructure1999]. An ethnographer reflexively approaches the infrastructure and reflexively constructs it ‘by every choice the ethnographer makes in selecting, connecting, and bounding the site and via the interactions through which s/he engages with the material artifacts and the people who define the field‘ [@blombergReflections25Years2013, p. 389]. This is one way of defining the boundaries, which puts a lot of responsibilities on the researcher. A different way is following the acts of speech and acts of doing (practices) as brought into the discourse by social actors. As @beaulieuResearchNoteColocation2010 notes, being present together with these actors at the same location just one method for following knowledge production. Following her proposal, other senses of ‘co-presence’ can be achieved through different traces of the mediations and infrastructures for identity matching.

Surprisingly, the matching of identity data in identification has not been closely examined in literature. There is a need to analyse the technical details of security infrastructures, such as the information representations of various systems and the mechanisms of linking identities between systems. The second section of this chapter develops the conceptual framework for analysing data matching in distributed settings. This framework aims to help analyse data matching based on the different perspectives from literature, and visualized in the framework along three axes: the matching of data models, data matching within organizations, and data matching across organizations. The next chapter will detail how this conceptual is operationalized to analyse the matching of identity data in EU and member states‘ systems.

## ‘Infrastructural inversions’ for matching indentity data

What methods and frameworks are best suited to trace the developments of matching of identity data in border and migration control? In recent years, there has been an increasing amount of literature on Europe’s technological systems of border, migration control, and security as infrastructures [@dijstelbloemBordersInfrastructureTechnopolitics2021; @potzschEmergenceIBorderBordering2015]. Such large technical systems or infrastructures have been a long-term concern for scholars studying the processes, actors, tensions in the long-term historical developments of technologies [@hansethDevelopingInformationInfrastructure1996; @hughesNetworksPowerElectrification1983; @starStepsEcologyInfrastructure1996]. In this way, existing research on border, migration control, and security as infrastructures recognizes the critical role played by underlying infrastructures in organizing and giving shape to large-scale phenomena. Key to infrastructural perspectives is to bring out how, for instance, data matching technologies would ‘consist of numerous systems, each with unique origins and goals, which are made to interoperate by means of standards, socket layers, social practices, norms, and individual behaviors that smooth out the connections among them’ [@borgmanKnowledgeInfrastructuresIntellectual2013, p. 5]. Conceptualizing the socio-technical systems to manage people’s identities as security infrastructures makes it possible to untangle the networks in these complex ‘ecologies’ [@starStepsEcologyInfrastructure1996].

A well known result of these overlapping between infrastructures and practices is that infrastructures have a tendency to be taken for granted and to ‘remain as invisible backdrops to social action’ [@harveyIntroductionInfrastructuralComplications2016a, p. 3]. Studies on infrastructures have therefore provided different strategies to invert this tendency of infrastructures to disappear, and to make visible the interconnections between technical minutiae and the politics of knowledge production [e.g. @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Methods proposed by these authors to invert the tendency of infrastructure to disappear include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019]. These kinds of ‘infrastructural inversions’ [@bowkerScienceRunInformation1994] makes it possible to acknowledge the links between technical systems for identity matching and the processes of identification in transnational security settings.

The case of the EU interoperability project is one example where tensions in the emergence and growth of infrastructure can make visible infrastructures that otherwise tend to disappear. The communication from the European Commission, for instance, remarked that the ‘EU’s architecture of data management for border control and security is marked by fragmentation [...] caused by the various institutional, legal and policy contexts in which the systems have been developed’ [@europeancommissionCommunicationCommissionEuropean2016, p. 3]. Similarly, the work of proto-infrastructure scholar Thomas P. Hughes [@hughesChapterReverseSalients1983] on the growth of the electrical supply network, was influential to show how the growth of systems is not ‘fore-ordained’ but evolves in a complex social and technical environment. Hughes showed how systems often arise out of the difficulties inside existing technical systems, or what he calls ‘critical problems’. Critical problems get identified when there are so called ‘reverse salients’, problems with specific components that hold back the overall system as it evolves to realize certain goals.[^reverse-salients]. Following the EC communication, the existing ‘architecture of data management’ is seen as a critical problem that holds back further developments for information systems at EU level.

[^reverse-salients]: His metaphorical concept imagines systems moving in a battle line towards a goal with specific components as inverted salients that hold back the overall development.

As @bowkerSortingThingsOut1999 have pointed out, choices made during design processes easily become invisible, as only what is in the systems is usually (and at best) documented. Infrastructural inversions can thus help understand the choices made in the various complex settings for such ‘architecture of data management’. This is crucial, as infrastructures connect different contexts to large-scale networks that bridge these differences to allow for exchanging and making use of information collected at various local situations [@dijstelbloemBordersInfrastructureTechnopolitics2021]. Understanding choices made in the EU information systems would make it possible to uncover tensions in linking people’s data from different systems. As scholars of digital infrastructures have shown, the networks that connect and shape relations between a multitude of actors such as individual, states, companies, technologies are often sources of tensions in the developments toward certain goals[@edwardsUnderstandingInfrastructureDynamics2007; @ribesLongNowInfrastructure2009].

Despite these insights on the invisibility of infrastructures, scholarship on security shows a tendency to primarily refer to the (in)visibility of humans. Less attention has been given to the invisibility of those data infrastructures that allow making human mobility, migration, and borders visible. This is a key aspect, though. The technical mechanisms used to link people’s data within and across are key for tracking people and detecting patterns. People on the move become visible through infrastructures that are not neutral, but enact them in certain ways [@pelizzaProcessingAlterityEnacting2019]. Passenger screening systems, for example, will try to identify passengers against police watchlists and detect suspicious patterns in someone’s travel history. The passenger date models and rules to detect such patterns embedded in the infrastructures can be conceptualized as ‘scripts’ through which security subjects are enacted.

Since the 1990s, the question of how to make visible infrastructures has primarily been addressed by approaching infrastructures as a ‘relational concept’ [@starStepsEcologyInfrastructure1996]: what is infrastructure for one person might be a topic for someone else. Such relational approaches maintain that infrastructures are not objects of study by itself, but show preference for ethnographic methods to study how infrastructures emerge through interactive processes and practices [@starEthnographyInfrastructure1999; @karastiStudyingInfrastructuringEthnographically2018]. Alternatively, an often cited article by @larkinPoliticsPoeticsInfrastructure2013 argues that there are degrees of visibility infrastructure. Large-scale infrastructures can also have a certain ‘aesthetics’, such as a state’s infrastructure project that is made prominently visible. In the case of EU systems, a similar example may be found in the grand project to make various EU systems interoperable to solve EU migration and security challenges. But I contend that this aspect is less important to understand the otherwise less noticed work done by security infrastructures for identity matching. I argue that there is, however, still room for other kinds of methodological contributions to further the analysis the technical minutae of matching identity data in transnational security infrastructures and beyond.

First and foremost, are the informational representations or semantic classifications used to identify people — i.e., the ontologies or data models implemented in the databases. Databases utilized at the border support authorities in collecting and linking different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g., @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. In this way, databases institute certain kinds of visions; yet the embedded politics and design choices can easily become invisible. We might thus ask how we can retrace knowledge representation choices, in order to comparatively understand how diverse systems — run by different authorities — enact people and how these diverse representation are matched.

### First inversion strategy: Informational representations and data matching

Data models and ontologies are entwined in digital infrastructures and practices, and institute certain ways of knowing and working [@bowkerSortingThingsOut1999; @hineDatabasesScientificInstruments2006; @lamplandStandardsTheirStories2009; @timmermansWorldStandardsNot2010]. Data models designed to represent phenomena can be considered as a technology that enact particular kinds of knowledge, organizations and practices [@bloomfieldVisionsOrganizationOrganizations1997]. The politics in the processes of designing these informational representations shape how information is represented and affect what becomes visible and consequently, what turns invisible. Applied to the field of border and migration management, this implies that data models do not only represent people at the border: they also make them knowable in specific ways. For example, family composition fields that can count up to five members assume the Western parental family model. Data models that foresee only female and male gender options assume migrants who are binarily sexualized. Even the choice of identifying people through fingerprints instead of study degree or family tree shrink the possibilities in the way they can be known [@pelizzaIdentificationTranslationArt2021].

The current, as well as the historical antecedents, of registration practices and use of database technology as tools to sort populations for controlling borders and migration has been widely researched, [@lyonSurveillanceSocialSorting2003; @torpeyInventionPassportSurveillance2000; @ansorgeIdentifySortHow2016]. In his book on ’The invention of the passport’, @torpeyInventionPassportSurveillance2000 for example recounts the long historical developments of how states came to establish their imperative authority to control movements of people. He stresses the role of documents such as the passport and related practices to register, identify and track people and their movements. With people moving between different territorial authorities, authorities introduced different forms of documentation and registration to distinguish between persons that are allowed access to and rights within the territory. These mundane artefacts such as passports and population registers do embed powerful data models to sort and control people.

More recently, database technologies have played an important role for states to surveil and control migration through data collection practices, allowing the collecting of different kinds of biographic and biometric data to make people and bodies ‘legible’. At the same time these technologies also have opened possibilities for ideas of sorting populations for different treatments. According to @vanreekumDrawingLinesEnacting2017, infrastructure developed for monitoring and controlling persons and border crossings can be understood as forms of visualization that make these phenomena known or visible. In their view, the schematized and abstract imageries of maps, flows, and border crossings that underpin the technologies and practices do not just make these things visible, but are in fact co-constitutive what borders and crossings are. And to make such phenomena visible van Reekum and Schinkel emphasize the role of documents and categories to render visible and infer borders and border crossings. What is less clear is how multifarious categories and representations collected about people, their movements are matched and linked together.

Database technologies are, as such, one technology in the infrastructures often declared to make people at the border visible, by allowing states to collect — and connect — different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g. @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. However, these examples of research on technologies on the alleged invisibility of people show a tendency to adopt authorities’ perspective. Less attention has been given to the invisibility of those information representation in infrastructures that allow the management of mobility, migration and border control. Therefore, what is less clear is how we can make visible these parts of the infrastructure to retrace what and how specific forms of knowledge are represented.

To retrace what might be lost or made invisible in the development of data models and standards we can compare different standards for similar phenomena. @cornfordRepresentingFamilyHow2013, for instance, have compared digital standards that have been developed as public service initiatives in the United Kingdom for representing of family relationships. Their comparison highlights ‘the kinds of family relationships that are recorded and those that are not recorded or harder to record, any hierarchies, implicit or explicit, for family forms or relationships and the implicit and explicit assumptions that underlie the terms and classifications used’ (p. 8). In our case this means comparing data models for representing persons that are embedded in different authorities’ information systems for migration and border control.

Data models and standards cannot always be compared separately, as often it is only through the interplay between different standards that subjects are enacted as new kinds of entities. @ruppertNotJustAnother2013, for example, has studied information systems in the UK for sharing data on young persons who are at risk of (re-)offending, and for allowing interventions. In her study she shows how the ‘joining up’ of data stored, which includes various biographical information from different agencies, produces a ‘subject multiple’. Drawing on the work of Jane Bennet, Ruppert sees the consequence of connected data as then not just from a sum of the individual parts, but that is a part a becoming of something else. In the case of border and migration control someone may for example only become an irregular migrant through connecting identity data from different databases.

The politics of data model design shapes how information is represented, and affects what becomes visible and, consequently, what becomes invisible [@hansethInscribingBehaviourInformation1997; @bowkerSortingThingsOut1999]. Discussing the invisibility of humans cannot avoid discussing the invisibility of those data infrastructures that define how they become visible. The first infrastructural strategy has therefore suggested that there is a need to examine infrastructures‘ informational representation and recover choice made in these technical standards that easily become invisible, yet are continuously involved in processes of enacting populations, territories, borders.

The rise of Big Data has prompted, among others, the field of Critical Data Studies (CDS) to renew the call to make visible other dimensions of data related to the volume, variety, and uses of data[@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. Such data practices are never entirely local and information technology, including standards, in this sense have become powerful methods to trace and connect sites [@latourReassemblingSocialIntroduction2005]. The next infrastructural inversion strategy will therefore propose to consider another important element to connect sites: the data practices related to data management and linking identities within and across organizations.

### Second inversion strategy: data practices

It is clear that data are never ‘raw’, but always constituted by different choices and constraints [@bowkerSortingThingsOut1999; @gitelmanRawDataOxymoron2013]. In addition, techniques and practices of managing and cleaning data should be analysed as part of the ‘cooking’. In recent years, there has been an increasing amount of literature that have shed light on the process in which data is generated, circulated, deployed [@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. Yet, scant attention has been paid to actual practices and techniques used to accomplish matching and linking data in security settings — such as probabilistic record linkage, fuzzy matching, data deduplication. This is surprising because by bringing different data into relation with each other, the tools recontextualize data and can have organizational consequences such as redistributing of tasks and responsibilities between different actors. There is therefore a clear need to understand how such technologies for matching identity data are put into practice.

Similarly, there is growing interest in more fine-grained analyses of knowledge production mechanisms and practices of digital technologies in border and migration management and how they enact, among other, populations and states [@mcharekTopologiesRaceDoing2014; @jeandesbozSmarteningBorderSecurity2016; @glouftsiosGoverningCirculationTechnology2018]. Contributions from two recent Special Issues [@cakiciPeoplingEuropeData2020; @scheelEnactingMigrationData2019], for instance, took a practice based approach to analyse data practices and pay attention to the mechanisms through which knowledge enacts migration. And through the concept of ‘alterity processing’, @pelizzaProcessingAlterityEnacting2019 has provided a framework to account for the multiplicity of ways in which non-European populations are enacted when reaching Europe, and their simultaneous enactment with institutions through data practices and infrastructures. As such, these different authors question the kind of worlds the data practices help to make — and potentially engage with ontological politics to think how they could be enacted differently [@lawEnactingSocial2004].

Overall, examining the role of data matching practices and technologies is essential to understand much contemporary coordination work within information infrastructures [@leeHumanInfrastructureCyberinfrastructure2006; @ribesSociotechnicalStudiesCyberinfrastructure2010; @monteiroArtefactsInfrastructures2013]. That is to say, in the case of data matching practices, the activities and technologies that make data findable, shareable, connectable are frequently crucial to make possible distributed cooperative work. In the context of scientific work, for example, scholars have shown the — often invisible — efforts needed to make data sets shareable and useable by others [e.g., @edwardsScienceFrictionData2011; @kervinBackstageWorkData2014; @plantinDataCleanersPristine2018]. A data practice approach thus makes it possible to empirically explore how data quality practices and technologies support and shape data practices in collaborative work within and between organizations.

The move to situated data practices using concepts from practice theory still leaves some things unresolved. Rather than technologies, people or other actors, practice theories takes social practices as the central topic of inquiry [@reckwitzTheorySocialPractices2002; @schatzkiIntroductionPracticeTheory2005; @shoveDynamicsSocialPractice2012]. Only recently have the materiality of technological artefacts been incorporated as ‘aspects of practice-arrangement nexuses’ in which the materiality and practices are intertwined and constitute each other [@schatzkiMaterialitySocialLife2010]. Taking data matching practices as an analytical starting point can focus attention on the performativity of practices as they are situated and entangled with technological artefacts [@ruppertDataPracticesMaking2021]. Analysing data practices, such as the processing of an asylum application based on information collected about a person by different organizations and agencies, can then highlight how tools are supporting this work to link data from different sources. At the same time, such data practices of cleaning duplicate data may perform certain identity data as irregular. But while practice theory has been very productive to understand the situatedness of practices, debate continues about how social practices travel, how different practices are related to each other and other ’large phenomena’ [@shoveMattersPractice2016; @nicoliniSmallOnlyBeautiful2016].

Following @schatzkiIntroductionPracticeTheory2005, practice approaches can either elaborate on the various interconnected practices of a subdomain of human activity, or consider practices as ‘the place to study the nature and transformation of their subject matter‘. This latter view is supported by @shoveDynamicsSocialPractice2012 who proposed that ‘the dynamics of social practice’ can be useful to understand social changes by looking at the change and stability of practices. In this way, I propose that examining the dynamics of data practices for matching identity data can be useful to understand developments of identification in security settings. Operationally, these dynamics of data matching practices can be examined by investigating the dynamics of data practices: how they change, stabilize, travel.

Analysing the dynamics of data matching practices can show how technologies are intertwined with these practices to shape, transform, stabilize identification in transnational security infrastructures.  The second infrastructural inversion strategy to make visible the matching identity data in transnational security infrastructures foregrounds usual backstage elements as work practices of managing, linking, cleaning data. Difficulties arise, however, when an attempt is made to understand how the entangled technologies themselves have evolved and moved across contexts and organizations. The third inversion strategy will thus focus on how to make visible the processes of social shaping of technologies.

### Third inversion strategy: lifecycle of technologies

STS scholars have for a long time chronicled the choices and directions of design of technological artefacts and systems through different approaches to examine and understand the nexus between users, technologies, and designers. Social constructivists, in particular, have emphasized the social construction of technological artefacts (rather than human action determined by technologies) [@mackenzieSocialShapingTechnology1985; @pinchSocialConstructionFacts1984]. This scholarship accordingly emphasized how the construction of technologies is tentative and shaped by specific contexts and actors in which the artefact emerges [@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. Concepts such as ‘interpretative flexibility’ [@pinchSocialConstructionFacts1984] and ‘boundary objects’ [@starInstitutionalEcologyTranslations1989] emphasized how different social actors and groups attribute different meanings to artefacts and how technologies are put to use. Likewise, these concepts are crucial to analyse and contrast how the design and evolution of the technologies and how actors perceive, use, configure the tools for searching and matching data.

Scholars have proposed various concepts to compare the development of technologies with how actors use technologies. Concepts such as ‘script’ have been introduced to account for the gaps between designed intended uses (a ‘script’) and actual uses. While notions such as ‘configuration’ emphasize how expectations and imaginaries of designers are inscribed into technological artefacts, and how they users and artefacts are figured together — delimiting and making possible certain forms of agency. However, critics question the ability of some of these approaches to account for the longer-term design and evolution of technologies [@williamsMovingSingleSite2012]. This scholarship argues that technologies cannot studied through a single ‘snapshot’ in time. Rather, they argue that there is a need for a kind of ’biography of artefacts and practices’ [@pollockSoftwareOrganisationsBiography2009].

In summary, and drawing on these literatures, there is a need to investigate data matching technologies and practices in a way which allows taking at same time a situated and a more longitudinal approach to investigate how they are shaping and shaped by transnational security infrastructures. Looking at different moments in the life of the data matching technology and practices should be addressed.

## A conceptual framework for analysing data matching in distributed settings

Following the overview of existing work and the gaps in their methods, I now put forward a conceptual framework for tracing data matching practices and technologies in transnational security data infrastructures. Such a framework is needed since no single method or theory can be adapted to examine the multiple facets of data matching practices and technologies within and across organizations. In this section, I therefore propose a new conceptual framework that combines established methods and theories into one framework that can guide the research in examining those multiple facets. The conceptual framework, in essence, integrates more computer science views on linking and matching data models and records, with social science views from fields associated with Science and Technology Studies (STS).

As previously said, for government agencies, business, and other organizations the tasks of finding records in databases that refer to the same person can be important in many situations. As organizations, states, agencies have been collecting ever more data about people, they may perceive performance benefits of having complete and accurate information about people, or even see it as beneficial to national security. The techniques for data matching — such as the mathematical models for linking records — are therefore well established. However, the creation, development, and consequences of deploying technologies for data matching are less well understood and researched. With the framework that I outline in this section I aim to address this gap in the research. I propose to trace data matching practices and technologies as sociotechnical phenomena, and from four different angles: (1) how the kinds of data that are collected about people can be compared and what this tells us about how organizations can search, match, and use this data, (2) how within organizations search and match for data about people that may have data quality issues, (3) how such identity data is matched between organizations, and (4) how, simultaneously with matching identity data, data matching technologies and practices travel and evolve across organizations.

To discuss these different aspects of analysing the matching of identity data, I propose to visualize the framework through several axes (Figure \@ref(fig:method-axes-visualization)). This visualization roughly follows a relational model to represent data as tables that model relationships between data. The three axes capture different dimensions of data, which are: _data models_, _categories of data_, and _data values_. Next, I will describe how each of these three axes and how different relations between the axes correspond to specific aspects of data matching within and across organizations.

```{r method-axes-visualization, echo=FALSE, fig.cap="A visualization of the different aspects of data matching examined through the framework.", out.width="100%", fig.align="center"}
knitr::include_graphics("figures/method-axes-visualization.pdf")
```

To begin, the axis for _data models_ (axis `z`) represents the abstract models that standardize the kinds of information about people collected by different organizations. An example of such a data model could be the schemas specifying data collected about migrants by a government agency. Each organization generally defines its own data model. However, some data models might even be shared to make interfacing between systems possible. Next, each data model specifies _categories of data_ (axis `x`) which are collected about people. For example, the categories of data of the example asylum agency’s data model may include ‘name’, ‘place of birth’, ‘date of birth’. Finally, there are the actual _data values_ (axis `y`) in systems’ databases for a data model and its categories of data. For example, a value for the category of data ‘place of birth’ may be ‘Brussels’.

That being so, I propose to schematize different facets of data matching as combinations of pairs of these axes. In Figure \@ref(fig:method-axes-visualization), three axes each zoom in on specific facets of data matching within and across organizations.

### Matching data models

The first pair _(data models `z`, categories of data `x`)_ represents the identification of semantic similarities and differences between different data models and their categories of data. Such a process will also be used in the research approach to support discursive analyses of data models. In this way, the analysis intends to compare the expectations and imaginaries of diverse actors concerning data collected about people and the ways data can be searched, matched, used. For example, the data model of a government agency may contain the category of data ‘family name’ while another agency uses ‘surname’ as category of data. Even though the name of the categories of data are distinct, they clearly refer to a similar concept. In this case, a part of someone’s personal name that indicates their family. To connect information about people from these two agencies’ systems should therefore identify similarities between their data models and these two categories of data.

This first aspect of matching people’s identity data that is addressed by the conceptual framework is therefore the relations between different information systems’ data models. That is, how different data models for collecting information about people-on-the-move are situated in relation to one another, and what this tell about the organizations which developed and make use of these models to collect, search, and match identities.

To do this analysis, the method draws, first, on studies of classification and its consequences which have addressed the question of how to retrace the ethical and political work of otherwise mundane devices of representations — i.e., data models. Researchers have previously used and combined various makeshift methods — from narrative interviews [@gazanImposingStructuresNarrative2005] to discursive textual analysis [@caswellUsingClassificationConvict2012], from participant observation [@meershoekConstructionEthnicDifferences2011] to archival and genealogical research [@gassonGenealogicalStudyBoundaryspanning2006] — for the ethnographic and historical studies of information systems [@starStepsEcologyInfrastructure1996] and their classifications. Furthermore, it is altogether necessary to take a more interdisciplinary approach and also pay close attention to the technical details of classification, as to avoid only considering the effects of classification [@kitchinCodeSpaceSoftware2011].

Following Geiger and Ribes [-@geigerTraceEthnographyFollowing2011], data models can be considered ‘thin’ traces: being rather standardized schemas made of categories and values, they are hardly meaningful in themselves. The method to analyse them therefore should consider how to turn them into ‘thick’ data. Nevertheless, given their implementation in diverse infrastructures, data models can be an excellent starting point to understand how geographically distributed sites are connected [@latourReassemblingSocialIntroduction2005]. Burns and Wark [@burnsWhereDatabaseDigital2020], for example, have dubbed such approach ‘database ethnography’ and used traces left behind of a database as a site to analyse how social meanings of phenomena change over time — but, only using their own ad-hoc mix of methods.

On the other hand, matching data models shares concerns with methods developed in computer science fields interested in the use of semantic technologies, such as knowledge engineering, linked data, and natural language processing. For example, the relationship between the different types of categories and their variations is closely linked to the outcome of processes known as schema or ontology matching, which finds correspondences between concepts of schemas or ontologies [@euzenatOntologyMatching2007; @kementsietsidisSchemaMatching2009]. Another linked technique called ontology learning aims to automate processes for creating ontologies. These (semi-)automatic mechanisms then include extracting concepts and relationships between concepts from a domain of discourse by analysing a corpus of documents in that domain. There are thus shared concerns about how to examine the way different knowledge representations interrelate, and about how to recover such representations from a discursive domain. Yet, there are fundamental differences which make the methods hard to effectively support a discursive analysis.

Methods of knowledge engineering are generally more concerned with creating technologies that help machines understand better means for the systems, and with integrating different sources of information work together. By contrast, the method for this research needs to support discursive analysis by taking differences in representations as a starting point to analyse imaginaries of diverse organizations and authorities and to combine the analysis with ethnographic observation. To address this aspect of data matching there is therefore a need for approaches that can support analyses of formalized data models in two ways. First, to support analyses of information systems which define their own data models, even if these systems are not immediately comparable. Second, to systematically and quantitatively support discursive analysis of ‘thin’ data models, also by detecting differences and absences between systems.

### Data matching within organizations

The second pair _(categories of data `x`, data values `y`)_ relates to the categories of data and their actual values in databases, which may not always correspond to the expectations of the data models. For example, when values for the categories ‘first name’ and ‘last name’ are mistakenly switched in the database. Practices and technologies may therefore be employed to remediate such data quality issues — i.e., data matching. Overall, two applications of data matching within organizations and their databases can be distinguished: real-time data matching and deduplication. Real-time data matching mainly addresses the issue of retrieving information about a person through search queries. For example, police officers may need query databases based on the categories of data ‘name’, ‘nationality‘, ‘date of birth’ and to see if approximate matches exist for those personal details. Data matching techniques will therefore be used to find similar matching identities’ data, rather than only exact matches. Deduplication, on the other hand, is the use of data matching technologies for identifying records in a database that refer to the same person and fusing the multiple records.

On one hand, it is crucial to understand the moments when actual data are not aligned with the expected data model. For example, when multiple records were created for a person due to name variations. In this way, the relation between data expected by data models and actual data in databases can draw attention to the data practices of how organizations collect and use knowledge about people-on-the-move. For example, how they choose to take up technologies for searching and matching to help deal with data uncertainties deriving from frictions between the moments of collection and use of data. On the other hand, the design and use of these tools for dealing with such data matching need scrutiny. The design and use of such data matching tools can be compared to understand the relations between users, designers, and technologies. Such comparisons can identify expectations and imaginaries of designers that are inscribed into technological artefacts and how they are all configured together.

Scholars in fields of STS and beyond have devised different approaches to examine and understand the relations between users and technologies [@oudshoornHowUsersMatter2003], and how expectations and imaginaries of designers can be inscribed into technological artefacts. For example, the approach of script analysis was advanced to examine the assumed competences of users and affordances that are embedded in artefacts [@akrichSummaryConvenientVocabulary1992; @latourWhereAreMissing1992]. The script of an artefact then requires users to adopt designers’ envisaged behaviours and actions to interact with an artefact. This approach also allows accounting for situations when those assumptions by artefact designers don’t match the actual uses and practices. @woolgarConfiguringUserCase1990 furthermore noted how system designers might attempt to ‘configure the user’ of the system by incorporating the user into the sociotechnical system. The method to understand such configuration is by treating the computer systems as a text that is read by users and can be interpreted differently. Designers may thus attempt to anticipate and delimit this flexibility. In these views, a sociotechnical system functions well when users and the system are successfully configured together.

Suchman [-@suchmanHumanmachineReconfigurationsPlans2007] furthermore argued that the inscriptions of users and uses are never that coherent, and that a more open-ended and indeterminate approach towards artefacts is required. She proposes configuration as a ‘method assemblage’ that pays particular attention to technologies and ‘the imaginaries and materialities that they join together’ [@suchmanConfiguration2014, p. 48]. Configuration as a methodological device can thus be, in accordance with Suchman’s view, useful to unpack the material and discursive elements of software for dealing with data uncertainties.

This axis in the conceptual framework therefore aims to answer the question of what kinds of imaginaries and materialities are joined together in data matching technologies. Analysing and comparing the technical designs with actual uses, and thus also where they mismatch, can give evidence of the various imaginaries, expectations, assumptions at play. Yet, the approach also needs be both specific to the situatedness of the design, and keep an open-ended and indeterminate view of how such technologies are put to use [@suchmanConfiguration2014].

### Data matching across organizations

Finally, the pair _(data models `z`, data values `y`)_ addresses data matching across organizations and agencies. Identity data records that refer to the same person may exist in systems and databases of different organizations and government agencies. Data matching in this case refers to the data practices related to identifying and potentially linking or merging such identity data spread across various databases. A crucial challenge for data matching techniques is due to the fact that unique identifiers are not always available and, hence, that matching relies on personal information which is not always complete or accurate. Personal information such as names, dates of birth frequently contain typographical errors and variations. For example, a woman may have used her maiden name as surname in one system while information collected in another database uses her husband’s surname after marriage. However, the question for us here is how — by bringing such data in relation with each other — data matching practices and technologies shape the relations between different actors.

Furthermore, these data matching practices and technologies cannot be studied independently of the infrastructures they are embedded in. Data matching technologies can be crucial to make certain collaborative work possible, such as the processing of an asylum application based on information collected about a person by different organizations and agencies. At the same time, the work of cleaning and managing data often can become invisible.

Based on the insights from infrastructural studies, data matching technologies in fields of security can therefore become an inconspicuous element in transnational data infrastructures. This in line with current debates in fields of International Relations (IR) and Security Studies who have started to acknowledge the role technology international politics [@amicelleQuestioningSecurityDevices2015; @hoijtinkTechnologyAgencyInternational2019; @bellanovaAlgorithmicRegulationSecurity2020]. Crucially, these scholars extend agency to the technologies and artefacts used in security practices. Glouftsios has, for example, shown how the often-invisible maintenance for the large-scale information systems for border security in the EU has a crucial role in sustaining the governance of international mobility [@glouftsiosGoverningBorderSecurity2020].

In the case of data matching technologies, the underlying assumption is that such technologies can trigger or even settle controversies about which actors produce more reliable data. For example, when matching identity data from different agencies these tools will bring certain records in relation with each other. If there are uncertainties about the data, they may then play a role in disputes about which agency’s data are correct. The hypothesis is that data matching technologies can thus inscribe certain forms of agency that can reconfigure practices of security and migration in transnational data infrastructures. Infrastructural inversions are therefore key to calling attention to the background or invisible work of mundane data management practices.

### Travelling data matching software

Furthermore, and related to the last pair, is the question of how not only identity data are matched across systems, but also how technologies and practices for data matching travel across organizations. As companies develop tools to support these tasks and deploy them at multiple locations and times, the concomitant knowledge of how to manage typographic errors, name variations, missing data, and so forth may also travel across organizations. This axis is consequently related to how data are not only matched between different organizations and agencies in security contexts. Rather, the hypothesis is that, at the same time, knowledge and technologies for such data practices travel between organizations.

Although the notions of ‘script’ and ‘configuring the user’ have been generative in understand the relations between users and technologies, they tend to overemphasize the role of designers [@oudshoornHowUsersMatter2003]. More recent approaches have therefore suggested to, for example, take a more comprehensive approach to examine important moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. An alternative approach has been developed by scholars called the ‘Biography of Artefacts and Practices’ which can detail the complexity of development of such technologies and related practices.

Following this approach, the emphasis of this aspect in the conceptual framework is to understand the biography of a data matching software packages. This approach is thought to highlight the wide range of actors involved in the development of such security technology and how the software travels and evolves over extended periods of time and between in different locales. This expands the narrower focus of the user-design-technology nexus used in the second axis to take a more comprehensive view of how the technology has been shaped over time, and by a broader number of actors at multiple sites.
