# Data matching within organizations {#ch-dm-within-org}

\chaptermark{Data matching within organizations}

__Abstract__

\noindent

This chapter examines the relationship between technologies for searching and matching identity data and routine bureaucratic identification practices in migration management. Practices for establishing or verifying a client's identity at various stages of bureaucratic processes, such as residency or naturalisation applications, are fairly routine and frequently mediated by technology. In light of debates on the materiality and performativity of identification, we can now view identification not as a problem of representation between clients and their identity data, but rather as practices mediated by multiple actors and as performative. A compelling addition to these debates could come from the literature on street- and system-level bureaucracies (SLB), which has argued that the everyday bureaucratic practices and use of discretion ultimately amounts to policymaking. From this vantage point, identification — a sometimes hazy area devoid of clear policies and rules — is enacted through everyday practices. While SLB scholars have debated the effects of new technologies on, for example, the discretion available to civil servants, they have been less specific about the constraints and affordances of technologies shaping decisions. I propose investigating these dynamics empirically by focusing on "re-identification." For the purposes of this chapter, "re-identification" refers to the process by which clients of bureaucratic procedures are re-identified in data infrastructures at various points in those procedures. As part of my research into re-identification, I examined how a Dutch government migration agency uses a software package to search and match client data. The findings contribute to debates about the materiality and performativity of identification by shifting the focus from first registration to practises of re-identification across data infrastructures. The analysed software package mediates (re-)identification practices and redistributes competencies through the software's affordances to mitigate data quality issues and uncertainties.

---

\vspace*{\fill}
\noindent
_Possibly insert citation here._
\newpage

## Introduction {#intro}

The stakes in identification can be high, and authorities' use of specialised technologies to search and match identity data can greatly mediate uncertain identification encounters. An often invoked real-life example of the complexities of identification is when one of the "Boston bombers," Kyrgyz-American Тамeрла́н Царна́ев, was _not_ pulled aside for questioning when leaving from and returning to JFK Airport in New York for a trip to Dagestan in the Northern Caucasus in 2012 (an area considered as a high risk travel destination by the US government). [^boston-bombers] According to an investigative report for the United States House Committee on Homeland Security, which media outlets reviewed, he was mistakenly not identified as a person of interest and questioned [@schmittAgenciesAddedBoston2013; @winterRussiaWarnedTsarnaev2014]. In April 2013, he and his brother carried out a terrorist attack during the annual Boston Marathon. In 2011, Russian authorities had already informed their American counterparts of his ties to terrorist organisations. As a result, the US government added him to various watch lists and databases, including the Terrorist Identities Datamart Environment, which contains information on over 1.5 million people who are either known or suspected to be international terrorists. Such watchlisting systems are meant to compare data about individuals automatically, alerting and instructing authorities on what to do when they encounter someone whose data matches a watchlist entry. Due to missing information regarding Mr. Царна́ев's date of birth and variations in the transliteration of his name, "Tsarnaev" — "Tsarnayev." the system might not have raised an alert in his case. The case of the Boston bomber exemplifies at least three important features of modern identification practices mediated by digital technologies.

[^boston-bombers]: Tech companies often use the Boston bomber's misspelt names as an example of why watch list screening systems need their data matching technologies [see also @basistechnologyStrengtheningBordersIntelligent2021].
Businesses can use this type of scenario as part of their sales pitch to show how their technology can handle the ambiguity inherent in locating and connecting individuals' identities.
Despite the risk for perpetuating the securitization of identification, I find this case instructive as a practical illustration of the interdependence of various government agencies, border guards, and watchlisting systems in the process of identifying potentially risky travellers.

First, from street-level bureaucrats to system-level bureaucracies, all face some unavoidable uncertainties in their daily work of establishing and verifying clients' identities. Most organizations need to cope with the fact that databases contain data that is incomplete, not current, incorrect — or, even contain duplicate entries that refer to the same real-world persons [@keulenManagingUncertaintyRoad2012]. Hence, ensuring that data are correct, complete, accurate and that they can be shared, used, processed by different parties and information systems has become vital for the correct functioning of (bureaucratic) procedures.[^gdpr]^,^[^data] The European Union Agency for Fundamental Rights, for example, investigated how data quality in EU information systems for migration and border control affects fundamental rights [@fraWatchfulEyesBiometrics2018]. The agency's report remarks that in relation to uncertainty about a person’s identity "authorities often suspect identity fraud when cases of data quality are the real reason for concern" (p. 81). For alphanumeric personal data (such as surname, date of birth, nationality), these data quality issues can have various, and often quite unspectacular, reasons. The case of Тамерла́н Царна́ев touches on the fact that watchlists databases need Latin-characters names, yet, transliteration of a name can take many forms. Hence, working with different sources of data usually brings challenges of what I will call "re-identification." With this term, I aim to capture various re-identification processes such as determining someone's corresponding identity data in databases, or deciding if multiple database records (possibly from different organizations) are referencing the same real-word person.[^reidenfication]

[^gdpr]: The General Data Protection Regulation (GDPR), for instance, states the accuracy principle in Article 5(1)d. According to this principle, the personal data that organisations collect and use must be "accurate and, where necessary, kept up to date" and "every reasonable step must be taken to ensure that personal data that are inaccurate, having regard to the purposes for which they are processed, are erased or rectified without delay." Additionally, the GDPR contains several other Articles that discuss data interoperability and sharing.

[^data]: The word data is often treated as a mass noun, and hence something that cannot be counted or divided (e.g., "the data is available"). In contrast, this chapter uses data in its countable plural noun form ("data are"). I follow the convention of using this form to highlight that data are multiple and "arise from and are used in varied circumstances worth acknowledging" [@loukissasAllDataAre2019, p. 13].

[^reidenfication]: In technical literature, the term re-identification is also used to describe processes of de-anonymizing data, i.e., revealing personal identities associated with anonymized data. Re-identification of previously anonymized individuals is, in fact, always a possible outcome of data matching processes. As @christenDataMatchingConcepts2012 explains, re-identification is possible because "record pairs classified as matches in a data matching project can contain information that is not available in the individual source databases that were matched" (p. 189). By matching data from different sources, individuals in those databases may still be (un)intentionally identified and disclosed even with incomplete identifying information. Consequently, there is an undeniable connection between re-identification as de-anonymization and the practices and technologies described in this chapter.

Second, new technologies keep being introduced to deal with "data frictions," [@edwardsVastMachineComputer2010] and re-identification should thus be understood in its (changing) sociotechnical context. That is, researchers must consider how technologies such as for searching and matching identity data (re)configure practices of re-identification. This point resonates with materialist and performativity debates on identification discussed in Chapter 2 [e.g.,@fors-owczynikMigrantsRiskIdentity2015; @leeseFixingStateVision2022; @pelizzaIdentificationTranslationArt2021; @vanderploegIllegalBodyEurodac1999; @pollozekInfrastructuringEuropeanMigration2019; @skinnerRaceRacismIdentification2018]. Following these debates, re-identification should not be understood as a problem of representation between people and their identity data, but how data infrastructures for identity management and (re-)identification "enact" individuals as migrant, criminal, risky traveller. From this perspective, we can rethink the above-mentioned quote from the Fundamental Rights Agency. Instead of asking if the doubts about someone's identity arises from inaccurate data or mistrustful border control practices, we must consider also how data infrastructures enact subjects as a potential identity fraud. Such a materialist and performative approach replaces discussions of identity as representation to account for heterogeneous set of actors involved in identification practices [@pelizzaIdentificationTranslationArt2021]. So far, however, literature has focused on the sociotechnical context of first (and often the biometric) registration (e.g., biometric refugee registration), and there has been little discussion about how people are re-identified and enacted throughout bureaucratic practices and data infrastructures.

Third, as the literature on street-level bureaucracy emphasises, government policies are established through the discretion that public employees use in their regular interactions with citizens to deal with complex situations that do not always fit neatly into the rules and regulations devised by legislators [e.g., @lipskyStreetlevelBureaucracyDilemmas2010]. (Re-)identification encounters are procedures in which there is both a lack of and a divergence from rules and regulations, and, as a result, there is considerable room for discretion. However, the problem of (re-)identifying clients during bureaucratic procedures has received little attention in the literature to date. A helpful example of an identification encounter where the tension between systems, policies, and local circumstances is apparent is provided by @pelizzaIdentificationTranslationArt2021. She describes the back-and-forth between an applicant, a police officer, and a translator to convert the applicant's name from Arabic to Latin characters during first registration at a Greek border. The name that emerges from this identification encounter is, in Pelizza's words, the result of a "chain of translations" of the migrant's name from oral to written to finally end up in the information system to serve as the official version to be used in additional administrative procedures. The process Pelizza described is again very different from an example I encountered in The Netherlands where, in case there are doubts or refusal to give a name, the person will be assigned a name in the system using a label which includes information about the sex of the applicant, and the time and place of registration (e.g., "NN regioncode sex yymmdd hhmm"). In both examples, public servants (re-)identify people by tailoring their actions to the individual involved, all within the constraints and affordances of a given sociotechnical setting.

However, while the street-level bureaucracy literature has long debated the constraining or enabling effects of new technologies, such as related to automated decision-making [@bovensStreetLevelSystem2002; @buffatStreetlevelBureaucracyEgovernment2015], it has been less specific about the entangled technologies. Nonetheless, it is clear that the expectations and materialities about data (and their quality) shape identification encounters. The designs and data models of technical solutions, such as those used to search for a person's record or to determine whether two identity data records refer to the same person, embed many assumptions about those data [see also @pelizzaScriptsAlteritySecurityunderreview], which shape bureaucratic re-identification practices. In the case of identity data, such tools assemble knowledge and enact equivalences between otherwise disparate naming practices. For example, the male and female forms of a family name might each have a slightly different final syllable, but they could still be considered equivalent. I therefore inquire in this chapter about the re-identification of clients of bureaucratic procedures in data infrastructures. By shifting the focus from first registration to practices of re-identification in data infrastructures, my overarching goal is to contribute to the ongoing debate on the materiality and performativity of identification.

The research seeks to investigate the research gap on re-identification by empirically studying a software package used in practices and technologies of re-identification at the migration and naturalisation service in The Netherlands (IND). The analysis draws on data gathered through fieldwork — interviews, documents, field notes — at the supplier of a software package used at the IND agency. The findings indicate that the design of search and matching tools embeds assumptions about databases and their data records, which shapes and is shaped by bureaucratic re-identification practices. Assumptions like these include the possibility of incompatible naming practices and conventions, which would mean that databases could never be fully accurate. Thus, it can be hypothesized that the use of data matching technologies was accompanied by a redistribution of roles due to the affordances and limitations of how government agents can look up identities as well as when automated matching algorithms take the place of user-specific identification expertise.

Together, the findings aim to make a two-fold contribution to the discussion surrounding the performativity and materiality of identification at the nexus of critical security studies, science and technology studies, and beyond. First, combining these literatures with concepts from the street-level bureaucracy literature makes it possible to follow how policy in everyday bureaucratic re-identification encounters are enacted and mediated through specific sociotechnical contexts, such as for dealing with ambiguities in personal data. Second, while previous research has focused on first registration, this study sheds light on other processes of re-identification that occur throughout bureaucratic processes and practices of working with uncertain data in migration management.

The following section seeks to connect the literature from street-level and system-level bureaucracies to the literature on the materiality and performativity of identification on the boundary between STS and Critical Security Studies. The remaining parts of the chapter will discuss the empirical case and findings.

## Background and related work

Many interactions between migrants and public authorities involve forms of identification, to establish or verify applicants' identity in different steps of bureaucratic processes relating to granting asylum, issuing, residency permits, naturalisation, and so forth. As a result, identification can be viewed as a two-sided phenomenon associated with both the granting and restriction of benefits [@aboutIdentificationRegistrationPractices2013; @caplanDocumentingIndividualIdentity2001]. Incorrect identification can, for instance, jeopardise the validity of an asylum seeker's entire application and lead to false accusations that the person provided a false identity on purpose. It goes without saying that rules and organizational policies form the foundation of the bureaucratic organisations that handle individual cases. Yet, as the literature on street-level bureaucracy has shown, public-service workers in charge do not just carry out relevant policies; they are also actively involved through the discretion workers use. What does it mean, then, to regard practices of (re-)identification as being a part of routine bureaucratic procedures?

### (Re-)identification as a bureaucratic practice

Michael Lipsky's book "Street-level bureaucracy: dilemmas of the individual in public services" (published in 1980) is widely credited with popularising the concepts of street-level bureaucracy and discretion. According to this widely held view, diverse front-line public service workers influence public policy through their regular interactions with the general public. As [@lipskyStreetlevelBureaucracyDilemmas2010, p.13] states: "street-level bureaucrats have considerable discretion in determining the nature, amount, and quality of benefits and sanctions provided by their agencies." For instance, a border patrol agent may have discretionary authority to grant entry to a traveller based on the results of an identification encounter with the traveller at passport control. Lipsky's original argument, however, required updating in light of the increased use of digital technologies in government and the rise of e-government. The transformations brought about by the digitalization of public agencies have forced scholars to rethink the role of street-level bureaucrats and their daily interactions [@bovensStreetLevelSystem2002; @buffatStreetlevelBureaucracyEgovernment2015; @buschDigitalDiscretionSystematic2018; @snellenElectronicGovernanceImplications2002].

The transformations brought about by information and communication technologies were conceptualised by @bovensStreetLevelSystem2002 in an influential article as occurring first at the "screen-level" and then at the "system-level" of bureaucracy. The term "screen-level bureaucracies" refers to how interactions between officials and citizens have become increasingly mediated through computer screens. For instance, the personal data of a residency permit applicant is filled out using electronic template forms in a case management system. Or, increasingly by applicants themselves as citizens are provided access to government information systems [@landsbergenScreenLevelBureaucracy2004]. Meanwhile, decision trees, business rules, and algorithms that model the policies and regulations guide the decision to grant the permit. The term "system-level bureaucracies" refers to an even higher level of automation and digitization when it comes to collecting data and carrying out routine tasks. The following is an idealised description of the practitioners' new roles in such an organisation: @bovensStreetLevelSystem2002:

> "The members of the organization are no longer involved in handling individual cases, but direct their focus toward system development and maintenance, toward optimizing information processes, and toward creating links between systems in various organizations. Contacts with customers are important, but these almost all concern assistance and information provided by help desk staff. After all, the transactions have all been fully automated." (p. 178-179)

From this quote, it is not a big leap to assume that re-identification plays a crucial role in the automated systems of system-level bureaucracies. To make the right decisions, automated processes must correctly re-identify the individual cases of bureaucratic processes. System development will thus be required to automatically re-identify individual cases and to ensure that data is accurate and up to date, that no duplicate entries exist, and so on. Furthermore, in a system-level bureaucracy, re-identification will be inextricably linked to connecting individual records across systems and organisations.

In the transformations from street-level, screen-level, and system-level bureaucracies, there is also a debate over whether automation is desirable for fairness, efficiency, or undesirable because it removes human judgement and autonomy in decision-making to adapt to local and individual circumstances. In her literature review, @buffatStreetlevelBureaucracyEgovernment2015 identifies the "curtailment thesis" and the "enablement thesis." as the two main camps in these debates. According to her, authors in the first thesis generally argue that ICT limits the discretion of front-line officers and shifts it to other actors. Following this thesis, if an information system, for example, can make an automated decision for granting a residency permit, the outcome will be less subjective since it the system can precisely follow policies and regulations. Such a thesis is of course overly technologically deterministic. Other studies show that technologies play a more nuanced role than just being counter to human judgement; rather, they are resources in sociotechnical arrangements that both limit and expand the ways in which technologies, frontline workers, and citizens interact with one another. This body of work is referred to as the "enablement thesis," by @buffatStreetlevelBureaucracyEgovernment2015. Finally, a more recent thesis can be found in the "digital discretion" literature, which advocates for the use of "computerised routines and analyses to influence or replace human judgement" [@buschDigitalDiscretionSystematic2018, p. 4] in order to strictly adhere to policy and lessen human discretion. This chapter takes a different, STS-influenced approach, emphasising how identification occurs in relation to the affordances and constraints of technologies that enact the bureaucratic realities they seek to describe.

Two important themes emerge from the studies discussed so far. First, that public workers will enact (re-)identification policies through their daily actions and discretion. Second, routine (re-)identification practices within bureaucratic organisations are intertwined with shifts in their underlying sociotechnical systems. Overall, the literature presented supports the notion that re-identification is critical as bureaucratic organisations' interactions with their clients become more digitised and automated. What remains unclear is how individuals are re-identified as part of bureaucratic procedures that combine human and automated computer systems.

### Materialist and performative approaches to identification

There is a growing body of literature that recognizes the importance of identification as intermingled with government’s obligations and rights (e.g., citizenship, residency), as well as coercive measures [@aboutIdentificationRegistrationPractices2013; @caplanDocumentingIndividualIdentity2001]. Traditionally, academics have also emphasized the links between state-making of modern nation states and the elaboration of registration and identification systems, such as the creation of civil registers or passport documents [@breckenridgeRegistrationRecognitionDocumenting2012; @caplanDocumentingIndividualIdentity2001; @torpeyInventionPassportSurveillance2018]. An often referred to term for the state’s capacity of identifying its citizens as a form of is the notion of _legibility_ of @scottSeeingStateHow1998. Scott noted how increased interaction of states and their population (e.g., for purposes of taxation) went hand in hand with projects of standardization and legibility as attempts to unambiguously identify its people. So, in the example Scott, while cultural naming practices are very diverse and can serve local purposes, the standardization standardized surnames "was a first and crucial step toward making individual citizens officially legible" (p. 71). In these practices, the identity of the person is not a problem of representation between a person and information captured about them, but as a reduction of multiplicity with a mutually enactment of identity, states, institutions [@lyonIdentifyingCitizensID2009; @pelizzaIdentificationTranslationArt2021].

A growing body of literature in the intersection between STS and Critical Security Studies has added an important dimension to the discussion on identification by accounting for the materiality and performativity of devices and practices[@coleSuspectIdentitiesHistory2001; @gargiuloMonitoringSelectingSecurity2017; @skinnerRaceRacismIdentification2018; @pelizzaIdentificationTranslationArt2021; @suchmanTrackingTargetingSociotechnologies2017]. @bellanovaControllingSchengenInformation2022 for instance have studied the actors and practices involved in the processes of maintaining the EU Schengen Information System (SIS). The SIS system allows authorities to create and consult alerts on, among other, missing persons and on persons related to criminal offences. By looking at how these alerts "acquire the status of allegedly credible and accurate information that becomes available to end-users through the SIS II" (p. 2) they make evident its role in conditioning international mobility. @fors-owczynikMigrantsRiskIdentity2015 have shown how three systems in The Netherland translate and frame categories of risk to identify potentially risky migrants and travellers. Incorporating the role of sociotechnologies into the analysis of identification practices makes it possible to empirically explore security and account for the materiality and performativity of devices and practices of identification of migrants.

Currently, surprisingly little is known about how practitioners handle ambiguities in personal identity data during re-identification encounters. For instance, a report from the European Court of Auditors describes how "when border guards check a name in SIS II [the Schengen Information System], they may receive hundreds of results (mostly false positives), which they are legally required to check manually" (ibid., p. 31). The SIS II system in question in this instance computes and presents an excessive number of matches, which creates difficulties in routine border control procedures. The question remains as to how technologies for dealing with data uncertainties enable or constrain these bureaucratic practices of re-identification.

### Data uncertainties in practices of re-identification

Critical data studies have made it clear that data is never "raw," that databases can be messy and with mistakes, and that work is therefore needed to put data to use [@gitelmanRawDataOxymoron2013; @loukissasAllDataAre2019]. For example, a report from the European Court of Auditors mentions that a major EU information system supporting border control contains millions of potential data quality issues, such as first names recorded as surnames or missing date of births [@ecaEUInformationSystems2020]. Many such discrepancies are related to work practices and issues of fitting local circumstances to global standards [@bowkerSortingThingsOut1999]. In a Chapter on the International Classification of Diseases as a standard for coordinating work, Bowker and Star describe the difficulties of busy doctors to fill in death certificate and how "the act of assigning a classification can be socially or ethically charged" — such as classifying a death as a suicide. And as @loukissasAllDataAre2019 remarks, it is clear that databases might contain lots of such errors hence "local knowledge [is needed] to see that such errors are not random" (p. 67). In this way, data are testimony to the local conditions of their production that need to be linked across space and time in future processes of re-identification. If data quality problems and uncertainty are facts of life [@keulenManagingUncertaintyRoad2012], then organizations need to cope with this uncertainty in practices of re-identification.

The technical mechanisms for (re-)identifying if two or more persons represented by data records actually refer to the same real world persons or not is itself known by many names: object identification, entity resolution, record linkage, data matching [@batiniObjectIdentification2016]. Basically, these techniques will compare attributes of data records and use classification methods to determine matches [@christenDataMatchingConcepts2012]. Numerous classification techniques exist: some may be based on following specific rules, while others may use more probabilistic methods. Metrics can, for example, calculate the similarity of two sequence of characters based on the number of operations that would be required to transform one into the other. In this way, the names "Sam" and "Pam" may be considered closely related (was it a typo?). Other approaches may even calculate such similarities by comparing how names are pronounced (in English). Rules based matching may include rule such as to ignore honorifics and titles (e.g., Mr., Ms., Dr.) when matching personal data. In this way, knowledge, techniques, rules, methods for matching identity data are assembled into sociotechnologies that inform organizations and practices of identification.

Data deduplication is commonly used to refer to the problem of data matching when dealing with duplicate records from a single data source. A migrant may, for example, be mistakenly registered more than once in a system due to technical issues. Generally, a deduplication process will then periodically compare records with every other record in the database to score its likelihood of matching. After determining the set of possible matches, a domain expert will need to be involved to decide on found matches. A final step of the data matching is then to fuse the multiple records referring to the same entity into a single record. More interesting is how the process of ‘normalizing’ duplicates can be ‘a key to learning about the heterogeneity of data infrastructures’ [@loukissasAllDataAre2019]. Therefore, following Loukissas, deduplication can give insights into the heterogeneous practices of identification in migration and border control.

This section has attempted to provide a brief summary of the literature relating to identification in relation to street- and system-level bureaucracies, the performativity and materiality of devices and practices of identification, and problems and solutions for dealing with data uncertainties. Essentially, I find that a compelling addition to this debate could come from the literature on street- and system-level bureaucracy. In this view, the everyday bureaucratic practices of re-identification and use of discretion would amount to identification policy in practice. For this reason, it is crucial to investigate how particular sociomaterialities give shape to (re-)identification encounters.  The next section describes the empirical case and methods used to investigate these issues.

## Empirical case and methods {#empirical-case}

Re-identification in migration management is investigated using data from fieldwork conducted in person and digitally between July 2020 and July 2021. More broadly, the research looked at how specific identity data searching techniques offered by a data matching software package are embedded with specific expectations that influence users' work routines and the organisations where they are used. To situate and analyse interactions between technologies, workplaces, and work practices, I employed a variety of methods, including observations, interviews, and document analysis [@ackermanResourcesCoevolutionArtifacts2008; @luffWorkplaceStudiesRecovering2000; @suchmanHumanmachineReconfigurationsPlans2007]. An initial understanding of the field and the application of this technology in transnational information infrastructures for migration and border control served as the foundation for the fieldwork, which can be viewed as a type of "strategic ethnography" [@pollockEInfrastructuresHowWe2010]. As such, the study is part of the dissertation's overarching goals to investigate the co-evolutions of development and deployments of the software solution deployed in EU and Member State identity management systems and identification practices.

During the fieldwork period, I collaborated with the Dutch company WCC Group, which develops software for handling data matching and deduplication. This software is used, for instance, in the case management system of the Netherlands' immigration and naturalisation service as well as in the exchange of visa data in the EU Visa Information System central system. [^wcc]. More specifically, I joined WCC's "Identity Team" (ID Team) to study their software solution for searching and matching identity data. Following the research design, it was essential to be able to examine, on the one hand, the software's design and development, and, on the other, its use by public organizations in identification practices. During my time as a temporary member of the ID team, I travelled to the company's headquarters in Utrecht (The Netherlands), where I analysed supporting documents, conducted individual interviews, and sat in on some team meetings.

[^wcc]: The company was formerly known as WCC Smart Search & Match.

The study compared technical details of WCC's "ELISE ID platform" (ELISE, formerly known as "ELISE Smart Search & Match") with its use in the IND's re-identification processes. Government agencies, such as the IND, use the software for more advanced searching, matching, and linking of client data on the organization's existing databases. Incorporating various algorithms into a coherent system to resolve identity-matching uncertainties is what makes this technology novel interesting.  For instance, matching can account for the possibility that birth of date data is incomplete or that the date and month values were accidentally switched. Re-identifying a client through a search (for example, based on their name, nationality, and date of birth) will not just produce exact matches; instead, the search results are a list of clients with a value denoting how likely it is that identity data records match.

To put it another way, the ELISE software is designed to deal with inexact re-identification caused by difficulties in matching personal data from different locales, scripts, cultural contexts, and so on. As a rule, the various algorithms account for human error in both the database and the search criteria. The hypothesis of this study was that such a nuanced approach to identity data would influence re-identification practices via the tool's distinct features. Furthermore, the study assumes that for such searches to be effective, for (street-level) bureaucrats to effectively use the search and interpret the results, both the system and its users must be aligned [@suchmanHumanmachineReconfigurationsPlans2007; @woolgarConfiguringUserCase1990]. Focusing on the kinds of problems people have when using the search and match functions in IND systems was thus thought to show the assumptions and expectations that lie behind the sociotechnical configuration.

In this chapter, I draw from my research into how the IND uses the ELISE ID platform in to re-identify clients throughout bureaucratic processes. The IND is in charge of, among other things, processing applications from people who wish to stay in the Netherlands or acquire Dutch nationality. For practical purposes, the IND case management system makes use of the ELISE software to handle data issues like duplicate records and to facilitate searching of applicant data in their back-office system. Conceptually, the use of the ELISE search and match software at the IND is useful to understand how these different parts come together and shape re-identification practices.

Methodologically, I looked for evidence of frictions between tool design and tool use by comparing how different organisational actors at the IND use data matching capabilities in their daily tasks of re-identifying clients. Such a comparison was thought to help find out how the software solutions affect IND system users' re-identification practices. That is, when it comes to the search and match features, it is conceivable that discrepancies between designed use and actual use are to blame for any misunderstandings in re-identification. In these situations, frictions can also reveal inconsistencies between users and the system in terms of how to search and what reliable data is.

### Data collection

The design and expected uses of the software were looked at using documents about the ELISE software system's more general technical details. Second, through documents related to the system's specific implementation at the IND. All of these materials, including technical design documents and meeting minutes, were provided to me by WCC's ID Team, along with background information and recent developments.

By examining the technical documents, it was possible to develop a preliminary understanding of the search and match software's technical operation and the software's deployment at the IND. In addition to technical documents, public communications and reports provided context for IND information system development. The findings of the document analysis were used as a jumping off point for structuring the questions to ask in the interviews.

Following the document analysis, I conducted semi-structured interviews to gain insight into the actual development and use of the search and match tools. Two main groups of themes and interviewees formed the basis of my research. The first group was centred on IND staff whose duties include looking up and matching identities in their databases, which necessitates the use of the ELISE ID platform. The second group was centred on WCC staff who were, more generally, involved in some way or another in the development, deployment, maintenance of the software. This method allowed us to look at how the ELISE software, WCC, and IND all adapted to one another.

Initial contact with the IND organisation was made possible thanks in large part to members of the ID Team. Unfortunately, no in-person interviews could be conducted because of precautions taken in response to the Covid-19 pandemic. For this reason, participants were contacted to schedule an online meeting or phone call. All things considered, a benefit of these online meetings may have been that it was actually simpler to schedule interviews because neither I nor the participants had to travel. On the other hand, not all communication may have come across the same way, which may have made it harder to network and schedule more interviews. Following the initial contacts, a snowball sampling method was used to reach out to additional users willing to contribute to the research. I interviewed five persons from the IND about how they used the search and match tools, and seven people from WCC. Each interview lasted approximately one hour. While I did my best to ensure that a representative cross-section of the IND and WCC was surveyed, I had limited influence over the size of the sample and the respondents' reliance on their networks introduced a bias toward more senior members of the organisations.

### Data coding and analysis

For analysing the fieldwork data, I followed standard methods for coding and analysing qualitative data. After collecting and preparing data from documents and interviews (including transcription), I coded and analysed the data using the computer-assisted qualitative data analysis software ATLAS.ti. After an initial inductive coding based on common themes and ideas, I reviewed and gathered similar codes to find patterns, processes, and typologies. For example, I developed a non-exhaustive typology of data frictions for alphanumeric identity data attested by the IND interview data. This typology then included types such as: frictions resulting from human errors during data entry, ambiguities and incommensurabilities in transliterations, differences in identification policies, and so forth. In the next section, I will present the principal findings of the comparing the differences in design and use of the search and match tools in practices of re-identification.

## Findings

At least two general factors characterise the IND data infrastructure in which re-identification takes place. First, the IND interacts with a variety of other (non)governmental organisations, complicating client re-identification. Second, the technical components of the IND data infrastructure mediate re-identification. This section will begin with an overview of the agency's data infrastructure and its interactions with other organisations. The emphasis of this section will then shift to the development and use of the technical components involved in re-identifying clients.  

### (Re-)identification at the IND and beyond

The IND's bureaucratic re-identification processes and practices are best understood in the context of the agency's information infrastructure. Through a formal collaboration between various governmental and non-governmental organizations, the IND is one link in what is (metaphorically) called the "migration chain" (_migratieketen_, hereafter MK). Each link in this chain (called a "chain partners" _ketenpartners_,  KP) joins together processes that foreign nationals in The Netherlands go through: from entering the country, to obtaining a residence permit, to naturalisation, to departure/expulsion. The IND is specifically defined as an "identifying chain partner" (_identificerende ketenpartner_), which means that the agency can determine and register personal data of foreign nationals in the common registers, as well as change existing personal data. There are some differences between different identifying chain partners. Most importantly, only the _Vreemdelingenpolitie_ (national police) and _Koninklijke Marechaussee_ (national gendarmerie) can allocate identities in cases of undocumented migrants or suspected identity fraud. (Re-)identification in bureaucratic procedures of the IND should be understood in relation to this wider collaboration and coordination of migration policy and information exchange.

A crucial component in the information infrastructure that enables collaboration and coordination of the affiliated KP is the _Basisvoorziening Vreemdelingen_ (BVV) system. The BVV's data and issuance of uniquely identifying numbers (called _v-number_) constitutes a kind of "single source of truth" to share and consult information about foreign nationals between KP. Simultaneously, the BVV database is filled and changed from the KP's systems with identity data, information about travel and identity documents, biometric characteristics and status data (e.g., the outcome of asylum application). That is, even though each KP has its own database and information about migrants, data are kept aligned via the BVV [@InformatievoorzieningVreemdelingenketen2015]. For instance, INDiGO users searching for person matches always need to first consult the BVV. In practice, re-identification involves an interplay of multiple systems and practices. It can thus be suggested that the BVV's unique identification of migrants can be considered a kind of "boundary object" [@starInstitutionalEcologyTranslations1989] that makes it possible maintain a common identity across the MK while also allowing for the specific contexts of the partner organizations.

Various mechanisms are in place to ensure that "unique, unambiguous personal data of optimal quality are available in the migration chain" [@PIL2022, p. 9]. An important element is a protocol for standardizing the work of identifying and registering foreign nationals in the MK[@ministerievanjustititieenveiligheidProtocolIdentificatieLabeling2020].The protocol also describes data governance processes such as how identity data may be modified or when data must be destroyed. As such, the document describes the various partners, their links in the information architecture and role in the MK. As mentioned in the literature review, this dissertation sees identity as the outcome of practices involving heterogeneous actors. Hence, the number of organizations involved in the MK would lead to what @pelizzaIdentificationTranslationArt2021 calls "a proliferation of sociotechnical ‘spokespersons’" (p. 2) that would complicate future re-identification. As a result, the protocol for ensuring uniquely identifying data and enabling re-identification can be thought of as a way to reduce the multiplicity of possible identities.

In an interview with an IND employee, I was given an illustrative example of the proliferation of identities and the subsequent problems of re-identification (Interview 2021-01-29). The example recounts a scenario in which a migrant applies for residency at a municipality, which must then notify the IND in order to update the applicant's data. In the following interview quote, the interviewee explains how an automated message exchange can fail to re-identify the applicant due to differences in naming and identification practices and policies:

> In principle, the municipality only registers applicants who submitted such an application [for a residence permit] to the IND. A condition for registering with the municipality is that applicants must identify who they are. So that can be done, for example, with a birth certificate, a copy of a passport, or an identity document, or other documents, so to speak. The municipality does have a different kind of policy on identification than, for example, the IND. They have a different ranking of pieces that they, well, consider important to have.

> For example: we — the IND — see a copy of a passport sufficient, or an ID card, or even a laiser-passer. The last one is a kind of document issued by the embassy if the client does not have a passport or ID card. But the municipality… for them, the most important document to register someone is actually a birth certificate. And then you sometimes have differences, because, for example, applicants from, well, for example from Ukraine. They have, say, a name, and then a patronymic. That [patronym] actually refers to the name of their father. And then the family name. And, well, that patronym is often included in the registration of the municipality.

> But the IND, on the other hand, does not necessarily register on the basis of the birth certificate data; because those data were once given at birth, but of course they may have changed after many years. Because it is possible, by the way, that you take your marriage name, for example. So, if the client submits a passport with the marriage name, the IND will register the client on the basis of the passport data. While the municipality uses the birth certificate data. So you already have a difference. And we may then receive an automatic message [from the municipality], which the system cannot automatically link to a client.

This example demonstrates an important difficulty of the IND's re-identification processes and practices in determining whether or not a client is already known to the IND. Generally speaking, (re-)identification can happen in processing various information streams of the IND. The IND can, for instance, receive new applications, such as an application for a residency permit. In this case the applicant is commonly not yet in the IND system and a new client may need to be inputted into the system. However, the applicant may also already be known in the MK/BVV. In that case the client data needs to be linked(Interview 2021-01-29). As the following interviewee remarks about the process, there may be nonetheless by differences in the data records of the IND and BVV which will need to be investigated and corrected:

> Well, we actually search first on the system called BVV [...] We click on a button and then a search is made for the personal details that then appear. Well, if we have a hit, it means, for example, that either the Royal Netherlands Marechaussee, or Foreign Affairs, or the police have ever registered the client. Well then the data only occurs on the system called BVV. And if so, well, we'll make a link. Then we click on a button, and then there is a connection between the data from the BVV with the data we have received from the municipality. And if that is not the case, but for example that you can find the client in the BVV, but also in our IND system. That's when you press another [search] button. And when it turns out that the client appears in the BVV and in the Indigo system. Well, then we check in the Indigo system whether the names match completely, for example. In case of small changes in the name data, we also look further into the file. And if we do come to the conclusion: this is the same person. Then we also make the connection, so we register the client. We link the data together. Well, then you just have 1 client file, and then nothing is wrong. However, because there is a difference in personal details, for example, we have to report this in the system.

So far, this section has examined three aspects of re-identification in relation to the IND's information infrastructure. First, that (re-)identification in bureaucratic procedures of the IND should be understood in relation to the wider collaboration and coordination efforts. Second, that different mechanism for ensuring uniquely identifying data and enabling re-identification can be thought of as a way to limit the proliferation of identities from the various partner organizations. Third, that nonetheless these various mechanisms, the IND faces difficulty in determining whether or not a client is already known. In any case, whether entering a new client or adding new information to an existing client's file, the systems' search and match tools appear to be crucial for re-identifying clients. Let us therefore now look more closely to the integration of the ELISE search and match software package with IND's information system INDiGO.

### A brief history and overview of INDiGO

The IND's information infrastructure is an example of a system-level bureaucracy [@bovensStreetLevelSystem2002], with decisive roles for technologies, automation, information management for processing digital dossiers. The IND systems were designed to separate policy implementation, such as applying business rules related to the Dutch Aliens Act, from information management, such as data storage, searching, and matching [@kpmgitadvisoryAuditINDiGOWillen2011]. Technically, the case management system (called INDiGO) achieves this separation by utilizing a Service Oriented Architecture (SOA) design, which is a software architecture that aims to compartmentalize system functions into (more-or-less) independent services.

The design decision essentially aims to separate what INDiGO documentation refers to as the "flow" from the "know." By isolating aspects related to the application of laws, regulations, and domain knowledge, a rule management system "steers" the processing of digital dossiers with automated support (Figure \@ref(fig:landscape)).[^indigo-delays] This design addresses a common issue with rigid (governmental) IT systems, in which domain knowledge and business rules are tightly entwined with the system's operation. As a result, the IND rule management is easier to manage by the IND itself and better able to adapt to changes in legislation and regulations [@blankenaINDKanNieuwe2013]. Yet, how can we understand re-identification in relation to the interactions between IND system components? In this context, the ELISE software package is critical as the INDiGO component that supports client data searching and matching.

[^indigo-delays]: The SOA architecture of the INDiGO system received praise during its initial development in 2009 [@toetIndigosysteemVanIND2009], but the rollout of the new system in subsequent years did face a number of delays and problems [@bergsmaSysteemINDDuurder2013].

Based on my analysis, there are at least three uses of the ELISE software package for re-identification of IND clients. The ELISE software was initially used during the transition from the old IND information system (INDIS) to the new INDiGO. During a transition period when INDIS and INDiGO were running in parallel, the IND used the ELISE software to migrate legacy data by re-identifying matching client identities between the two systems.[^indis-indigo] Second, INDiGO makes use of the ELISE software to make client data searches easier. In place of a "traditional" search, which can fail to return results when search criteria are too strict or contain errors, the software's fuzzy search algorithms provide more advanced and reliable searching capabilities. Third, the ELISE software is used on a regular basis to search the database for possible duplicate client data. Broadly speaking, the software attempts to match all recently created clients to all other clients in the database. Potential duplicate matches that meet certain criteria will then be flagged and investigated further. In all three uses, clients are re-identified using match scores calculated by the software based on the likelihood that a client in the database meets the given search criteria. Let us now look more closely at the various types of searching and matching.

[^indis-indigo]: This is technically possible by running the software's algorithms on data that has been _replicated_ from both INDIS and INDIGO (i.e., regularly copying data from both data sources into the ELISE in-memory database). The initial rollout of the INDiGO system started in 2009 and was completed in 2013.

```{r landscape, echo=FALSE, fig.cap="A rough visualization of the IND’s information infrastructure supporting the various tasks of administering interactions with applicants, processing their applications, and collaborating between different organizational units and MK partners."}
knitr::include_graphics("figures/ind-landscape.pdf")
```

### Different types of searching and matching

The ELISE searching and matching is designed to function generically and decontextually as a separate component in the architecture of the INDiGO system. The system's technical documentation describes how queries from IND end-user applications are sent to the ELISE service, which then executes fuzzy matching algorithms and returns the results for display in the graphical user interfaces. As a result, the searching and matching service has little information about where in the INDiGO system and process it is called, or who is querying the system. This first finding contradicts much STS research on the co-construction of users and technologies [for example, @hyysaloNewProductionUsers2016; @oudshoornHowUsersMatter2003, @woolgarConfiguringUserCase1990]. To provide a generic search and match software package, the ELISE service does not consider specific users. And, in contrast to the process described by @pollockFittingStandardSoftware2003, there has been little back-and-forth between a "generic user" from the software package and a more "specific user" for the IND system. What does the lack of specific users imply for the IND's re-identification practices?

The interviews showed that different types of searches and matching can be distinguished. According to my analysis of the responses I received from IND staff members, different searches require different levels of verification and substantiation. The most basic and widely used method of searching for client data is to use rather precise personal data such as a name or identification number. These IND staff, for example, must process new information pertaining to an application. The employee may interact with the applicant directly at a font office counter or process documents at their back office desk. A distinguishing characteristic of these general uses of the search is that little interpretation of the input data (identification numbers, personal data) is typically needed to query the system.

To begin, search input may become more ambiguous and open to interpretation — and thus more error-prone. IND staff members who deal with phone calls or handwritten documents may fall into this category. In these circumstances, comprehending or piecing together the information to formulate a search query may require more effort. For example, the client's name may be misspelled or difficult to read in a handwritten document. Some search tool features may also become more relevant. In the case of phone calls, for instance, ELISE features such as taking into account different phonetic versions of a name or the possibility of numbers or letters being accidentally switched may be more relevant. While users working with handwritten documents may find functionalities that account for transcription mistakes and name variations more relevant.  In these re-identification situations, the ELISE system can be thought of as a mediating link in the chains of "translations into legible identities of individuals" [@pelizzaIdentificationTranslationArt2021, p.1]. Some examples of data frictions for the search input are given in Figure \@ref(fig:search-input), including errors in strings, integers, and dates, challenges with understanding and transcribing personal data, and methods used for inputting the data.

```{r search-input, echo=FALSE, fig.cap="This diagram shows how frictions with search query input were found by analysing interview data."}
knitr::include_graphics("figures/code-groups/data-friction-input.png")
```

Another distinction between search and match tool uses is the need to interpret and scrutinize search results. There are cases that require extensive interpretation of results despite having clear-cut input. For instance, there are important automated information exchanges with other MK partners, that sometimes necessitate manual intervention if the process fails to automatically identify the correct client to link the data. As previously mentioned, municipalities in The Netherlands, for example, use an automated information exchange to send residency data to the IND. In most cases, the search and match software will identify the corresponding IND (the match with the highest match score). If no good match is found, an IND employee will be required to identify the corresponding client in the IND's system. Furthermore, such examples provide a different perspective on the discussion of discretion in relation to automation in the public sector [@petersenRoleDiscretionAge2020] by being specific about the technologies [@monteiroSocialShapingInformation1996]. In practice, re-identification encounters are carried out in relation to particular sociotechnical contexts, sometimes automatically, other times requiring intervention.

Finally, there are times when the input is ambiguous and the search results must be scrutinized closely. For example, the first registration and start-up of a procedure for a client, particularly if it involves processing written documents. Documents, as previously stated, may be unclear and contain errors. Simultaneously, there is more uncertainty and a requirement to carefully investigate if a person is not already registered in the IND or MK databases. For example, an applicant may have been previously registered under their name before marriage. Some examples of data frictions for the search output are given in Figure \@ref(fig:search-output), including dealing with too many results, or none at all.

```{r search-output, echo=FALSE, fig.cap="This diagram shows how frictions with interpreting search results were found by analysing interview data."}
knitr::include_graphics("figures/code-groups/data-friction-output.png")
```

Based on these findings, it is clear that there are gaps between the expected, decontextualized designed use of the ELISE system and the difficulties IND personnel face in interpreting search inputs and/or results. Because of the amount or types of information available for processing an application, users may have specific needs and expectations from the search. Some users, for example, can search for a person's record based on an existing identification, whereas others, such as those working with postal pieces, must conduct more difficult searches based on ambiguous handwriting and incomplete data. This disparity could be one of the reasons why some interviewees reported having difficulty using the search. For example, they stated that they must conduct multiple searches using various combinations of search criteria. I propose categorizing these uses based on the degree of complexity INDiGO users face when interpreting search input and output. As a result, four combinations of the needs to interpret personal information to search (input) and the needs to interpret search results (output) can be identified (Figure \@ref(fig:matrix) summarizes these four combinations and findings as a matrix).

```{r matrix, echo=FALSE, fig.cap="This visualization compares the needs for interpreting the input with needs for interpreting the results in the form of a matrix."}
knitr::include_graphics("figures/ind-users-matrix.pdf")
```

### Use and understanding of search functionalities

The ELISE system's expected use is actually very similar to the previously mentioned automatic search processes. In that case, when a client takes up residency at a municipality, an automatic update is sent to the IND with their personal information, triggering a search query in INDiGO. Ideally, the client can be re-identified using the calculated match criteria, and their status in the IND database is updated. As a result, it doesn't (or shouldn't) matter for INDiGO how match results are calculated, and ELISE algorithms are thus (by design) a black-box component. This automated process, I believe, demonstrates how ELISE searching and matching algorithms are indifferent to the context in which they are used. Next, I'll show how this indifference contrasts with INDiGO users' more adaptive use and understanding of the search tools during re-identification processes.

Methodologically, I propose to detect conflicting expectations that impede successful re-identification by contrasting the design of ELISE with the INDiGO users' knowledge of search functionalities. Prior research has found that alignment between users and systems is required for these configurations to function properly [@hyysaloNewProductionUsers2016; @oudshoornHowUsersMatter2003; @woolgarConfiguringUserCase1990]. Similarly, I want to demonstrate that alignment between the use and understanding of search functionalities is required for the search to function properly and allow IND personnel to re-identify clients. Otherwise, this lack of knowledge can impede effective use of the tools, resulting, for example, in the creation of duplicate records when the correct record could not be found using the search tools.

When search results are based on matches made using clients' information from "historical fields" the informants consistently reported having trouble interpreting the results. This particular aspect of the IND's data model allows for the possibility of multiple values for a given data category. For example, the IND might keep track of a client's current and previous addresses. The confusion for INDiGO users stems from the fact that only the "primary fields" are displayed in the search results. An interviewee described the problem users face in understanding a match on historical data as follows:

> "And what is actually very interesting in [the case of the IND] is that someone does not just have 1 address, but can have several addresses, for example. Or even several names. And, he may have changed his name, for example. So then the old name is also saved. You actually have a primary field, for example for name or for address. And you have historical fields. And they are all searched with ELISE. [...] So we actually have the history of every field. That can contain one value, but it can also contain 10 values. And if you match that. I think that there is also an interesting point with user expectations. [...] I think they're not always aware of that. That if they find someone, it can also be based on an old date of birth, which has been entered incorrectly or. Or based on an old name." (Interview 2020-08-05)

This specific use of data categories with multiple historical values can be interpreted as being related to the "materialities" [@dourishStuffBitsEssay2017] of the data (model) influencing the system's operation. Dourish used this term to refer to not only the physical aspects of data, but also how digital representations allow and constrain how data can be used. The materialities of the digital representation of the client's personal information stored in databases make historical data available for re-identifying clients, but can be challenging to explicate to the user. According to one interviewee, they would usually be able to identify such cases based on their experience. Users are well-known for making adjustments to their routines to work around such system flaws [@gasserIntegrationComputingRoutine1986]. The user's workaround for a match on historical data is to look at the specifics of the results to find out why the match was included:

> "[...] experience has taught me that often the name has been changed. So there is also a history; so if someone has a different name, if a name is changed — and that is sometimes changed considerably — then you will indeed get it as matching. And when clicking through on the history of the name, you see that the logic comes from there, that it knows from history that it was called differently. And that is why it is shown. That is my experience." (Interview 2020-11-10)

Unlike in a case mentioned by, for instance, @pollockWhenWorkaroundConflict2005, users were unable to shape the system's design to fix the workarounds. Rather than attempting to resolve such frictions, interviewees appear to rely on experience to understand results and re-identify the right client. Another interviewee stated, when asked about more advanced search functionalities, that users may not always understand how the search works but can usually re-identify clients:

> [N]ow and then it is very hazy how [the search] exactly works. For example, sometimes a letter seems to be more important than other times. Depending on where it is located. But you usually see if you misspell such a letter that you then do not get a hundred percent hit.But then you still get sixty percent or so. In some cases it is also higher than that percentage. But you usually find [the client]. (Interview 2020-08-05)

Most interviewees are aware of some aspects of fuzzy search, but more features might operate without their explicit knowledge. INDiGO users are aware, for example, that the matching system takes into account differences in names spellings, name variations based on transliterations, and switched numbers such as in date of births or identification numbers. It did not appear that interviewees were familiar with more advanced matching based on name initials or name variations (such as "Aleksandra" vs. "Ola"), (de)compounding names (such as "Van Der Lei" vs. "Vanderlei"), or affinity matrices (such as "soft matching" of birth years within an acceptable range). This suggests, at least in parts, that the system enacts users as passive participants in the search with matching logic as implicit or of minor importance to the users. This suggests that what makes ELISE "smart" comes from its ability to intervene in the search process, rather than relying solely on user input.  Here, we can see how knowledge and expertise related to re-identification are distributed between humans and computer systems.

It should be noted that ELISE configuration in INDiGO may also be to blame for some these search issues. In reality, ELISE is designed to allow changing the behaviour and weight of search criteria. This is not the case with the INDiGO implementation, and the graphical user interface clearly does not appear to assist inexperienced users in formulating effective queries. From a technical standpoint, neither the query specification nor the presentation of results are under the ELISE component's control in the INDiGO GUI. Most INDiGO users learn how to use the tools by asking coworkers for assistance or through trial and error. As a result, different IND users and departments may have different levels of experience with the search and match features.

### Interpreting the search results and search strategies

> [...] on the BVV you can sometimes really get an error message with the result that no hits have been found because you are actually providing too many search options. Limiting the search options will yield results. While in INDIGO it is indeed that too; it always gives results. But, same effect; so the more data you sometimes provide, the more risk of match distribution, so that the customer is not always at the top that you are looking for.

> If you have more data, you look at what more you can put in it. So you're actually trying to make it as broad as possible. If you have a date of birth, you have a street name, or you have something else, to increase the matching percentage. And then you actually also look — if there are multiple search results — then you actually look first at the highest matching percentage.

> So then you try to see if you cannot find another way to find the customer. It may sound a bit strange, but sometimes you can see that… to which lawyer it was submitted, via which lawyer it was ever submitted. And then you can look through the lawyer, which client he has under him and that way you can also indirectly find out which client it is. But that is the difficulty when it comes to finding customers.

A third source of re-identification friction is how users interpret the system's search results, which are ranked by match percentage, in order to re-identify the correct client. The top results, those with the highest match percentage, are supposed to represent the most relevant matches. For example, a query based on a family name and a date of birth would return a list of results with data that roughly matches those fields of data, even if the name or date is slightly different. In theory, the matching personal data will be easily identified among the top results. In practise, this is not always the case. It became clear from the interviewees' responses that users have developed various techniques for making sense of results and exceptional cases.  As one interviewee put it, sometimes INDiGO users just have to "play" with the search tools until they find the right client again.

> "[...] what you often see in how they work is that hey, they use it first with one type data. And if they still get too many results, or they don't see it, they try with an extra piece of data. Or they try it with another kind of data. So you see, to find a person, they sometimes do five searches in a row. Also, a little, OK they could enter everything at once, but you can see they play with that a little bit." (Interview 2020-08-05)

In general, the search is designed to work best by including as much information as possible in the search query. The ELISE data matching algorithms can then, ideally, use all of this information at once to calculate match scores. But, as the comment below demonstrates, there was a sense amongst interviewees that giving more input data does not necessarily lead to better results:

> "My own experience with searching for personal data is: the more data you enter, the more difficult the result will be. And the worse the result actually gets. So I often build it up. I do less data and if necessary I add some data if there are too many results." (Interview 2020-11-10)

The interviewee thus alluded to the use of a search approach they came up with. Unlike the expected use of ELISE, entering too much query information may sometimes worsen the results and introduce uncertainty in re-identifying the correct client. As the comment below shows, users may thus reduce uncertainty in re-identification by trying out different combinations of data or leaving out some information that can be used to cross-check the results:

> "And there is also a kind of self-check in [the search process]. So I often search by first name, last name; to start with. [...] But I often try not to do too much and see if that result is there. And on that basis, okay, the date of birth also matches the date of birth that I have. So I don't always deliver what I have available as information. But I also partly use it as a checkpoint for the search result that I then get to the top. It also works a bit more efficient for me. Because it makes no sense to enter much more data. Because you can find the customer anyway, also sufficient on the basis of first and last name. And you get insight so that you immediately know that you have the right one." (Interview 2020-11-10)

As a result of this conflict, users seem to have come up with their own "search strategies" (see Table \@ref(tab:strategies) for some examples). Strategies include withholding some information that can be used to cross-check the results or manually trying different combinations of data. These two strategies, in particular, contradict the search engine's purported optimal operation, which is to provide the system with as much information as possible. The users' strategies can be thought of as mental models, which have been used in fields such as human-computer interaction (HCI) to explain and predict how users think about how computer systems work. As users gain experience using the system, these mental models grow and aid in their ability to reason about how it works and respond to unforeseen circumstances. Nevertheless, mental models can be erroneous. For instance, people frequently press the call button at a pedestrian crossing multiple times in the mistaken belief that this will speed up the process of activating the walk sign. Similarly, the findings indicate a misalignment between INDiGO users' mental models for searching client data and how the ELISE search and match algorithms work.

The gaps between the user and the system in figuring out how a system works and figuring out what a system produced have been described by @normanDesignEverydayThings2013 as the "gulf of evaluation" and the "gulf of execution." The gulf of execution is the difference between what a user intends to do and how well the system' user interface supports their actions. The gulf of evaluation then corresponds to the amount of effort needed by users to evaluate the system's state and results. These concepts help us comprehend how INDiGO users are, in the words of one interviewee, "actually trying to fine tune [the search] so you can get the right person up" (Interview 2020-11-02). The phrase "getting the right person up" perfectly expresses the general difficulties users encounter when re-identifying their clients in an effort to have them appear among the top search results. In light of this, it is conceivable that re-identification problems occur when there is a discrepancy between the users desired outcome (the re-identifying of a client) and the representation of the system's actual outcome (a set of search results).

```{r strategies, echo=FALSE, ft.align="center"}
path = file.path(DATA_PATH, "search_strategies.csv")
data = read.csv(path, header = TRUE)

data$Code = substring(data$Name, first=34)

block_table(data[,c("Code", "Comment")], header = TRUE, properties = pt)
```

So far, the findings suggest that the ELISE software package for searching and matching identity data has become inextricably linked with data practices for re-identifying clients throughout the IND's bureaucratic procedures. On the one hand, the software package facilitates re-identification work by limiting common problems and uncertainties associated with personal identity data, such as transliteration variations and typos. The current search design, on the other hand, turns the searching and matching algorithms into a black box that can be opaque and difficult to use at times. Next, we will consider how these re-identification issues may be contributing to the INDiGO system's persistent problem with duplicate identity data.

### Duplicates and deduplication

Duplicates, or the presence of multiple (unlinked) records for the same person in a database, are a common issue that organizations need to address [@christenDataMatchingConcepts2012; keulenManagingUncertaintyRoad2012]. There are a number of scenarios in which a person could have two separate identity records, such as when two different systems are integrated. However, more often than not, duplicate records are added inadvertently because of other factors, such as work pressure or a lack of (automated) checks that make sure a person's data is not already present in the database. The following interview quotes illustrate how duplicate identity records may be created and how duplicates may be found:

> Yes, specific departments within the IND have that [problem of creating duplicates], which can indeed create a duplicate client more often. For example, counter staff can do that. Due to lack of, well yes, just having less experience with the system. At least with search keys. Of course, they have to work quickly because they have the client in front of them, so to speak. So maybe there is a bit more time pressure. And besides, my department, which is more trained on searching. So maybe we are generally searching a little better in the system. We also have those tools, those control tools, namely also in the system for that. And then we also have the postal department, which is called DRV, digital registration and preparation. And of course, they have many more clients on a daily basis, so there is of course a good chance that they will create a duplicate client. But also my department, even though we are very trained in this. We are also still creating duplicate clients. More than we'd like. (Interview 2021-01-29)

> We [the IND] also have contact with [MK] chain partners such as the Vreemdeling Politie [national police], the Koninklijke Marechaussee [national gendarmerie] and the like. They can also create clients themselves. Buitenlandse Zaken [foreign affairs] can also create clients. And it often happens that sometimes a client has already been created by the Vreemdelingen Politie and then [the client] comes to our counter, that it [the data] is created again and that it has been created again in a different way. (Interview 2020-11-02)

> So the moment someone creates a client and then a new document arrives. And the person after that, that could be a year later, that could be five years later, he's looking and actually finding nothing. But the client still comes first. And if he then creates it again, he often does not immediately realize it himself, but I get that back or from the decision process, from the client occurs twice. Or someone else also receives a document that then comes in and starts looking for the client, only to find out that it occurs twice. Then we have a deduplication process that then starts to turn the 2 clients into 1 client again. (Interview 2020-11-10)

These excerpts highlight three important themes related to the existence of duplicates in the INDiGO system: how duplicates may be introduced, the potential risk of working with duplicates, and how duplicates are removed again. First, interviewees indicated that, in general, recurrent causes for duplicates to be introduced in the INDiGO system are, among others, related to time constraints, a lack of familiarity with the search tools, insufficient training, knowledge gaps, and the system's integration with other organizations. Second, having multiple client records for the same person raises the risk of the IND making wrong decisions. Third, what is known as the "deduplication process," is required to find duplicate records and turn the multiple client records into one again.

The ELISE search and matching system is also used to detect and remove duplicates. As mentioned in an earlier excerpt, interviewees cited a lack of automated checks as a significant reason for the creation of some duplicates. According to documentation, IND decided not to use automated checks to see if a client is already in the database when searching for or creating a client record.[^offline-online] Unfortunately, the reasoning behind this decision is not entirely clear. But as a result, the IND's deduplication service only uses automated database queries to scan for potential duplicates on a periodic basis. In general, the service looks for similarities between every new person record added to the database after a certain date and every other person record already in the database. For matching duplicates with ELISE, the IND has established specific rules for calculating the match score. If the match score between two records is greater than or equal to a threshold, both records are flagged as potential duplicates and must be investigated.

[^offline-online]: The INDiGO's technical design documents refer to these two approaches to finding possible duplicates as online and offline deduplication.

In addition to the automated query, "deduplication requests" are another, more direct method by which the "Titles and Identity" (T&I)  is informed of potential duplicates. Such requests may be sent by other IND departments or KP organisations, stating the data records and the justification for the request for deduplication. As an example, suppose an IND employee is processing an application and tries to re-identify a client. If the IND employee then sees that the client shows up twice in the list of results, they can fill out a form by hand and send it to T&I to ask them to merge the two records into one. The comment below shows how the "deduplication request memo" (_ontdubbelverzoek memo_) was made to standardise these kinds of requests within the organisation:

> Internally within the IND, yes, such a deduplication memo must be sent. And we have there on those requests, we have a kind of standard analysis. So basically an analysis based on system facts. For example, what comes in it is: we want to know if there is an identity difference of the customer. Because we've actually been working with that since — let's see — October 2019, when we also focused on identity differences. We call this "identiteitvraagstrukken" [identity question/problem] internally. That needs to be taken care of, and that was never done before. Until we got a case with a difference that we were aware of, hey, if we actually deduplicate files, we choose to make certain identity leading while this is not correct. Because my department just doesn't have that authority. And that authority is laid down in policy documents, among other things. (Interview 2021-01-29)

After the system identifies potential matches, domain experts evaluate duplicates and "deduplicate" records as necessary. During this deduplication process, users of the "Titles and Identity" (T&I) department will gather evidence to determine whether or not the records refer to the same person. If IND personnel determine that the records are duplicates, one client record will be designated as "leading." This means that instead of merging the two database records, one is made inactive. Some of the evidence gathered by users to make such decisions may be a combination of "weak evidence" (for example, identical addresses) rather than "strong evidence" (for example, identical identification documents). Given the importance of the decision to deduplicate, great care is taken to document the entire process and decision in a document known as a "memo." This way, others can see how the decision was made. Deduplication at the IND, as I see it, consists of four stages: (1) locating duplicates, either manually or through the query; (2) alerting users to duplicate records, either through the system or via email; (3) gathering evidence through T&I to decide whether or not to deduplicate the records; and (4) performing the deduplication in the system and logging the memo (sketched in Figure \@ref(fig:deduplication)). The following excerpt demonstrates how this seemingly technical decision of deduplicating data records is euphemistically referred to as selecting a "survivor" and a "loser" record. That is, the "loser" data record will be made inactive, which could cause real world problems for that person if done incorrectly, hence the need for documentation.

> [...] you choose who… we call it “the survivor.” You choose who will be "the survivor" and "the loser." It sounds very hard, but that's how we choose who will be the survivor. And all data from the survivor remains leading. And you can possibly still find somewhere — if you search very well — some personal details, data of when he was born and the like. But certain data, file documents. You really can't do any more; unless it is really stated in the file document under which customer it was. But other than that you really can't figure it out any more. Cases, only if file documents are linked to them and that file document contains a case number. But otherwise you really wouldn't know any more. There is nowhere that a copy is stored of this was the situation. None of that is there. That is also the reason why people fill out that memo. So that you at least — should it go wrong — that you notice a little, that you can figure out a little. (Interview 2020-11-02)

```{r deduplication, echo=FALSE, fig.cap="The author's conceptualisation of the IND's deduplication process."}
knitr::include_graphics("figures/deduplication-diagram.pdf")
```

The third stage of the deduplication process, which involves gathering information to help determine whether or not to deduplicate the records, also exemplifies the interconnectedness of the MK's various information systems and their transnational connections. As such, verifying identity records in other national and international systems is frequently mentioned in IND documents outlining the various justifications for choosing to deduplicate. One example of such a justification for deduplication is when the National Police find a link between two records by using data from Eurodac or EU-VIS, which are EU information systems.  In these situations, it is clear that other national or even international systems are involved in resolving local questions about personal identity data.

In general, the results suggest that duplicates in the INDIGO system are a recurring and significant issue for the IND. When everything goes well, tools like ELISE smart search and match assist INDiGO users in finding records and avoiding mistakes (mediated by the fuzzy search). However, respondents acknowledged that duplicates may still be created due to factors like time constraints and varying levels of familiarity with the tools. Thus, duplicates that still made it into the system are periodically searched for by a deduplication service. In an ideal world, checks would be performed immediately upon an INDiGO user attempting to add a new client record. Indeed, such automated checks already exist for some of the IND's online application processes. Nonetheless, these same automated procedures can also result in the creation of duplicate records, even if great care is taken to define the criteria and match score thresholds for potential duplicates. Together these results provide important insights into how re-identification occurs within a system-level bureaucracy. Most semi-, non-, and fully automated bureaucratic processes depend on being able to correctly re-identify clients. However, the findings suggest that sometimes it might be simpler to produce duplicate data that needs to be cleaned again later rather than stopping a bureaucratic process when re-identification fails.

## Discussion

The findings suggest that the IND's re-identification practices are influenced by its use of the ELISE software package for searching and matching clients in at least three ways. Re-identification is influenced by the software's expectation that there is always some uncertainty at play when searching through databases. In other words, these tools are built around the idea that both search queries and database records can have errors. When entering names into a database or search query, IND employees sometimes make typos, reverse the order of fields like first and last name, and other similar errors. In this way, the probabilistic method of handling data mediates every search query. Instead of returning a single value in response to a database lookup, the system returns a set of client records ranked by how closely they match the query. In the first place, the INDiGO system's fuzzy search algorithms and probability based ranking of matches mediate all client searches, whether they are performed by humans or automated systems.

Second, re-identification is influenced by the software package's method of assembling knowledge about identity data in order to enact equivalences between otherwise disparate practices. The tool takes into account a variety of factors, such as the fact that there is no universally accepted method of transliterating names, that people use diminutive names, or that people may adopt their spouse's surname after marriage. As a result, the ELISE search can be characterised by its attempt to compensate for, and eventually supplant, knowledge of all such variations and uncertainties in ways that no single person could. However, evidence from the various ad hoc search strategies indicates that these expectations do not always match actual use. A possible implication of the current design is a redistribution of competences and knowledge, with ELISE's operation supplanting users' knowledge of identity data and re-identification.

A third influence on re-identification is the materiality of identity data and the contrast between alphanumeric/biographical data and biometric data. While we have mostly focused on the former, it is worth noting that INDiGO originally also made use of ELISE's fingerprint matching capabilities to determine whether or not a client was already known by comparing their fingerprints at first registration to the entire IND database. Such "one to many" matching is no longer carried out, though, as fingerprints are now only used to verify an individual's identity. According to one participant, matching fingerprints only "one to one" simplified re-identification for the IND:

> Yes, the work processes have changed at the IND. So it wasn't that it [ELISE search and match] didn't work. Because, yes, we still use the same algorithm to match fingerprints, so to speak. That was already included in ELISE, actually on top of the software. But now it's always 1 on 1 finger match. So we get the fingerprint of the customer who says who they are. So someone says his number, for example, his v-number. And then the fingerprint is collected there and compared with what he puts on the counter in terms of fingerprint, on the scanner. (Interview 2020-08-05)

The decision to use only fingerprints for authentication is a surprising finding from the analysis because it appears to contradict the widely held belief that biometrics are a more reliable form of identification. In my discussions with WCC's tech staff, for example, the use of biometrics was consistently cited as being simpler and more objective than other methods of identification based on alphanumeric data. According to them, determining, for example, whether two fingerprints match is more precisely defined in biometrics in terms of mathematics or computation. When matching on less precisely defined alphanumeric data, context plays a larger role. As a result, biometrics are frequently regarded as a means to reducing identification uncertainty. But this view overlooks the reality that local conditions also affect biometrics. Prior studies have noted that it takes effort to get a good fingerprint scan, such as knowing where to place one's hand on the machine, keeping fingerprints moist, and so on [@kloppenburgSecuringIdentitiesBiometric2020]. It can be argued that these "materialities of information" [@dourishStuffBitsEssay2017], influence re-identification systems and practices. Expanding on the work of Dourish, it can thus be suggested that the findings indicate that re-identification is intrinsically linked to the shape that data takes. In other words, the materialities of more loosely defined biographic data versus more strictly defined biometric data facilitate and constrain re-identification in different ways.

A fourth influence on IND's re-identification is the architecture of the system-level bureaucratic INDiGO system. The service-oriented architecture, as previously stated, separates the application's searching and matching functionality from other system components. This decoupling from other parts of the business logic, such as the rules engine used to implement policy, appears to have both benefits and drawbacks. A benefit is that the search component can be efficiently run and maintained because it is not tightly coupled to other aspects of the IND system. A drawback is that the system does not take into account the context of where and by whom searches are performed. Yet, researchers have shown that such interactions with the system and context are mutually constitutive [@dourishWhatWeTalk2004; @suchmanHumanmachineReconfigurationsPlans2007]. The current architecture may have the effect of treating all users (staff from various units and other automated systems) equally as passive participants without regard to the context of their interactions.

In short, the findings contribute to debates on the materiality and performativity of identification in at least two ways. In the first place, the results shed light on lesser-known practices and technologies of reidentifying people across data infrastructures, thereby contributing to debates on the materiality of identification. By concentrating on re-identification, we were able to see how identifying people on the move involves more than just first registration and biometric identification data. Re-idenfication includes other processes such as determining someone's corresponding identity data in databases, or deciding if multiple database records are referencing the same real-word person (deduplication). Second, the findings show how data matching technologies and bureaucratic information systems have shaped the way routine bureaucratic re-identification is carried out. According to the findings, the technologies' incorporation of previously dispersed bodies of knowledge, such as rules and methods for matching identity data, results in a redistribution of competence and expertise among both human workers and machines. In addition, there are technical factors, such as the probabilistic approach and system architecture, which can cause a disconnect between the user's perception of how search works and the system's interpretation of a search query. More research, however, is needed to understand how technologically mediated re-identification influences bureaucratic discretion and, by extension, the (back office) enactment of identification policies [see, for example, @ustek-spildaStatisticiansBackofficePolicymakers2020].

## Conclusion

This study set out to investigate everyday bureaucratic practices and technologies of re-identifying clients across the data infrastructure of the Netherlands' immigration and naturalisation service. In general, the findings highlighted the interconnectedness of a software package for matching identity data with IND data practices for re-identifying clients throughout bureaucratic procedures. On one hand, the software package supports the work by the system's affordances to mitigate data quality problems and uncertainties. If, for example, IND staff make mistakes when registering or querying client data, the correct client should still be retrievable. On the other hand, it was shown that the system's design transforms search into a black box that incorporates and redistributes diverse re-identification knowledge between humans and machines. However, the various strategies used by IND staff to look up client information and to interpret search results point to the cognitive effort needed to re-identify clients.

Although the current study is based on a small sample of participants, the findings suggest several practical implications. First, the way ELISE computes and presents results highlights the materialities of personal identity data [@dourishStuffBitsEssay2017] and ranked results [@pollockRankingDevicesSociomateriality2012]. On the one hand, the materialities of INDiGO's data models and alphanumeric/biographic data shape how data can be used for re-identification. It is possible, for instance, to match on previous names, but it may be challenging for a user to comprehend when the system presents such a match on historical data in the results. The presentation and ranking of search results, on the other hand, influences how users make sense of the matches and, as a result, shapes re-identification practices. Overall, the interviewees' responses indicated a need for a more deliberate design on how to best guide users in formulating queries and explaining results. Additionally, INDiGO users may have different needs in terms of searching and matching identity data than a generic "view from nowhere" [@suchmanWorkingRelationsTechnology1993]. Instead, a different system design could make use of _cognitive frameworks_ [@sharpInteractionDesign2019] to model how users interact with technology and lessen the cognitive work required to complete tasks and, consequently, reduce the risks of creating duplicates.

Several questions remain unanswered when it comes to conceptualising (re-)identification in the context of system-level bureaucracies and device materiality and performativity. It is clear that the ELISE software package has a specific history in the way it brings together previously disparate bodies of knowledge, techniques, rules, and methods into a generic software for matching identity data. For instance, could the generic design of ELISE be part of the software vendor's strategy for "generification" [@pollockSoftwareOrganisationsBiography2009] of (re-)identification? The influence of software vendors and their broader market strategies has, unfortunately, been mostly ignored by current research on the design and use of such (security) devices. The focus of the following chapter will thus be on how this software has evolved over time and while being used by various organisations.
