# Data matching within organizations {#ch-dm-within-org}

\chaptermark{Data matching within organizations}

__Abstract__

\noindent

This article considers how everyday bureaucratic practices of identification and verification in migration management are intertwined with technologies for searching and matching identity data. These practices include rather mundane, and often technologically mediated, ways of ascertaining the identity of clients in steps of the bureaucratic procedures, such as residency or naturalisation applications. An interesting debate in the intersection between STS and critical security studies has replaced identification encounters as a problem of representation to identification as enacted and mediated by various actors. A compelling addition to this debate could come from the literature on street-level bureaucracy, which has argued that the everyday bureaucratic practices and use of discretion amounts to policy making. Yet, while this literature has debated the constraining or enabling effects of new technologies on the (discretionary) practices of civil servants, it has been less specific about the technologies. In this article, in investigate how particular expectations and materialities about data take shape in tools that give shape to (re-)identification encounters. I draw from fieldwork data on the design and use of a technology for searching and matching migrants' data at IND, the government agency implementing the policy on foreign nationals in The Netherlands. Findings from this study show how the software package mediates procedures and redistributes competences through the software’s affordances to mitigate data quality problems and uncertainties.

---

\vspace*{\fill}
\noindent
_Possibly insert citation here._
\newpage

## Introduction {#intro}

The stakes in identification can be high, and the specialized technologies to search and match identity data employed by authorities can greatly mediate identification encounters. An often invoked real-life example of the intricacies of identification is when one of the "Boston bombers", Kyrgyz-American Тамерла́н Царна́ев, was not pulled aside for questioning when he arrived at the JFK airport in New York City in January 2012 to board a flight to the North Caucasus. According to a report prepared by investigators for the U.S. House Committee on Homeland Security, and of which documents have been reviewed by the media outlets, he should have been identified as a person of interest and questioned [@schmittAgenciesAddedBoston2013; @winterRussiaWarnedTsarnaev2014]. In April 2013 he and his brother perpetrated a terrorist attack during the annual Boston Marathon. Back in 2011, Russian authorities notified their U.S. counterparts of his connections to terrorist groups. As a result, United States government authorities pre-emptively added his information to their watch lists and databases, such as the Terrorist Identities Datamart Environment, a database containing more than 1.5 million records of known or suspected international terrorists. Such watchlisting systems are designed to automatically compare information about people and to alert and instruct authorities on what to do when they encounter a person with matching identity data. In the case of Mr. Царна́ев, the system may not have triggered an alert due to incomplete information about his date of birth and variations in the transliteration of his name: "Tsarnaev" — "Tsarnayev." In my view, this example highlights at least three crucial aspects of contemporary digitally mediated bureaucratic practices of identification.

<!-- The stakes in identifying and matching identities can be high, and authorities generally resort to specialized technologies to deal with the inherent uncertainties and ambiguities in data. Louise Amoore [-@amooreBiometricBordersGoverning2006], for example, recounts how, post 9/11, dataveillance systems — integrating different databases and systems to create risk profiles for governing mobility — were set up, with the help of IT services and consultancy companies, to be used in the United States government launched war on terror. Similarly, companies use the case of the misspelled names of the Boston bomber as an exemplar for the necessity of their data matching technologies in watch list screening systems [see also @basistechnologyStrengtheningBordersIntelligent2021]. To some extent, the presentations used the case as an exemplar that narrates the importance of finding and linking identity information in, for example, criminal investigations. By now, an increasing number of domains and organizations rely on various _data matching_ technologies to support and coordinate work practices. Other examples of data matching can be found in systems that, for instance, link patient data in the health sector, identify passengers who may need further investigation at the border, or alert financial services if a client is on a sanctions list [@christenDataMatchingConcepts2012]. -->

First, from street-level bureaucrats to system-level bureaucracies, all face an amount of unavoidable uncertainties in their daily work of establishing and verifying the identity of their clients. Most organizations need to cope with some amount of uncertainty as databases will contain data that is incomplete, not current, incorrect — or, even contain duplicate entries that refer to the same real-world persons [@keulenManagingUncertaintyRoad2012]. Hence, ensuring that data are correct, complete, accurate and that they can be shared, used, processed by different parties and information systems has become vital for the correct functioning of bureaucratic procedures.[^gdpr] [^data] A report from the European Union Agency for Fundamental Rights, for example, investigated how data quality in EU information systems for migration and border control affects fundamental rights [@fraWatchfulEyesBiometrics2018]. The report’s authors remark that in relation to doubts about a person’s identity "authorities often suspect identity fraud when cases of data quality are the real reason for concern" (p. 81). For alphanumerical personal data (such as surname, date of birth, nationality), these data quality issues can have various reasons — which can actually seem quite banal. The case of Тамерла́н Царна́ев touches on the fact that watchlists databases need Latin-characters names, yet, transliteration can take many forms. Hence, working with different sources of data brings challenges of what I will call _re-identification_: such as identifying the right data for someone and who they claim to be, deciding if multiple records from different sources are referencing the same real-word persons, determining how accurately data records represent those persons [@batiniDataQualityIssues2016].[^reidenfication]

[^gdpr]: For example, Article 5(1)d of the General Data Protection Regulation (GDPR) states the principle of accuracy. According to this principle personal data collected and processed by organizations need to be "accurate and, where necessary, kept up to date" and that "every reasonable step must be taken to ensure that personal data that are inaccurate, having regard to the purposes for which they are processed, are erased or rectified without delay". Furthermore, several other Articles of the GDPR make statements about data sharing and data interoperability.

[^data]: The word data is often treated as a mass noun, and hence something that cannot be counted or divided (e.g., "the data is available"). In contrast, this article uses data in its countable plural noun form ("data are"). I follow the convention of using this form to highlight that data are multiple and "arise from and are used in varied circumstances worth acknowledging" [@loukissasAllDataAre2019].

[^reidenfication]: In technical literature, the term for re-identification is also used for the process de-anonymization, i.e., to discover the personal identity belonging to anonymized data. In practice, the outcome of a data matching can be a re-identification of individuals. As @christenDataMatchingConcepts2012 explains, re-identification is possible because "record pairs classified as matches in a data matching project can contain information
that is not available in the individual source databases that were matched" (p. 189). That is, matching data from different databases without identifying details can still (un)intentionally identify and disclose individuals in those databases.

Second, new technologies keep being introduced to deal with such "data frictions," and bureaucratic encounters of identification should thus be understood in their (changing) sociotechnical context. That is, researchers need to consider how technologies such as for data matching, their materiality and performativity, (re)configures practices of identification. This point resonates with calls from materialist debate in STS inspired critical security studies to empirically study (in)security and international politics through practices and devices [see e.g., @amicelleQuestioningSecurityDevices2015; @suchmanTrackingTargetingSociotechnologies2017; @hoijtinkTechnologyAgencyInternational2019]. Taking serious sociotechnical practices of re-identification would allow accounting for the complex and often less visible organizational and political consequences. In this way, for example, the quote from the Fundamental Rights Agency above can be turned around. Instead of asking if the problem lies in the data or border control practices, we must pay attention to how sociotechnical practices enact security subjects and potential identity fraud. Such a materialist and performative approach replaces discussions of identity as representation to account for heterogeneous set of actors involved in identification practices [@pelizzaIdentificationTranslationArt2021]. So far, however, literature has focused on more prominent moments of first (and often the biometric) registration (e.g., biometric refugee registration) and there has been little discussion about how people are re-identified throughout bureaucratic practices and infrastructures.

Third, as the street-level bureaucracy literature stresses, government policies are established through the discretion public employees use in their regular encounters with citizens, and to deal with more complex contexts despite detailed rules and regulations devised by legislators [e.g., @lipskyStreetlevelBureaucracyDilemmas2010]. Still, the practices and technologies to establish the identity of people and (re-)identify people throughout bureaucratic procedures have remained underexplored. Identification encounters are, however, procedures where there is often a lack of and divergence from rules and regulations, and, hence, room for discretion. An example where this tension is visible can be seen in @pelizzaIdentificationTranslationArt2021 where she describes the process of first registration at the Greek border and the back-and-forth between the applicant, a policeman, and a translator to determine the applicant's name from Arabic to Latin characters. In this identification encounter, the resulting name is, according to Pelizza, the outcome of a "chain of translations" of the migrant’s name from oral, to written, to finally end in the information system to act as the official version that will be used in further bureaucratic procedures. The process Pelizza describes is again very different from an example I encountered in The Netherlands where, in case there are doubts or refusal to give a name, the person will be assigned a name in the system using a label which includes information about the sex of the applicant, and the time and place of registration (e.g., "NN regioncode sex yymmdd hhmm"). In both these examples of identification encounters, public employees will do more than implement policies and regulations on identification, but apply their discretion to fit for the specific person and within the limitations and opportunities provided by a sociotechnical context.

However, while the street-level bureaucracy literature has long debated the constraining or enabling effects of new technologies [@bovensStreetLevelSystem2002; @buffatStreetlevelBureaucracyEgovernment2015], it has been less specific about the entangled technologies. Yet, it is clear that particular tool's expectations and materialities about data (and their quality) shape identification encounters. The designs, data models of technical solutions, such as for searching for a person’s record or for deciding if two identity data records refer to the person, embed many assumptions about those data [see also @pelizzaScriptsAlteritySecurityunderreview] that furthermore shape everyday work practices. In the case of identity data, such tools assemble knowledge and enact equivalences between otherwise disparate naming practices. Family names, for instance, may have variant endings for male and female which can, nevertheless be treated as matching. In this article, I therefore ask how clients of bureaucratic procedures in migration are _re-identified_ in data infrastructures throughout bureaucratic. My overarching goal is to contribute to the current materiality and performativity debate on identification by shifting focus from first registration to practices of re-identification in data infrastructures.

The research seeks to investigate the research gap on re-identification by empirically studying data matching in a practices and technologies of data matching in migration management. I analyse the design and use of a software package for searching and matching identity data deployed at the migration and naturalization service in The Netherlands (IND). The analysis draws on data gathered through fieldwork — interviews, documents, field notes — at the supplier of the data matching software and at the IND agency. Findings show that the design of the tools for searching and matching embeds assumptions about databases and their data records which give shape to bureaucratic re-identification practices. For instance, such assumptions include that different, incommensurable naming practices and conventions may exist, that databases thus will never be complete and accurate, and that there may be gaps between expected and actual data. It can thus be suggested that the use of data matching technologies comes with a new distribution of roles to provide resources and restrictions of how government agents search for identities, and what automatic matching rules will be executed.

Taken together, these findings contribute to the materialist debate on identification in the intersection of Science and Technology Studies and Critical Security Studies in two ways. First, combining these literatures with concepts from the street-level bureaucracy literature makes it possible to how policy in re-identification encounters is enacted and mediated through the specific sociotechnical contexts, such as for dealing with ambiguities in personal data. Second, while researchers have tended to focus on first registration, sheds light on other processes such as re-identification throughout bureaucratic processes and practices of working with uncertain data in migration management.

The next section aims to establish the links between the literature from street-level to system-level bureaucracies and the literature in the boundary between STS and Critical Security Studies on the materiality and performativity of identification. The remaining part of the article will discuss the empirical case and findings.

<!-- How do the findings contribute, what are design implications? -->

<!--chapter:end:sections/01-intro.Rmd-->

## Background and related work

<!-- This Section draws on and brings together strands of literature which are relevant to the design and use of data matching technologies within organizations. The concepts and techniques of dealing with the uncertainties in sharing, connecting, finding data are by now well established. Lesser known are the potentially transformative effects of such technological innovations when they are assimilated into workplace and security settings. Various approaches have been developed that can help in examining how technologies for searching and matching personal identity data shape and are shaped by these settings and their heterogeneous set of actors and practices. Overall, I follow calls for being specific about technologies and the kinds of expectations and materialities they join together in seeking to reduce uncertainties about personal identity data. -->

Many interactions between migrants and public authorities involve some form of identification, to establish or verify the identity of the person in various bureaucratic processes relating to granting asylum, issuing, residency permits, naturalization, and so forth. As such, the practice of identification is a Janus-faced and multi-sited phenomena intermingled with obligations and claims for benefits [@aboutIdentificationRegistrationPractices2013; @caplanDocumentingIndividualIdentity2001]. The bureaucratic organizations dealing with such individual cases are, of course, based on regulations and administrative routines. Yet, as the street-level bureaucracy literature has shown, public-service workers in charge of such cases are not just implementing policies, but are actively involved through the discretion workers employ. What then does it mean then to consider practices of (re-)identification as part of everyday bureaucratic practices?

### Identification as a bureaucratic practice

The foremost understanding of the concepts of street-level bureaucracy and discretion were articulated by Michael Lipsky and popularized in his 1980 book: "Street-level bureaucracy: dilemmas of the individual in public services". In this widely acknowledged view, diverse front-line public service workers influence public policy through their regular interactions with the general public. As [@lipskyStreetlevelBureaucracyDilemmas2010, p.13] states: "street-level bureaucrats have considerable discretion in determining the nature, amount, and quality of benefits and sanctions provided by their agencies." For instance, in an identification encounter between a border guard and a border crosser at passport control, the border guard may have some freedom to make a decision to permit entering the country. Lipsky’s original argument, however, needed updating with the increased use of digital technologies in public administration and the rise of e-government. More recent literature on street-level bureaucracy and discretion pays particular changes on the transformations brought about by the digitalization of public agencies has forced scholars to rethink the role of the street-level bureaucrats and their daily interactions [@bovensStreetLevelSystem2002; @buffatStreetlevelBureaucracyEgovernment2015; @buschDigitalDiscretionSystematic2018; @snellenElectronicGovernanceImplications2002].

In an influential article, @bovensStreetLevelSystem2002 conceptualized the transformations brought about by ICT into, first, "screen-level" and, second, "system-level" bureaucracies. The first concept draws attention to how interactions between officials and citizens became increasingly mediated through computer screens. For instance, the personal data of a residency permit applicant is filled out using electronic template forms in a case management system. Or, increasingly by applicants themselves as citizens are provided access to government information systems [@landsbergenScreenLevelBureaucracy2004]. Meanwhile, the decision to grant the permit is guided by decision trees, business rules, algorithms that model the policies and regulations. The concept of system-level bureaucracies was thus coined to refer to an even more intensified digitization of data collection and processing and resulting organizational changes. In ideal terms, @bovensStreetLevelSystem2002 defined the transformed role of practitioners in such an organization as follows:

> "The members of the organization are no longer involved in handling individual cases, but direct their focus toward system development and maintenance, toward optimizing information processes, and toward creating links between systems in various organizations. Contacts with customers are important, but these almost all concern assistance and information provided by help desk staff. After all, the transactions have all been fully automated." (p. 178-179)

In the transformations from street-level, to screen-level, and system-level bureaucracies lies a widely debated tension between automation and digitization as desired for those reasons of fairness, efficiency versus undesired as a removal of human judgements and autonomy in decisional processes to adapt according to local and individual circumstances. In a literature review on these debate, @buffatStreetlevelBureaucracyEgovernment2015 has singled out two major sides which she calls "curtailment thesis" and the "enablement thesis". According to her, authors which can be grouped in the first thesis generally argue that discretion of front-line is curtailed by ICT and shifts to other actors. In this view, if, for instance, an information system can make an automated decision for granting a residency permit it would be less subjective as it can exactly follow policies and regulations. Such thesis is of course overly technologically deterministic. Other research, which @buffatStreetlevelBureaucracyEgovernment2015 groups in the "enablement thesis," highlights a more complex role of technologies, not just as contrary to discretion, but as resources in sociotechnical arrangements that constrain and enable interactions between technologies, frontline workers, citizens.

Two important themes emerge from the studies discussed so far. First, that agents enact policies through their daily practices and use of discretion. Second, that changes in the sociotechnical systems of bureaucratic organizations can curtail or provide additional resources to everyday bureaucratic practices. What is not yet clear is the impact of these two notions on how people are re-identified throughout bureaucratic practices.

<!-- The rise in automated decision making, artificial intelligence. Digital discretion as "the use of computerized routines and analyses to influence or replace human judgment" [@buschDigitalDiscretionSystematic2018, p. 4] -->

### Materialist and performative approaches to identification

There is a growing body of literature that recognizes the importance of identification as intermingled with government’s granting obligations and rights (e.g., citizenship, residency), as well as coercive measures [@aboutIdentificationRegistrationPractices2013; @caplanDocumentingIndividualIdentity2001]. Traditionally, academics have also emphasized the links between state-making of modern nation states and the elaboration of registration and identification systems, such as the creation of civil registers or passport documents [@breckenridgeRegistrationRecognitionDocumenting2012; @caplanDocumentingIndividualIdentity2001; @torpeyInventionPassportSurveillance2018]. An often referred to term for the state’s capacity of identifying its citizens as a form of is the notion of _legibility_ of @scottSeeingStateHow1998. Scott noted how increased interaction of states and their population (e.g., for purposes of taxation) went hand in hand with projects of standardization and legibility as attempts to unambiguously identify its people. So, in the example Scott, while cultural naming practices are very diverse and can serve local purposes, the standardization standardized surnames "was a first and crucial step toward making individual citizens officially legible" (p. 71). In these practices, the identity of the person is not a problem of representation between a person and information captured about them, but as a reduction of multiplicity with a mutually enactment of identity, states, institutions [@lyonIdentifyingCitizensID2009; @pelizzaIdentificationTranslationArt2021].

A growing body of literature in the intersection between STS and Critical Security Studies has added an important dimension to the discussion on identification by accounting for the materiality and performativity of devices and practices[@coleSuspectIdentitiesHistory2001; @gargiuloMonitoringSelectingSecurity2017; @skinnerRaceRacismIdentification2018; @pelizzaIdentificationTranslationArt2021; @suchmanTrackingTargetingSociotechnologies2017]. @bellanovaControllingSchengenInformation2020 for instance have studied the actors and practices involved in the processes of maintaining the EU Schengen Information System (SIS). The SIS system allows authorities to create and consult alerts on, among other, missing persons and on persons related to criminal offences. By looking at how these alerts "acquire the status of allegedly credible and accurate information that becomes available to end-users through the SIS II" (p. 2) they make evident its role in conditioning international mobility. @fors-owczynikMigrantsRiskIdentity2015 have shown how three systems in The Netherland translate and frame categories of risk to identify potentially risky migrants and travellers. Incorporating the role of sociotechnologies into the analysis of identification practices makes it possible to empirically explore security and account for the materiality and performativity of devices and practices of identification of migrants.

Very little, however, is currently known about how practitioners deal with uncertainties of personal identity data in re-identification encounters. For instance, a report from the European Court of Auditors, describes how "when border guards check a name in SIS II [the Schengen Information System], they may receive hundreds of results (mostly false positives), which they are legally required to check manually" (ibid., p. 31). In this case, the mentioned SIS II system computes and presents excessive amounts of matches that cause frictions in everyday practices of border control. The question remains of how these bureaucratic practices of re-identification are enabled or constrained by technologies for dealing with data uncertainties.

### Data uncertainties in practices of re-identification

Critical data studies have made it clear that data is never "raw", that databases can be messy and with mistakes, and that work is therefore needed to put data to use [@gitelmanRawDataOxymoron2013; @loukissasAllDataAre2019]. For example, a report from the European Court of Auditors mentions that a major EU information system supporting border control contains millions of potential data quality issues, such as first names recorded as surnames or missing date of births [@ecaEUInformationSystems2020]. Many such discrepancies are related to work practices and issues of fitting local circumstances to global standards [@bowkerSortingThingsOut1999]. In a Chapter on the International Classification of Diseases as a standard for coordinating work, Bowker and Star describe the difficulties of busy doctors to fill in death certificate and how "the act of assigning a classification can be socially or ethically charged" — such as classifying a death as a suicide. And as @loukissasAllDataAre2019 remarks, it is clear that databases might contain lots of such errors hence "local knowledge [is needed] to see that such errors are not random" (p. 67). In this way, data are testimony to the local conditions of their production that need to be linked across space and time in future processes of re-identification. If data quality problems and uncertainty are facts of life[@keulenManagingUncertaintyRoad2012], then organizations need to cope with this uncertainty in practices of re-identification.

The technical mechanisms for (re-)identifying if two or more persons represented by data records actually refer to the same real world persons or not is itself known by many names: object identification, entity resolution, record linkage, data matching [@batiniObjectIdentification2016]. Basically, these techniques will compare attributes of data records and use classification methods to determine matches [@christenDataMatchingConcepts2012]. Numerous classification techniques exist: some may be based on following specific rules, while others may use more probabilistic methods. Metrics can, for example, calculate the similarity of two sequence of characters based on the number of operations that would be required to transform one into the other. In this way, the names "Sam" and "Pam" may be considered closely related (was it a typo?). Other approaches may even calculate such similarities by comparing how names are pronounced (in English). Rules based matching may include rule such as to ignore honorifics and titles (e.g., Mr., Ms., Dr.) when matching personal data. In this way, knowledge, techniques, rules, methods for matching identity data are assembled into sociotechnologies that inform organizations and practices of identification.

Data deduplication is commonly used to refer to the problem of data matching when dealing with duplicate records from a single data source. A migrant may, for example, be mistakenly registered more than once in a system due to technical issues. Generally, a deduplication process will then periodically compare records with every other record in the database to score its likelihood of matching. After determining the set of possible matches, a domain expert will need to be involved to decide on found matches. A final step of the data matching is then to fuse the multiple records referring to the same entity into a single record. More interesting is how the process of ‘normalizing’ duplicates can be ‘a key to learning about the heterogeneity of data infrastructures’ [@loukissasAllDataAre2019]. Therefore, following Loukissas, deduplication can give insights into the heterogeneous practices of identification in migration and border control.

This section has attempted to provide a brief summary of the literature relating to identification in relation to street- and system-level bureaucracies, the performativity and materiality of devices and practices of identification, and problems and solutions for dealing with data uncertainties. Essentially, I find that a compelling addition to this debate could come from the literature on street- and system-level bureaucracy. In this view, the everyday bureaucratic practices of re-identification and use of discretion would amount to identification policy in practice. For this reason, it is crucial to investigate how particular sociomaterialities give shape to (re-)identification encounters.  The next section describes the empirical case and methods used to investigate these issues.

<!-- ### Data practices and technologies in security contexts -->

<!-- While the impact of new information technologies on the workplace has been the subject of long debates in fields such as CSCW [see for example, @bowkerSocialScienceTechnical1997; @leonardiMaterialityOrganizingSocial2012; @orlikowskiSociomaterialityChallengingSeparation2008; @pollockSoftwareOrganisationsBiography2009], only recently these effects have started to gain more attention in practices of migration and border security. In essence, many theories and frameworks have examined and theorized about technologies’ (in)ability of transforming organizations and the coordination of work [@pollockSoftwareOrganisationsBiography2009]. While some scholars have theorized the introduction of information technology as a force of organization change [e.g., @kallinikosWorkControlComputation2009], others instead emphasized the struggles encountered by actors of fitting global software to local circumstances [e.g., @hansethDevelopingInformationInfrastructure1996]. In addition, social constructivists have stressed the tentative meanings and specific contexts and actors in which artefacts emerge [@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. Critical studies of technological innovations and knowledge practices in migration management, borders, and security have thus also incorporated these aspects when examining how devices, practices, actors are mutually constituted and entangled. -->

<!-- ### Configuring data matching -->

<!-- STS scholars have devised different approaches to examine and understand the relations between designers, users and technologies [@oudshoornHowUsersMatter2003], and thus how expectations of designers can be inscribed into devices of migration and border security. For example, the approach of script analysis was advanced to examine the assumed competences of users and affordances that are embedded in artefacts [@akrichDescriptionTechnicalObjects1992]. The script of an artefact then requires users to adopt designer’s envisaged behaviours and actions to interact with an artefact. This approach also allows for accounting for situations when those assumptions by artefact designers don’t match the actual uses and practices. In the context of border security, for instance, a biometric fingerprint scanner might have certain assumptions about bodies to make them legible [@kloppenburgSecuringIdentitiesBiometric2020a]. @woolgarConfiguringUserCase1990 furthermore noted how system designers might attempt to ‘configure the user’ of the system by incorporating the user into the sociotechnical system. The method to understand such configuration is by treating the computer systems as a text that is read by users and can be interpreted differently. Designers may thus attempt to anticipate and delimit this flexibility. In these views, a sociotechnical system functions well when users and the system are successfully configured together. -->

<!-- While notions such as ‘script’ and ‘configuring the user’ have been generative in understanding relations between users and technologies, they tend to overemphasize the role of certain actors such as designers [@oudshoornHowUsersMatter2003]. More recent approaches have therefore suggested to, for example, take a more longitudinal approach to examine important moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. Suchman [-@suchmanHumanmachineReconfigurationsPlans2007] furthermore argued that the inscriptions of users and uses are never that coherent, and that a more open-ended and indeterminate approach towards artefacts is required. Analyses of sociotechnologies of migration and border security should therefore include a wider range of actors and moments of design, development, deployment, use. -->

<!-- Configuration can, in accordance with Suchman’s view, also sensitize us to the material and discursive elements of software for dealing with data uncertainties. She distinguishes two broad uses: ‘First, as an aid to delineating the composition and bounds of an object of analysis, in part through the acknowledgement that doing so is integral not only to the study of technologies, but to their very existence as objects’ (ibid., p. 48). The software tools for searching and matching data, for instance, structures the way officials can find and access information about people, and set boundaries between them, such as their capacities for acting on searches and results. The second use she describes is in ‘in drawing our analytic attention to the ways in which technologies materialize cultural imaginaries, just as imaginaries narrate the significance of technical artefacts’ (ibid., p.48). In the case of data management such expectations may be in the way data quality uncertainties are thought to be resolved. For example, either by focusing attention on activities to detect and correct data issues, or by making systems directly cope with uncertainty in data [@keulenManagingUncertaintyRoad2012]. -->

<!-- Following these approaches, this research aims to answer the question of what kinds of expectations and materialities are joined together in technologies for data management and reducing semantic uncertainty. Analysing and comparing the technical designs with actual uses, and thus also where they mismatch, can give evidence of the various expectations and assumptions at play. Yet, at the same time we should be both more specific to the situatedness of the design and keep a more open-ended and indeterminate view of how such technologies are put to use. -->

<!--chapter:end:sections/02-literature.Rmd-->

## Empirical case and methods {#empirical-case}

The research presented in this article draws on data from fieldwork conducted digital and in person from July 2020 to July 2021. In this research, I investigated how particular forms of searching for identity data provided by a data matching solution — such as taking for granted uncertainty about data and the incommensurability of different naming practices — embed particular expectations that shape users, their work practices, and the organizations where it is deployed in particular ways. I made use of standard methods (observations, interviews, document analysis) to situate and analyse the interactions between technologies, workplaces, and work practices [@ackermanResourcesCoevolutionArtifacts2008; @luffWorkplaceStudiesRecovering2000; @suchmanHumanmachineReconfigurationsPlans2007]. The fieldwork was informed by an initial understanding of the field and the use of this technology in transnational information infrastructures for migration and border control. As such, the research design can be considered a kind of "strategic ethnography" [@pollockEInfrastructuresHowWe2010] with specific choices regarding the setting and the scope of the research. Overall, the research is part of a more comprehensive endeavour to examine co-evolutions of development and deployments of a software solution deployed in EU and Member State identity management systems and practices of identification.

During the fieldwork period I joined the Netherlands-based company WCC Group, which created a software product for dealing with data matching and data deduplication, and which is used, for example, in the exchange of visa data in the central system of the EU Visa Information System (EU-VIS), and in the case management system of the Netherlands’ immigration and naturalization service.[^wcc].  More specifically, I joined the "Identity Team" (ID Team) of WCC (an organizational unit handling the company’s "Identity and Security Solutions" segment) to investigate the design, use, and evolution of their software solution for searching and matching identity data. Following the research design, it was crucial to be able to examine, on one hand, the design and development of the software, and, on the other, its use by institutional actors in practices of identification.  As a temporary member of the ID team, I visited the company’s headquarters in Utrecht (The Netherlands), had access to relevant documentation, carried out individual interviews with people, and attended some of the team’s joint meetings.

[^wcc]: Previously, the company was named WCC Smart Search & Match.

The research focused on comparing technical details of WCC’s software system called the "ELISE ID platform" (ELISE) — previously known as "ELISE Smart Search & Match" — with its use in processes of identification. The software products for searching and matching data are, among others, used by government agencies with some forms of identity management systems, such as for searching, matching, linking data information about clients. Briefly stated, the ELISE software provides advanced functionalities for searching and matching of information on customer’s existing databases. The novelty of the technology comes from using various kinds of algorithms to match identity data. For instance, by taking into account birth of date data may be incomplete or switched values for the date and month. Essentially, a search for a client’s data (e.g., based on their name, nationality, date of birth) will return a set of records with values indicating the probability that two identity data records match, rather than only exact matches.

Stated differently, the software is designed to deal with inexact searches and matches arising from difficulties of matching between personal data from different locales, scripts, cultural contexts, and so forth. Furthermore, the various algorithms expect that there can be mistakes in both the database and the parameters of a search. The starting point of this research was that such a subtle approach to identity data would shape identification practices through the tool’s specific characteristics and capabilities. The research assumes that in order for such search to work well, for (street-level) bureaucrat to effectively use the search and interpret the results, both the system and its users need to be attuned [@suchmanHumanmachineReconfigurationsPlans2007; @woolgarConfiguringUserCase1990]. Focusing on the kinds of frictions users encounter while using the search and match functionalities in the IND systems was therefore thought to highlight the underlying assumptions and expectations in the sociotechnical configuration.

For this article, I draw on the analysis of the integration of the ELISE ID platform with the systems of the immigration and naturalization service of the Netherlands (hereafter, IND). The IND is responsible for, among others, processing the applications from people who want to stay in The Netherlands or who want to become Dutch nationals. Practically, the IND case management system uses the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system and to deal with data issues such as duplicate records. Conceptually, the use of the ELISE search and match solution at the IND is relevant to understand how these heterogeneous elements are formed together and shape bureaucrat’s practices of identification.

To begin, I sought to find evidence of frictions between design and use of the tools by comparing how different organizational actors at the IND use the data matching capabilities in their daily tasks when searching for identities. Such comparison was thought to help discover how users’ expectations about data — and uncertainty surrounding data — is influenced by the software solutions. That is, how these expectations may be the source of tensions in expected and actual use of the search and match functionalities. Under such circumstances, frictions can also give evidence of discrepancies between users and the system of how to search and what constitutes good data. As I will explain further, there are indeed differences between the designed probabilistic identity matching, and user’s understanding of, for instance, the search results of the data matching process.

### Document analysis

The design and expected uses of the software were analysed through documents related to, first, the more generic technical details of the ELISE ID platform. And second, through documents related to the specific implementation of the platform at the IND. Members of WCC’s ID Team provided me these documents — such as technical design documents and meeting minutes —, as well as additional context and updates on current developments. My analysis of documents was furthermore grounded in understandings developed by STS of documents and practices of documenting as objects of study. As @shankarRethinkingDocuments2016 explain, different STS scholars broadened the understanding of documents as artefacts that don’t just document and stand for something in the world. Rather, documents play a role in social contexts — such as accounting for and coordinating of workplace activities — and cannot be separated from the practices through which they are produced.

Analysing the technical documents enabled, on one hand, to form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents gave insights in the genealogy of the package and the practices of configuring, deploying, designing the software. For example, by looking at different versions of documents (changes within the documents, added annotations, etc.). In addition to the more technical documents, public communications and reports helped gain contributory insights into the specific context of developments of the IND information systems. The outcome of the document analysis served as input for developing the interview protocols.

### Interview

The next step of the research was to gain insight into the actual development and use of the search and match tools, and to contrast this with the designed and expected use. For this, I conducted semi-structured interviews with a diverse set of actors. This approach was thought to contrast views on the development, expectations, uses of the tools for searching and matching identities. Overall, my interview approach revolved around two main groups of themes and participants. The first group resolved around people at the IND whose tasks involve the searching and matching of identities in their databases — and which thus involves the use of the ELISE ID platform. The second group centred around people at WCC who are, more generally, involved in some way or another in the development, deployment, (pre-)sales of their software. This approach made it possible to examine the mutual adaptations of the ELISE technology, WCC, and the IND.

Members of the ID Team facilitated establishing first contacts with the IND organization. Due to the Covid-19 pandemic related measures it was not possible to have any in person interviews. For this reason, participants were contacted to schedule an online meeting or phone call. All things considered, a benefit of these online meetings may have been that it was in fact easier to set up interviews — neither me nor participants needed to travel. On the other hand, not all communication may have come across in the same way, and it may have hampered the possibility for networking and scheduling more interviews. After the first contacts, a kind of snowball sampling was used for reaching out to more users willing to contribute to the research. In total, I conducted five interviews regarding the use of the search and match tools at the IND and seven interviews with people from WCC. Each interview lasted around one hour. However, I had little control over the sampling size of respondents and the reliance on their networks furthermore lead to a bias in who was included — in this case more senior people of the IND and WCC.

### Data coding and analysis

For analysing the fieldwork data, I followed standard methods for coding and analysing qualitative data. After collecting and preparing data from documents and interviews (including transcription), I coded and analysed the data using the computer-assisted qualitative data analysis software ATLAS.ti. After an initial inductive coding based on common themes and ideas, I reviewed and gathered similar codes to find patterns, processes, and typologies. For example, I developed a non-exhaustive typology of data frictions for alphanumeric identity data attested by the IND interview data. This typology then included types such as: frictions resulting from human errors during data entry, ambiguities and incommensurabilities in transliterations, differences in identification policies, and so forth. In the next section, I will present the principal findings of the comparing the differences in design and use of the search and match tools in practices of re-identification.

## Findings

### Identification at the IND

<!-- The IND primarily processes applications from people who want to stay in the Netherlands or who want to become Dutch citizens. The agency therefore handles large amounts of information about people and their cases with the help of a comprehensive information systems called _INDiGO_. This system supports the various tasks of administering interactions with applicants, processing their applications, and collaborating between different organizational units and partners. Internally, the INDiGO information system leverages the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system. -->

The bureaucratic processes and practices of (re-)identification at the IND agency and are best understood through the IND’s information infrastructure and its connections to other governmental and non-governmental organizations through the _migratieketen_ (lit., migration chain, hereafter MK). Roughly speaking, the MK is a formal collaboration between various organizations and ministerial agencies in The Netherlands — called _ketenpartners_ (lit., chain partners, KP) — that play a role in the processes that foreign nationals go through: from entering the Netherlands to obtaining a residence permit or departure or expulsion. In this MK the IND is what is known as an _identificerende ketenpartner_ (lit., ), which are those organizations authorized to determine and register personal data of foreign nationals in the common registers and to change existing personal data. This means that only the _Vreemdelingenpolitie_ (national police) and _Koninklijke Marechaussee_ (national gendarmerie) can assign a unique identity to an undocumented person. Identification in processes of the IND should thus be understood in relation to a wider collaboration and coordination of migration policy and information exchange.

The different organizations thus have a common interest in uniquely identifying migrants throughout the MK. Various mechanism are thus in place to achieve a shared responsibility for ensuring high quality personal data. An important aspect of this shared interest can be seen in the standardized way of identifying persons described in the document "Protocol identificatie labelling (PIL)" [@ministerievanjustititieenveiligheidProtocolIdentificatieLabeling2020]. First and foremost, this document defines standardized methods for the identifying and registering foreign nationals in the MK. Second, it describes data governance processes such as how this identity data may be modified or when data must be destroyed. Third, the document describes the various partners, their links in the information architecture and role in the MK. For example, all organizations of the MK need use the _Basisvoorziening Vreemdelingen_ (BVV), the central system which stores the basic information of foreign nationals and issues them a unique identification number and acts as a kind of single source of truth. Yet other partners may have their database and information about a person, so data needs to be kept aligned [@InformatievoorzieningVreemdelingenketen2015]. What such mechanisms for ensuring uniquely identifying data and hence further re-identification, is the outcome of practices that reduce the multiplicity of possible identity data.

(Re-)identification happens in various information streams of the IND, such as related to applications from people who want to stay in the Netherlands or who want to become Dutch citizens. The agency can, for instance, receive new applications, such as an application for a residency permit. In this case the person applying for this document is commonly not yet in the system (although they may be known in the MK/BVV). In other cases, the agency may receive new information to add to an existing applicant’s file. However, the difficulty that IND employees face regarding these two information streams is determining if the applicant is already known to the IND or not. To this end, the search and match tools available in the systems are used to either find and link the new information to an existing record, or, alternatively, create a new data record. Furthermore, the processing of these information streams combines various forms of materiality (documents sent by post, direct contact with clients, digital information streams) and (non-) human actors (front and back office employees, automated systems).

The IND’s information infrastructure can be seen as an example of system-level bureaucracy [@bovensStreetLevelSystem2002] which furthermore aims to separate the application of policy as applying business rules related to the Dutch Aliens Act from information management such as data storage, searching and matching [@kpmgitadvisoryAuditINDiGOWillen2011]. Technically, this separation in the case management system (called INDiGO) is accomplished by use of a Service Oriented Architecture (SOA) design pattern; a software architectural pattern which aims to compartmentalize different aspects of the system into self-contained services. The INDiGO system includes the ELISE ID platform as one such service to facilitate searching for applicant data against the biographic information in their back-office system. Figure \@ref(fig:landscape) gives a rough visualization of the architecture of the IND’s information infrastructure supporting the various tasks of administering interactions with applicants, processing their applications, and collaborating between different organizational units and MK partners.

```{r landscape, echo=FALSE, fig.cap="A high level sketch of the INDiGO architecture."}
knitr::include_graphics("figures/ind-landscape.pdf")
```

### Different types of searching and matching

As a separate component in the architecture of the INDiGO system, the ELISE searching and matching is designed to function in a generic and decontextualized manner. Technical documents describing the system report how IND end-user applications call the ELISE service with a query, which, in turn, runs the matching algorithms and returns the results for display in the graphical user interfaces. As such, the searching and matching service has minimal context about where in the INDiGO system and process it is called, nor about who is querying the system. This first finding from the document analysis runs contrary to STS research on the co-construction of users and technologies [e.g, @hyysaloNewProductionUsers2016; @oudshoornHowUsersMatter2003, @woolgarConfiguringUserCase1990]. The ELISE service, in its aim to provide a generic search and match computer package, does not consider specific users. And unlike the process described by @pollockFittingStandardSoftware2003, there has been little back-and-forth between a "generic user" from the software package and a more "specific user" for the IND system. What does this lack of specific users mean for the practices of identification at the IND?

The interviews made it clear that different kind of uses of the searching and matching can be distinguished. Based on the responses I received from IND employees, general uses of the search tools for searching can be differentiated from more specific uses that include verification and substantiation. The former includes most common uses of the search tools for searching a person by making use of rather clear-cut personal data such as a name or identification number. These IND employees need to, for example, process new information related to an application. This person may interact directly with the applicant at a font office counter, or process documents at their back office desk. A distinguishing characteristic of these general uses of the search is that workers usually do not require much interpretation of the input data (identification numbers, personal data) to query the system.

However, there may be some degree to which the personal data to input and search for becomes more ambiguous and open to interpretation — and hence, error-prone. This category could include users who for example deal with phone calls or handwritten documents. In such cases, more effort may be required to understand or piece together the information to formulate a search query. For example, the client’s name in a handwritten document may be incomplete or hard to decipher. Under such circumstances some features of the search tools may become more relevant. In the case of phone calls, for instance, functionalities of ELISE such as that it takes into account different phonetic versions of a name, or that numbers or letters may be accidentally switched may be more relevant. Whereas functionalities that take into account transcription errors and name variations may be more relevant for users dealing with handwritten documents. In these cases it becomes clear that in the processes of re-identifying clients the information can be conceived as chains of "translations into legible identities of individuals" [@pelizzaIdentificationTranslationArt2021, p.1] that is mediated by the ELISE system.

Another distinguishing feature between uses of the search and match tools is the need to interpret and scrutinize search results. On one hand, there are uses where there is clear-cut input, but who instead need extensive interpretation of results. For instance, there are important automated exchange of information with other MK partners, which requires manual intervention if the process fails to automatically identify the right client to link the data. Municipalities in The Netherlands, for example, send residency data to the IND via an automated information exchange. The corresponding client will be identified for using the same search and match tools (selecting the one with the highest match percentage). If no good match if found, an IND employee will need to be involved to identify the corresponding client in the systems of the IND. Such examples provide a different perspective on the discussion on discretion in relation to automation in the public sector [@petersenRoleDiscretionAge2020] by being specific about the technologies [@monteiroSocialShapingInformation1996]. In practice, identification encounters sometimes automated, sometimes needs intervention, and discretion is carried out in relation to specific sociotechnical system.

Finally, there are cases where, at the same time, input may be unclear and results need careful examination. For example, the first registration and startup of a procedure for a client — especially when this entails processing written documents. As mentioned previously, documents may be ambiguous and contain mistakes. At the same time there is additional uncertainty and need to scrutinize whether a person already is registered in the IND or MK databases. For instance, someone may have been registered a long time ago under their previous name before marriage.

```{r matrix, echo=FALSE, fig.cap="This visualization compares the needs for interpreting the input with needs for interpreting the results in the form of a matrix."}
knitr::include_graphics("figures/ind-users-matrix.pdf")
```

Based on these findings, it is clear that there are discrepancies between the expected use of ELISE and the actual practices by IND personnel. In contrast to the generic, decontextualized designed use, the actual users of the systems do have distinct needs (Figure \@ref(fig:matrix) summarizes these distinctions described above). The context of search may matter as users have specific needs and expectations from the search due to the amount or kinds of information they have available for processing an application. Some users can, for instance, search for a person’s record based on an existing identification, while others such as those dealing with postal pieces have to do more difficult searches based on more ambiguous handwritten and incomplete data. This discrepancy may be one reason why some interviewees indicated that they encounter difficulties when using the search. For example, they mentioned that they need to perform multiple searches with different combinations of search criteria.

As a matter of fact, the expected use is closely aligned with another use of ELISE at the IND: in automatic search processes. The IND uses such automated searches when, for example, an applicant takes up a residency at a municipality in The Netherlands. Municipalities will send automatic updates to the IND with their personal information, which will subsequently trigger a search query in INDiGO. These automated uses highlight how the ELISE search has a kind of inbuilt indifference to the context in which it is used. This indifference stands in contrast with specific needs and expectations of actual users.

### Use and understanding of search functionalities

Expectations in the tools can also be detected by contrasting the design and functioning of ELISE with the knowledge users at the IND have of how the search functionalities provided by ELISE actually work. There needs to be some alignment for the search to work well so that they are able to accomplish their daily tasks. Otherwise, this lack of knowledge can affect effective use of the tools and can, for example, result in the creation of duplicate records when the correct record was not found through the search tools.

In general, interviewees indicated that they have some knowledge of the various functionalities of the fuzzy search. For example, commonly mentioned features of the searches included that they are aware that the matching system takes into account differences in spellings of names, name variations based on transliterations, variations in numbers such as for date of birth or identification numbers. However, more advanced features such as matching based on name initials or name variations, (de)compounding names, or the matching based on affinity matrices did not seem to be well known. To some extent it can therefore be deduced that the system enacts such knowledge as implicit or of little importance for the users. Following this reasoning, what makes ELISE "smart" is its capacity to supersede the control of the search from the user. In this, sense there is evidence of kind of redistribution of knowledge and expertise about identification between the employees, systems.

<!-- These issues could also be seen as a problem of the configuration of ELISE in INDiGO. By design, ELISE does allow for the possibility to change, for example, the behaviour and weight of different search criteria. In the INDiGO implementation this is however not the case, and so for such less experienced users it is clear that the graphical user interface does not seem to offer much help in formulating effective queries. Essentially then, the ELISE component has no control on the GUI — neither how queries are specified nor how results are displayed. Learning to use the tools is mainly accomplished by getting help from colleagues or simply from practical use of the tools. Depending on how extensively users have to rely on the tools, there can also be big differences among the organizational units on how much knowledge about how the search and match functionalities work. -->

### Interpreting the search results and search strategies

A third approach to uncovering expectations in ELISE is by comparing how the system returns search results to the users — i.e., ranked by match percentage — with how users interpret those results. The top results — those with the highest match percentage — are supposed to indicate the most relevant matches. For example, a query based on a family name and a date of birth would include a list of results with data approximately matching those fields of data, including slight variations of the name or date. In theory, a person will easily identify the matching personal data within the top results. In practice, this is not always the case. From the interviewees’ responses it became clear that users have developed alternative ways of interpreting results and understanding special cases. Sometimes it is simply not clear to users why the results include certain records — unless they know from experience.

Such ambiguities are revealing of the frictions between the expected use of the ELISE search and users’ actual expectations and experiences. In general, the search is designed to work best by providing as much information as possible in the search query. The matching engine of ELISE can then, ideally, use all this information at once to calculate match scores. The result of this friction is that users appear to have developed their own "search strategies" (Table \@ref(tab:strategies) provides some examples). Strategies include manually trying different combinations of data, or even withholding some information that a user can then use to cross-check the results. This latter strategy especially runs counter to the supposed optimal functioning of the search engine by providing as much information as possible.

```{r strategies, echo=FALSE, ft.align="center"}
path = file.path(DATA_PATH, "search_strategies.csv")
data = read.csv(path, header = TRUE)

data$Code = substring(data$Name, first=34)

block_table(data[,c("Code", "Comment")], header = TRUE, properties = pt)
```

The results in this section indicate that the software package has become interwoven with data practices for identifying clients throughout the IND’s bureaucratic procedures. On one hand, the software package supports the work of re-identification by mitigating common ambiguities and uncertainties regarding identity data. On the other hand, the current design of the search makes the searching and matching algorithms a black box that is difficult to use. The next chapter, therefore, moves on to discuss implications of these findings.

### Duplicates and deduplication

Many of the frictions between designed and expected uses of the tools seem to contribute to a persistent problem of duplicates in the INDiGO system. That is, the presence of multiple (unlinked) records for a person in the database. Of course, such duplicates may occur due to various reasons, such as through the integration of different systems or technical difficulties with these integrations. In rare cases they may have even been added through bad intent.  More frequently however, such records are incorrectly added due to other factors, such as work pressure and a lack of (automated) checks that make sure that a person’s data is not already present in the database.

Duplicates are a serious problem for the IND because when multiple records exist for the same applicant this may lead to wrong decisions. Interview participants indicated that, generally speaking, the work pressure together with a varying degree of knowledge of how to effectively use the search and match tools complicates the processing of applicants’ data and adds to the possibility for creating duplicates. In this way, duplicates and the deduplication process furthermore bring to light the stakes when things go awry, as well as the kind of sustained repair work needed in the background.

Besides potentially contributing to the creation of duplicates, the ELISE searching and matching system plays a crucial role in detecting and resolving duplicates. On one hand, by identifying possible duplicates through an automatic, periodically run query on the database. Technical documents specified that IND chose to not use automated checks to verify if a person already exists in the database at the time when creating a new record. Hence, the deduplication service deployed at the IND only periodically checks for newly created duplicates. Broadly speaking, the service performs a match against all other person records in the database for all person records more recent than a specified date. All records in the database with a match score greater than or equal to a specified threshold will be outputted as potential duplicates. The match score itself is calculated using specific rules determined by the IND for fuzzy matching of ELISE. For this reason, whenever employees at the IND search the systems no automatic checks are run to check if a person already is known in the system. This lack of automated checks was also highlighted by interviewees as an important reason why some duplicates might get created.

On the other hand, after the systems finds potential matches when domain experts evaluate and process actual duplicates. The repair work to correct these duplicates again is known in the organization as the deduplication process. In this process, users of the department ‘Titles and Identity’ (T&I) will deal with potential duplicates by collecting evidence to eventually make a decision to link the multiple records or not. Some of the evidence they collect to make such distinctions may be ‘weak evidence’ (e.g., addresses are the same) than other ‘strong evidence’ (e.g., identification documents are the same). In addition, great care is taken to fully document the process and decision in a so-called memo, allowing for others to follow the process leading to the decision to deduplicate or not. Figure \@ref(fig:deduplication) sketches the different steps in this deduplication process — from finding possible duplicates, to processing the records and finding evidence, to making a decision.

```{r deduplication, echo=FALSE, fig.cap="The deduplication process at the IND."}
knitr::include_graphics("figures/deduplication-diagram.pdf")
```

What is remarkable in this process of resolving duplicates is indicative of the interconnectedness of different information systems of the MK and, furthermore, their transnational connections. Documents specifying the different reasons for deciding to deduplicate often make reference to verifying the identity records in other national and international systems. For example, one kind of strong evidence mentioned is when the National Police found a link between these records by making use of information in the EU information systems Eurodac or EU-VIS. In these situations, it therefore becomes clear that other national or even international systems play a role resolving local uncertainties about the personal identity data.

<!-- Another major factor that influences how duplicates may get created and when they are found is the technical design of the software tools. Technical design documents of the INDiGO deduplication distinguish between two approaches to find possible duplicates: online and offline deduplication. In the case of online deduplication, checks to verify if a person already exists in the database would be executed when creating a new record. Such a method would attempt to prevent that a person’s data is recorded more than once. While for offline deduplication, such checks are executed only sometime after records are created. Periodically, a batch of person records already present in the database will be verified for possible duplicates against all other person records. -->

<!-- Even though the original implementation only relied on offline deduplication, it seems that more recently online deduplication checks have been introduced for one particular case: for people apply for documents using the digital platform of the IND. This platform is, for example, used when a foreign student applies to come and study in The Netherlands. The digital platform in this case will automatically use ELISE search and match to verify if that person already exists in the system. If not, new records will be created automatically. -->

<!-- Hence, the set of categories of data that are important to define identity need to be carefully defined and fine-tuned. I further asked the same person from the previous quote if these automatic checks do not lead to too many duplicates. According to him, this can be minimised after fine-tuning the criteria. Although some specific cases may still need manual intervention: -->

<!-- In summary, we may conclude that duplicates in the INDIGO system are a recurring and important issue for the IND. Of course, there are already tools available to help employees find these records, link new data, and correct possible duplicates. First, tools such as ELISE smart search and match are used to help them find records — and making use of the fuzzy search to avoid some mistakes. Second, a deduplication service is run periodically to find duplicates which did enter the system. However, as noted by the respondents, work pressure and differences in knowledge on how to effectively use the tools, causes frictions in the first step and duplicates may still be created. The recommendation would therefore be to explore how automatic deduplication checks could be integrated within the current workflow. For example, at a point when a user is attempting to create a new record in the system. However, to do is effectively, great care will need to be taken to define the criteria and match score thresholds for a potential duplicates. A further recommendation would therefore be to learn from already existing automatic matching functionalities. Namely, the automatic checks in the digital application process and the deduplication service. Finally, it should be noted that less experienced users may somehow need to be guided on how to deal duplicates with a possible duplicates. They should not be hindered in their work at those points. If all this is taken into account, it could both improve the user experience as well as help the organization to work more effectively. -->

## Discussion

Several premises regarding data uncertainties in identification are at play in the case of ELISE ID platform at the IND. On the one hand, the ELISE package for searching and matching embed assumptions about databases and their data records — that databases will never be complete and accurate. Such expectations can most prominently be seen in the ELISE ID search and matching tools. These tools work, by design, on the assumption that both search queries and database records can contain mistakes.[^biometrics] The tool therefore utilizes a probabilistic approach of dealing with data. Rather than giving a definite result when querying the database for a record, the system responds a list of records with their likelihood of matching the query.

[^biometrics]: This article mainly discussed the use of alphanumeric data such as biographical data of a person. Matching these kinds of data are seen very differently compared to biometrical data such fingerprints. Software engineers at WCC I have spoken to described the use of biometrics as more straightforward and objective. That is, determining if, for example, two fingerprints match is defined more strictly. While matching on loosely defined alphanumeric data is much more dependent on the context. Biometrics is therefore often seen as way to reduce semantic uncertainty. However, such a view neglects that biometrics are also shaped by local conditions. Capturing a good fingerprint needs work such as positioning the hand correctly on the machine, moisturizing the fingerprints, and so forth [@kloppenburgSecuringIdentitiesBiometric2020].

The organization or architecture of the INDiGO system and its integration of ELISE also reveals built-in assumptions. The use of the service-oriented architecture in this design positions the searching and matching functionality as a separate and distinct concern of the application. This arrangement has a number of effects. The separation of ELISE from the other business logic (the rules engine implementing policy) has made it reliable and performant (it can be run and maintained separately from other aspects of the IND system). Yet, in the current implementation, the search itself does not take into account circumstances where and by whom it is run. Of course, such a context cannot just simply be defined or represented. The interactions with the system and the context are instead mutually constitutive [@dourishWhatWeTalk2004; @suchmanHumanmachineReconfigurationsPlans2007]. Rather than active participants in the interactions with the system, the current design, in effect, enacts users as passive participants with their own knowledge of migrant's identity data supplanted by the functioning of ELISE.

<!-- Second, the organization and architecture causes some data cleaning efforts to be kept at the backstage. This can most clearly be seen in the example of deduplication, where the automatic query periodically finds possible duplicates which are processed by one unit in the organization. Nevertheless, interview participants explained that the process for deciding on resolving a possible duplicate match evolved over time. With the addition of standard reasons and justifications for resolving duplicates, and additional documentation the work has become more traceable. However, for a record presented in the interface to an IND employee, it may not be apparent that any of this work has been performed on a record. The relation between visibility and invisible work thus operates as a ‘matrix’ [@starLayersSilenceArenas1999] where different technical and organizational choices make some things more or less visible. -->

Integral to the ELISE searching and matching capabilities is a view where people can make mistakes and where semantic uncertainty is unavoidable. In the case of names, people entering data into a system might make mistakes such as making a typing error, accidentally switching two fields such first and last name, and so forth. The name matching on the other hand deals with semantic uncertainty even if there may be errors and incommensurabilities of naming practices. For example, the tool takes into account that there may be different versions for the transliteration of name and where no single correct one necessarily exists, or that someone might start to use their partner’s last name after marriage. Hence, what makes ELISE "smart" is its attempts to compensate and finally supersede knowledge for all such variations and uncertainties in a way that no single user could. Evidence of different ad hoc search strategies, however, show that these expectations do not always match with actual use. For users, the system is a kind of black box that they learned from experience.

This combination of findings provides some support for the conceptual premise that everyday bureaucratic practices of re-identification are shaping and shaped by the data matching technology. Hence, it could be hypothesized that these (technologically mediated) routines contribute to the back office policy of identification [cf., @ustek-spildaStatisticiansBackofficePolicymakers2020].

<!-- Finally, even more premises play a role on the side-lines. For example, looking at the way the company communicates about the technology also ‘narrate[s] the significance of technical artefacts’ [@suchmanConfiguration2014, p. 48]. Examples can be found in materials used by the company to promote their products and demonstrate why their products are important for their customers. The case of a mistake with variations of the name of the Boston Marathon bomber on a watchlist is a good example of this. Such tropes imagine scenarios of dramatic impacts of wrong decisions being taken due to incorrect, incomplete data. These kinds of narratives thus play a role in communicating the importance of the product, even if it is not always as effectively used in practice. -->

<!-- Implications of findings for design of appropriate CSCW -->

<!-- * In the implementation at the IND the data cleaning efforts are still kept at the backstage -->

<!-- User experience of using the search tools -->

<!-- * Separation of concerns and the loss of context -->
<!-- * Making more prominent  -->

<!-- Software architecture -->

<!-- Managing and cleaning the data so that they can be used by others is often part of the back office; the part of an organization where administrative and logistical work is performed to perform service or products, but who do not deal directly with customers of that service or product. This back office work of data management closely relates to what @goffmanPresentationSelfEveryday1959 calls the ‘back stage’ of social life. This metaphor from the world of theatre highlights the impression management of actors and institutions. A database should give the impression that it is accurate and complete for the people interacting with it. Nevertheless, the backstage work of cleaning messy data should likely not be visible to those users. Moreover, automation of such tasks and processes may start and be more prominent at the back office [@snellenBlurredPartitionsThicker1992]. -->

<!-- A similar analogy applies in software parlance, where the concepts of front and back end of a computer system are used to indicate a ‘separation of concerns’ between the presentation of the application that is accessible to the user (front end), and the back end that performs operations invisible to the user such as data access. This design criterion of separation of concerns is a well-establish and influential. It emphasizes a modular way of designing software by separating and encapsulating different functions of a system (i.e., ‘concerns’), as a type of ‘divide and conquer’ strategy to manage the complexity of software development [@laplanteWhatEveryEngineer2007]. Yet not much reflection is given by engineers on the effects of such designs. -->

<!-- Of course, the distinctions between front and back office/stage/end depends on positioning. One person’s front can be the back of another. But what these metaphors and analogies show evidence of is the often implicit assumptions about how this kind of work should be made (in)visible. Such assumptions are furthermore materialized and made durable in the actual systems that are created to support these tasks. In order to understand these effects we therefore need to pay close attention to the design and uses of the technological artefacts. -->

<!-- Processes and practices of managing data can furthermore be considered as articulation work, which CSCW scholars have shown is crucial for collaborative work. In this view, it is the background or invisible work for making other things work smoothly. Such ideas are also evident in the metaphors and expectations surrounding the work of managing data; further shaping how this work is valued and made (in)visible [@dignazioDataFeminism2020]. Metaphors and work titles used for the kind data management — such as ‘data cleaner’ or ‘data janitor’ — underline how such metaphors even have gender and class connotations. Infrastructure scholars in particular however have shown that what counts as work depends on the context and definitions in which it is done [@starLayersSilenceArenas1999]. Data management activities are therefore not inherently visible or invisible. Rather, by taking serious technologies for using and managing data it is possible to see the particular expectations and values they embed; for example making such work more or less visible. -->

<!--chapter:end:sections/05-discussion.Rmd-->

## Conclusion

This study set out to explore the everyday bureaucratic practices and technologies of re-identification in migration management. The investigation of the use of a data matching software package at the immigration and naturalization service of the Netherlands (IND) has shown how such the software package has become interwoven with their data practices for identifying clients throughout their bureaucratic procedures. One of the more significant findings to emerge from this study is how the software package supports the work by the software's affordances to mitigate data quality problems and uncertainties. In the context of the IND, agents might make mistakes in registering or querying for data on a migrant’s application. The data matching software then facilitates their work by making it possible to retrieve the information.

The findings of this study have a number of practical implications. First, the way ELISE presents results calls attention to the kind of materiality and agency that rankings possess [@pollockRankingDevicesSociomateriality2012]. The way the system presents and ranks the results shapes how users make sense of the data and hence shapes their work practices. Second, the prescribed behaviour that users should input as much information as possible stands in opposition of users’ own experiences. When these users encounter unexpected results, they began developing their own heuristic techniques. There is, therefore, a definite need for more deliberation on how best to guide users in formulating queries and explaining results. Third, rather than a "view from nowhere" [@suchmanWorkingRelationsTechnology1993] there are clearly different needs in interpreting the parameters and results of searching and matching identity data that is specific to the context of the IND organization.

Finally, several questions remain unanswered at present when conceptualizing identification in the nexus between procedures in system-level bureaucracies and the materiality/performativity of devices. Aspects such as the generic design of ELISE are actually part of the software supplier’s strategy of "generification" [@pollockSoftwareOrganisationsBiography2009]. Further research should be undertaken to investigate the impacts of such supplier strategies and the of use their technologies in identification practices of migration and border security. This stands in contrast with much contemporary research on the design and use of such (security) devices, which does not always take into such roles of software suppliers and their broader market strategies. It is evident that the ELISE software package has a particular history that has been shaped as it moved across domains and locales. Future work will therefore focus on investigating how this software has evolved over time and while moving across organizations.

## Previously removed paragraphs and sections for possible re-use {-}

These issues could also be seen as a problem of the configuration of ELISE in INDiGO. By design, ELISE does allow for the possibility to change, for example, the behaviour and weight of different search criteria. In the INDiGO implementation this is however not the case, and so for such less experienced users it is clear that the graphical user interface does not seem to offer much help in formulating effective queries. Essentially then, the ELISE component has no control on the GUI — neither how queries are specified nor how results are displayed. Learning to use the tools is mainly accomplished by getting help from colleagues or simply from practical use of the tools. Depending on how extensively users have to rely on the tools, there can also be big differences among the organizational units on how much knowledge about how the search and match functionalities work.

The way ELISE ranks results calls attention to the kind of materiality and agency that rankings possess [@pollockRankingDevicesSociomateriality2012]. That is, the way the system presents and ranks the results shapes how users make sense of the data and hence shapes their work practices. The prescribed behaviour [@akrichSummaryConvenientVocabulary1992] that users should input as much information as possible stands in opposition of users’ own experiences. When these users encounter unexpected results, they began developing their own heuristic techniques.

One possible improvement would be to add more explanations of why a result was included. Again, this is related to the particular way ELISE integrates with INDiGO. Other possible improvements could target cases such as where matches are found based on data that is not shown in the search results — e.g., the historical data mentioned in the quote above.

How and when duplicates are found is essential to further understand this process of deduplication. A first, more straightforward way, the department receives such deduplication requests can be through a manual demand from another department or organization. This can, for example, occur when an IND employee receives a document that they use to start looking for the customer. If they then find out that the person appears twice, they can request to turn those two records into one by filling in and submitting a form to T&I.

### INDIGO SOA, separation of concerns, and Migratieketen {-}

The general idea is that these components separate what technical documents describing INDiGO name the ‘flow’ and the ’know’. More concretely, the Siebel component handles the flow — the processes and related data, while the rules engine handles the know — the knowledge of how to process applications, such as rules related to applying the Aliens Act (‘Vreemdelingenwet’). According to the design rationale, this separation should, in theory, allow the IND to implement changes in legislation and regulations relatively quickly [@blankenaINDKanNieuwe2013]. In general terms, the design follows what is known as the design criterion of separation of concerns; which aims to divide applications into modules that focus on distinct aspects.

A brief timeline of the INDiGO can be instructive to further understand the development of the system and its integration with ELISE. The SOA architecture of the INDiGO system actually received praise during its initial development in 2009 [@toetIndigosysteemVanIND2009], but the rollout of the new system in subsequent years did face a number of delays and problems [@bergsmaSysteemINDDuurder2013]. The initial rollout of the INDiGO system started in 2009 and was completed in 2013. It is important to note that not only did this entail a new IT infrastructure, rather the transition to a new system was linked to a comprehensive redesign of business processes and other organizational changes, with an aim to transform the organization to one that works digitally. In a stage-wise approach different organizational units at the IND started to use the system over these years, ending with a phase-out the old system (INDIS). Later developments include changes to INDiGO and information exchange after the ‘Dolmatov case’.[^dolmatov] 

<!-- In 2014 the framework agreement for development and maintenance of INDiGO changed and a tender was awarded to different parties. -->

[^dolmatov]: In 2013 Russian opposition activist and asylum seeker Aleksandr Dolmatov committed suicide in a Dutch detention centre. After an investigation it was found that one of the problems in the case related to an automatic update in INDiGO, as a result of which the system incorrectly stated that he could be deported [@kasHierWerdFout2013; @rightonVreemdelingenUitgezetDoor2013]

Two aspects of this transition from the previous INDIS to the current INDiGO are notable for the ELISE component. First, ELISE was important for the migration of legacy data to the new system. This means that during the transition period it played an important role in finding matching identities between the two systems. During this period the data used by ELISE was therefore sourced from both INDIS and INDIGO. Second, during these years of transition and development the ELISE component has also received upgrades with new searching and matching capabilities. As a separated service, however, these upgrades may not have been completely visible for users.

#### Interfacing with other partners {-}

<!-- 
BVV: https://www.ensie.nl/dienst-terugkeer-en-vertrek/ensies/Begrippenlijst%20Dienst%20Terugkeer%20en%20Vertrek%20/1 Vreemdelingenketen: https://www.ensie.nl/dienst-terugkeer-en-vertrek/vreemdelingenketen
https://web.archive.org/web/20210116203510/http://verblijfblog.nl/de-vreemdelingenketen-en-de-zaak-dolmatov/
-->

<!-- Migratieketen -->

Another important aspect of the INDiGO system is its connections to other organizations through the ‘migratieketen’ (MK). Roughly speaking, the MK is a formal collaboration between various organizations and ministerial agencies in The Netherlands — called ‘ketenpartners’ (KP) — that play a role in the processes that foreign nationals go through: from entering the Netherlands to obtaining a residence permit or departure or expulsion. Important aspects of this collaboration are coordination of migration policy and information exchange.

<!-- Protocol identificatie labelling -->

An example of coordination mechanisms in migration policy and information exchange can be seen in the standardized way of identifying persons described in the document ‘Protocol identificatie labelling (PIL)’ [@ministerievanjustititieenveiligheidProtocolIdentificatieLabeling2020]. First and foremost, this document defines standardized methods for identifying and registering foreign nationals in the MK. Second, it describes data governance processes such as how this identity data may be modified or when data must be destroyed. Third, the document describes the various partners, their links in the information architecture and role in the MK. The interactions between the different systems and databases are important for establishing identities. For example, for various tasks users of the INDiGO system are required to search another external database called the ‘Basisvoorziening Vreemdelingen’ (BVV).

The BVV is, in this respect, the central component in the MK. The information system stores all basic data of people who have a relation with the Dutch government in the context of the aliens act (‘Vreemdelingenwet’). The system allows digital exchange of identity data, information about travel and identity documents, biometric characteristics and status data between the partners. Various ministries and organizations in The Netherlands use this BVV system to share and consult information about foreign nationals. As such, it acts as a kind of single source of truth. That is, although other partners may have their database and information about a person, the data needs to be kept aligned [@InformatievoorzieningVreemdelingenketen2015]. This can also be seen due to the fact that through the BVV all foreigners receive a unique identification number — the _v-number_. In practice then, processes of establishing identity involves an interplay of multiple systems. In fact, INDiGO users searching for person matches always need to consider result from the BVV as well. It is therefore important to consider the search and match functionalities of both systems — the interfaces, the behaviour of the match engines, and how users interpret results.

<!-- Partners MK -->

Through the BVV and other interfacing mechanisms the IND is further connected to partners in the MK. Figure \@ref(fig:migratieketen) (adapted from [@veiligheidSamenwerkingKetenMigratieketen2018]) shows these various partners, and inner and outer ‘layers’ can be distinguished. The inner layer consists of the ministerial agencies and the outer layer consists of other governmental, non-governmental and international organizations.

<!-- Findings -->

Interactions between the IND and other organizations from the MK were also highlighted by respondents during the user interviews. The mentioned partners include, for example, the department of the Dutch police, Afdeling Vreemdelingenpolitie, Identificatie en Mensenhandel (AVIM), which can conduct searches on the identity and residence status of people to oversee compliance with the Vreemdelingenwet. Another mentioned partner is the Royal Netherlands Marechaussee (KMar) which is involved with border control of the external border of the Schengen area.[^frontex] In the context of visas, Buitenlandse Zaken (BZ) is pivotal. For the IND in particular, the exchange of information with municipalities is done to keep, for example, residency data up-to-date. When searching for a person INDiGO users will, in principle, first consult the BVV system though a screen in INDiGO. A match may then indicate that the person has already been registered by one of these other partners. This short description of the systems show the important role it plays in making possible this distributed collaborative work of applying the policies regarding foreign nationals.

There are many similarities in the categories of data. However, there are differences in the way the search works — which complicate the use of the tools.

<!-- gemeente -->

[^frontex]: In The Netherlands this means border control at airports, seaports and along the coast. Furthermore, through the KMAR participates in the European agency for border surveillance, the European Border and Coast Guard Agency (also known as Frontex) with knowledge, expertise, equipment, and personnel. Personnel from the Marechaussee are therefore also involved in border surveillance at the external borders of the Schengen area in other Member States. Both of these may create records in the BVV or consult them in identity investigations.

However, there are differences in the way the IND and BVV searches work — which complicate the use of the tools. An often mentioned suggestion from the interviewees was therefore to integrate these searches somehow. This would make sense since, as mentioned previously, the interactions between the BVV and IND systems are important to establish the identity of a person.

### Online vs offline deduplication {-}

How and when duplicates are found is essential to further understand this process of deduplication. A first, more straightforward way, the department receives such deduplication requests can be through a manual demand from another department or organization. This can, for example, occur when an IND employee receives a document that they use to start looking for the customer. If they then find out that the person appears twice, they can request to turn those two records into one by filling in and submitting a form to T&I.

Another major factor that influences how duplicates may get created and when they are found is the technical design of the software tools. Technical design documents of the INDiGO deduplication distinguish between two approaches to find possible duplicates: online and offline deduplication. In the case of online deduplication, checks to verify if a person already exists in the database would be executed when creating a new record. Such a method would attempt to prevent that a person’s data is recorded more than once. While for offline deduplication, such checks are executed only sometime after records are created. Periodically, a batch of person records already present in the database will be verified for possible duplicates against all other person records.

Technical documents specified that IND chose to only use the offline method to identify possible duplicates. Hence, the deduplication service deployed at the IND only periodically checks for newly created duplicates. Broadly speaking, the service performs a match against all other person records in the database for all person records more recent than a specified date. All records in the database with a match score greater than or equal to a specified threshold will be outputted as potential duplicates. The match score itself is calculated using rules determined by the IND and the fuzzy matching of ELISE. For this reason, whenever employees at the IND search the systems no automatic checks are run to check if a person already is known in the system. If we take into account the previous comments about the pressure employees face, this may be an important reason why some duplicates get created. But, more importantly, which could have also been prevented in the first place.

Even though the original implementation only relied on offline deduplication, it seems that more recently online deduplication checks have been introduced for one particular case: for people apply for documents using the digital platform of the IND. This platform is, for example, used when a foreign student applies to come and study in The Netherlands. The digital platform in this case will automatically use ELISE search and match to verify if that person already exists in the system. If not, new records will be created automatically.

### Font and back end {-}

<!-- Software architecture -->

Managing and cleaning the data so that they can be used by others is often part of the back office; the part of an organization where administrative and logistical work is performed to perform service or products, but who do not deal directly with customers of that service or product. This back office work of data management closely relates to what @goffmanPresentationSelfEveryday1959 calls the ‘back stage’ of social life. This metaphor from the world of theatre highlights the impression management of actors and institutions. A database should give the impression that it is accurate and complete for the people interacting with it. Nevertheless, the backstage work of cleaning messy data should likely not be visible to those users. Moreover, automation of such tasks and processes may start and be more prominent at the back office [@snellenBlurredPartitionsThicker1992].

A similar analogy applies in software parlance, where the concepts of front and back end of a computer system are used to indicate a ‘separation of concerns’ between the presentation of the application that is accessible to the user (front end), and the back end that performs operations invisible to the user such as data access. This design criterion of separation of concerns is a well-establish and influential. It emphasizes a modular way of designing software by separating and encapsulating different functions of a system (i.e., ‘concerns’), as a type of ‘divide and conquer’ strategy to manage the complexity of software development [@laplanteWhatEveryEngineer2007]. Yet not much reflection is given by engineers on the effects of such designs.

Of course, the distinctions between front and back office/stage/end depends on positioning. One person’s front can be the back of another. But what these metaphors and analogies show evidence of is the often implicit assumptions about how this kind of work should be made (in)visible. Such assumptions are furthermore materialized and made durable in the actual systems that are created to support these tasks. In order to understand these effects we therefore need to pay close attention to the design and uses of the technological artefacts.
