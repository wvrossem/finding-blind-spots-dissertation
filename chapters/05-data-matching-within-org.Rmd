# Data matching within organizations {#ch-dm-within-org}

\chaptermark{Data matching within organizations}

__Abstract__

\noindent

This chapter considers how everyday bureaucratic practices of identification and verification in migration management are intertwined with technologies for searching and matching identity data. These practices include rather mundane, and often technologically mediated, ways of ascertaining the identity of clients in different steps of bureaucratic procedures, such as residency or naturalization applications. Following debates in the intersection between STS and critical security studies, identification can no longer be understood as problems of representation between clients and their identity data, but identification as enactment and mediated by diverse actors. A compelling addition to these debates could come from the literature on street- and system-level bureaucracies, which has argued that the everyday bureaucratic practices and use of discretion ultimately amounts to policymaking. From this perspective, identification — a sometimes vague area lacking clear policies and rules — is enacted through the everyday practices. Yet, while this literature has debated the effects of new technologies on the discretion civil servants can employ, scholars have been less specific about the constraints and affordances of technologies shaping decisions. In this chapter, I propose to look at the under-researched area of "re-identification." The term re-identification is used here to refer to how clients of bureaucratic procedures in migration are re-identified in data infrastructures throughout bureaucratic procedures. I draw on data collected on the development and use of a software package for searching and matching migrants' data at a government migration agency in The Netherlands. Findings show how the software mediates (re-)identification practices and redistributes competences through the software’s affordances to mitigate data quality problems and uncertainties. These findings contribute to debates
on the materiality of identification by shifting focus from first registration to practices of re-identification throughout data infrastructures.

---

\vspace*{\fill}
\noindent
_Possibly insert citation here._
\newpage

## Introduction {#intro}

The stakes in identification can be high, and specialized technologies to search and match identity data employed by authorities can greatly mediate uncertain identification encounters. An often invoked real-life example of the complexities of identification is when one of the "Boston bombers", Kyrgyz-American Тамерла́н Царна́ев, was _not_ pulled aside for questioning when leaving from and returning to JFK Airport in New York for a trip to Dagestan in the Northern Caucasus in 2012 (an area considered as a high risk travel destination by the US government).[^boston-bombers] According to an investigative report for the U.S. House Committee on Homeland Security, and which media outlets reviewed, he mistakenly was not identified as a person of interest and questioned [@schmittAgenciesAddedBoston2013; @winterRussiaWarnedTsarnaev2014]. In April 2013 he and his brother perpetrated a terrorist attack during the annual Boston Marathon. Back in 2011, Russian authorities notified their U.S. counterparts of his connections to terrorist groups. As a result, United States government authorities pre-emptively added his information to their watch lists and databases, such as the Terrorist Identities Datamart Environment, a database containing more than 1.5 million records of known or suspected international terrorists. Such watchlisting systems are designed to automatically compare information about people and to alert and instruct authorities on what to do when they encounter a person with matching identity data. In the case of Mr. Царна́ев, the system may not have triggered an alert due to incomplete information about his date of birth and variations in the transliteration of his name: "Tsarnaev" — "Tsarnayev." In my view, this example highlights at least three crucial aspects of contemporary digitally mediated bureaucratic practices of identification.

[^boston-bombers]. Tech companies frequently use this case of the misspelled names of the Boston bomber as an exemplar for the necessity of their data matching technologies in watch list screening systems [see also @basistechnologyStrengtheningBordersIntelligent2021]. As part of their sales pitch, companies can use such an example to highlight the need for the company's technology to deal with uncertainties in finding and linking identity information. There is a risk of reproducing and contributing to the securitization of identification, nonetheless I find the case useful as a real-life example of the intertwining of, among others, government agencies, border guards, watchlisting systems in the identification of risky travellers.

First, from street-level bureaucrats to system-level bureaucracies, all face some unavoidable uncertainties in their daily work of establishing and verifying clients' identities. Most organizations need to cope with the fact that databases contain data that is incomplete, not current, incorrect — or, even contain duplicate entries that refer to the same real-world persons [@keulenManagingUncertaintyRoad2012]. Hence, ensuring that data are correct, complete, accurate and that they can be shared, used, processed by different parties and information systems has become vital for the correct functioning of (bureaucratic) procedures.[^gdpr]^,^[^data] The European Union Agency for Fundamental Rights, for example, investigated how data quality in EU information systems for migration and border control affects fundamental rights [@fraWatchfulEyesBiometrics2018]. The agency's report remarks that in relation to uncertainty about a person’s identity "authorities often suspect identity fraud when cases of data quality are the real reason for concern" (p. 81). For alphanumerical personal data (such as surname, date of birth, nationality), these data quality issues can have various, and often quite unspectacular, reasons. The case of Тамерла́н Царна́ев touches on the fact that watchlists databases need Latin-characters names, yet, transliteration of a name can take many forms. Hence, working with different sources of data usually brings challenges of what I will call "re-identification." With this term I aim to capture processes such as determining someone's corresponding identity data in databases, or deciding if multiple database records (possibly from different organizations) are referencing the same real-word person.[^reidenfication]

[^gdpr]: For example, Article 5(1)d of the General Data Protection Regulation (GDPR) states the principle of accuracy. According to this principle personal data collected and processed by organizations need to be "accurate and, where necessary, kept up to date" and that "every reasonable step must be taken to ensure that personal data that are inaccurate, having regard to the purposes for which they are processed, are erased or rectified without delay." Furthermore, several other Articles of the GDPR make statements about data sharing and data interoperability.

[^data]: The word data is often treated as a mass noun, and hence something that cannot be counted or divided (e.g., "the data is available"). In contrast, this chapter uses data in its countable plural noun form ("data are"). I follow the convention of using this form to highlight that data are multiple and "arise from and are used in varied circumstances worth acknowledging" [@loukissasAllDataAre2019, p. 13].

[^reidenfication]: In technical literature, the term for re-identification is also used for processes of de-anonymizing data, i.e., to disclose personal identities belonging to anonymized data. The outcome of a data matching processes can actually lead to a re-identification of de-identified individuals. As @christenDataMatchingConcepts2012 explains, re-identification is possible because "record pairs classified as matches in a data matching project can contain information that is not available in the individual source databases that were matched" (p. 189). Matching data from different databases (without identifying details) can still (un)intentionally identify and disclose individuals in those databases. There is thus an unmistakable link between re-identification as de-anonymization and the practices and technologies described in this chapter.

Second, new technologies keep being introduced to deal with "data frictions," [@edwardsVastMachineComputer2010] and re-identification should thus be understood in its (changing) sociotechnical context. That is, researchers need to consider how technologies such as for searching and matching identity data (re)configure practices of re-identification. This point resonates with materialist and performativity debates on identification discussed in Chapter 2 [e.g.,@fors-owczynikMigrantsRiskIdentity2015; @leeseFixingStateVision2020; @pelizzaIdentificationTranslationArt2021; @vanderploegIllegalBodyEurodac1999; @pollozekInfrastructuringEuropeanMigration2019; @skinnerRaceRacismIdentification2018]. Following these debates, re-identification should not be understood as a problem of representation between people and their identity data, but how data infrastructures for identity management and (re-)identification "enact" individuals as migrant, criminal, risky traveller. From this perspective, we can rethink the above-mentioned quote from the Fundamental Rights Agency. Instead of asking if the doubts about someone's identity arises from inaccurate data or mistrustful border control practices, we must consider also how data infrastructures enact subjects as a potential identity fraud. Such a materialist and performative approach replaces discussions of identity as representation to account for heterogeneous set of actors involved in identification practices [@pelizzaIdentificationTranslationArt2021]. So far, however, literature has focused on (more prominent) moments of first (and often the biometric) registration (e.g., biometric refugee registration), and there has been little discussion about how people are re-identified and enacted throughout bureaucratic practices and data infrastructures.

Third, as the street-level bureaucracy literature stresses, government policies are established through the discretion public employees use in their regular encounters with citizens to deal with complex contexts that don't always fit neatly into the rules and regulations devised by legislators [e.g., @lipskyStreetlevelBureaucracyDilemmas2010]. (Re-)identification encounters are procedures where there is simultaneously a lack of and divergence from rules and regulations, and, hence, ample room for discretion. To date, the problem of (re-)identifying clients throughout bureaucratic procedures has, however, received scant attention in literature. @pelizzaIdentificationTranslationArt2021 provides a useful example of an identification encounter where the tension between systems, policies, local circumstances is visible. She describes the back-and-forth between an applicant, a policeman, and a translator to determine the applicant's name from Arabic to Latin characters during first registration at a Greek border. In this identification encounter, the resulting name is, according to Pelizza, the outcome of a "chain of translations" of the migrant’s name from oral, to written, to finally end up in the information system to act as the official version to be used in further bureaucratic procedures. The process Pelizza described is again very different from an example I encountered in The Netherlands where, in case there are doubts or refusal to give a name, the person will be assigned a name in the system using a label which includes information about the sex of the applicant, and the time and place of registration (e.g., "NN regioncode sex yymmdd hhmm"). In both these examples of identification encounters, public employees enact policies and regulations on identification while adapting for the specific person and within the limitations and affordances provided by a sociotechnical context.

However, while the street-level bureaucracy literature has long debated the constraining or enabling effects of new technologies, such as related to automated decision-making [@bovensStreetLevelSystem2002; @buffatStreetlevelBureaucracyEgovernment2015], it has been less specific about the entangled technologies. Yet, it is clear that particular tool's expectations and materialities about data (and their quality) shape identification encounters. The designs, data models of technical solutions, such as for searching for a person’s record or for deciding if two identity data records refer to the person, embed many assumptions about those data [see also @pelizzaScriptsAlteritySecurityunderreview] that furthermore shape bureaucratic re-identification practices. In the case of identity data, such tools assemble knowledge and enact equivalences between otherwise disparate naming practices. Family names, for instance, may have variant endings for male and female which can, nevertheless be treated as matching. In this chapter, I therefore ask how clients of bureaucratic procedures in migration are re-identified throughout data infrastructures. My overarching goal is to contribute to the current materiality and performativity debate on identification by shifting focus from first registration to practices of re-identification in data infrastructures.

The research seeks to investigate the research gap on re-identification by empirically studying a software package used in practices and technologies of re-identification at the migration and naturalization service in The Netherlands (IND). The analysis draws on data gathered through fieldwork — interviews, documents, field notes — at the supplier of a software package used at the IND agency. Findings show that the design of the tools for searching and matching embeds assumptions about databases and their data records which shapes and is shaped by bureaucratic re-identification practices. For instance, such assumptions include that different, incommensurable naming practices and conventions may exist, that databases thus will never be complete and accurate. It can thus be suggested that the use of data matching technologies was accompanied by a redistribution of roles by the affordances and restrictions of how government agents can search for identities, and when automated matching algorithms supplant user's expertise in identification.

Taken together, these findings contribute to the debate on the materiality and performativity of identification in the intersection of Science and Technology Studies and Critical Security Studies in two ways. First, combining these literatures with concepts from the street-level bureaucracy literature makes it possible to follow how policy in everyday bureaucratic re-identification encounters are enacted and mediated through specific sociotechnical contexts, such as for dealing with ambiguities in personal data. Second, while researchers have tended to focus on first registration, this research sheds light on other processes of re-identification throughout bureaucratic processes and practices of working with uncertain data in migration management.

The next section aims to establish the links between the literature from street-level to system-level bureaucracies and the literature in the boundary between STS and Critical Security Studies on the materiality and performativity of identification. The remaining parts of the chapter will discuss the empirical case and findings.

<!-- How do the findings contribute, what are design implications? -->

<!--chapter:end:sections/01-intro.Rmd-->

## Background and related work

<!-- This Section draws on and brings together strands of literature which are relevant to the design and use of data matching technologies within organizations. The concepts and techniques of dealing with the uncertainties in sharing, connecting, finding data are by now well established. Lesser known are the potentially transformative effects of such technological innovations when they are assimilated into workplace and security settings. Various approaches have been developed that can help in examining how technologies for searching and matching personal identity data shape and are shaped by these settings and their heterogeneous set of actors and practices. Overall, I follow calls for being specific about technologies and the kinds of expectations and materialities they join together in seeking to reduce uncertainties about personal identity data. -->

Many interactions between migrants and public authorities involve forms of identification, to establish or verify applicants' identity in different steps of bureaucratic processes relating to granting asylum, issuing, residency permits, naturalization, and so forth. As such, identification can be considered a two-faced phenomenon linked to both granting and restricting of benefits [@aboutIdentificationRegistrationPractices2013; @caplanDocumentingIndividualIdentity2001]. When identification goes wrong, asylum seekers may, for example, undermine the credibility of their whole application and be wrongly accused of providing a fake identity on purpose. The bureaucratic organizations dealing with individual cases are, of course, based on regulations and administrative routines. Yet, as the street-level bureaucracy literature has shown, public-service workers in charge are not just implementing relevant policies, but are actively involved through the discretion workers employ. What does it mean then to consider practices of (re-)identification as part of everyday bureaucratic practices?

### (Re-)identification as a bureaucratic practice

The foremost understanding of the concepts of street-level bureaucracy and discretion were articulated by Michael Lipsky and popularized in his 1980 book: "Street-level bureaucracy: dilemmas of the individual in public services". In this widely acknowledged view, diverse front-line public service workers influence public policy through their regular interactions with the general public. As [@lipskyStreetlevelBureaucracyDilemmas2010, p.13] states: "street-level bureaucrats have considerable discretion in determining the nature, amount, and quality of benefits and sanctions provided by their agencies." For instance, in an identification encounter between a border guard and a border crosser at passport control, the border guard may have some freedom to make a decision to permit entering the country. Lipsky’s original argument, however, needed updating with the increased use of digital technologies in public administration and the rise of e-government. More recent literature on street-level bureaucracy and discretion pays particular attention to the transformations brought about by the digitalization of public agencies has forced scholars to rethink the role of the street-level bureaucrats and their daily interactions [@bovensStreetLevelSystem2002; @buffatStreetlevelBureaucracyEgovernment2015; @buschDigitalDiscretionSystematic2018; @snellenElectronicGovernanceImplications2002].

In an influential article, @bovensStreetLevelSystem2002 conceptualized the transformations brought about by ICT into, first, "screen-level" and, second, "system-level" bureaucracies. The first concept draws attention to how interactions between officials and citizens became increasingly mediated through computer screens. For instance, the personal data of a residency permit applicant is filled out using electronic template forms in a case management system. Or, increasingly by applicants themselves as citizens are provided access to government information systems [@landsbergenScreenLevelBureaucracy2004]. Meanwhile, the decision to grant the permit is guided by decision trees, business rules, algorithms that model the policies and regulations. The concept of system-level bureaucracies was thus coined to refer to the intensified digitization of data collection and processing, and accompanied organizational changes. In ideal terms, @bovensStreetLevelSystem2002 defined the transformed role of practitioners in such an organization as follows:

> "The members of the organization are no longer involved in handling individual cases, but direct their focus toward system development and maintenance, toward optimizing information processes, and toward creating links between systems in various organizations. Contacts with customers are important, but these almost all concern assistance and information provided by help desk staff. After all, the transactions have all been fully automated." (p. 178-179)

In general terms, this observation would imply that re-identification for the functioning of system-level bureaucracies. On one hand, information management, and thus also identity management, is an important factor for automated processes and decisions. For instance, making sure that personal identity data is accurate, up to date, that there are no duplicate entries, etc. On the other hand, re-identification is closely related to linking identity records across systems and organizations.

Moreover, in the transformations from street-level, to screen-level, and system-level bureaucracies lies a widely debated tension between automation as desired for reasons of fairness, efficiency versus as automation as undesired as a removal of human judgements and autonomy in decisional processes to adapt according to local and individual circumstances. In a literature review on these debates, @buffatStreetlevelBureaucracyEgovernment2015 singled out two major sides which she calls the "curtailment thesis" and the "enablement thesis." According to her, authors which can be grouped in the first thesis generally argue that discretion of front-line officers is curtailed by ICT and shifts to other actors. In this view, if, for instance, an information system can make an automated decision for granting a residency permit it would be less subjective as it can precisely follow policies and regulations. Such thesis is of course overly technologically deterministic. Other research, which @buffatStreetlevelBureaucracyEgovernment2015 groups in the "enablement thesis," highlights a more complex role of technologies, not just as contrary to discretion, but as resources in sociotechnical arrangements that constrain and enable interactions between technologies, frontline workers, citizens. Lastly, another more recent thesis can be found in the "digital discretion" literature which actually prescribes the use of "computerized routines and analyses to influence or replace human judgment" [@buschDigitalDiscretionSystematic2018, p. 4] as a way to closely follow policy and reduce human discretion.

Two important themes emerge from the studies discussed so far. First, that public-office workers enact policies through their daily practices and use of discretion. Second, that changes in the sociotechnical systems of bureaucratic organizations can curtail or provide additional resources to everyday bureaucratic practices. Overall, the literature presented thus far supports the view of the importance of re-identification as the transactions between clients and (system-level) bureaucratic organizations become automated. What is not yet clear is how people are re-identified throughout bureaucratic practices mixed work of humans and automated computer systems.

### Materialist and performative approaches to identification

There is a growing body of literature that recognizes the importance of identification as intermingled with government’s obligations and rights (e.g., citizenship, residency), as well as coercive measures [@aboutIdentificationRegistrationPractices2013; @caplanDocumentingIndividualIdentity2001]. Traditionally, academics have also emphasized the links between state-making of modern nation states and the elaboration of registration and identification systems, such as the creation of civil registers or passport documents [@breckenridgeRegistrationRecognitionDocumenting2012; @caplanDocumentingIndividualIdentity2001; @torpeyInventionPassportSurveillance2018]. An often referred to term for the state’s capacity of identifying its citizens as a form of is the notion of _legibility_ of @scottSeeingStateHow1998. Scott noted how increased interaction of states and their population (e.g., for purposes of taxation) went hand in hand with projects of standardization and legibility as attempts to unambiguously identify its people. So, in the example Scott, while cultural naming practices are very diverse and can serve local purposes, the standardization standardized surnames "was a first and crucial step toward making individual citizens officially legible" (p. 71). In these practices, the identity of the person is not a problem of representation between a person and information captured about them, but as a reduction of multiplicity with a mutually enactment of identity, states, institutions [@lyonIdentifyingCitizensID2009; @pelizzaIdentificationTranslationArt2021].

A growing body of literature in the intersection between STS and Critical Security Studies has added an important dimension to the discussion on identification by accounting for the materiality and performativity of devices and practices[@coleSuspectIdentitiesHistory2001; @gargiuloMonitoringSelectingSecurity2017; @skinnerRaceRacismIdentification2018; @pelizzaIdentificationTranslationArt2021; @suchmanTrackingTargetingSociotechnologies2017]. @bellanovaControllingSchengenInformation2020 for instance have studied the actors and practices involved in the processes of maintaining the EU Schengen Information System (SIS). The SIS system allows authorities to create and consult alerts on, among other, missing persons and on persons related to criminal offences. By looking at how these alerts "acquire the status of allegedly credible and accurate information that becomes available to end-users through the SIS II" (p. 2) they make evident its role in conditioning international mobility. @fors-owczynikMigrantsRiskIdentity2015 have shown how three systems in The Netherland translate and frame categories of risk to identify potentially risky migrants and travellers. Incorporating the role of sociotechnologies into the analysis of identification practices makes it possible to empirically explore security and account for the materiality and performativity of devices and practices of identification of migrants.

Very little, however, is currently known about how practitioners deal with uncertainties of personal identity data in re-identification encounters. For instance, a report from the European Court of Auditors, describes how "when border guards check a name in SIS II [the Schengen Information System], they may receive hundreds of results (mostly false positives), which they are legally required to check manually" (ibid., p. 31). In this case, the mentioned SIS II system computes and presents excessive amounts of matches that cause frictions in everyday practices of border control. The question remains of how these bureaucratic practices of re-identification are enabled or constrained by technologies for dealing with data uncertainties.

### Data uncertainties in practices of re-identification

Critical data studies have made it clear that data is never "raw," that databases can be messy and with mistakes, and that work is therefore needed to put data to use [@gitelmanRawDataOxymoron2013; @loukissasAllDataAre2019]. For example, a report from the European Court of Auditors mentions that a major EU information system supporting border control contains millions of potential data quality issues, such as first names recorded as surnames or missing date of births [@ecaEUInformationSystems2020]. Many such discrepancies are related to work practices and issues of fitting local circumstances to global standards [@bowkerSortingThingsOut1999]. In a Chapter on the International Classification of Diseases as a standard for coordinating work, Bowker and Star describe the difficulties of busy doctors to fill in death certificate and how "the act of assigning a classification can be socially or ethically charged" — such as classifying a death as a suicide. And as @loukissasAllDataAre2019 remarks, it is clear that databases might contain lots of such errors hence "local knowledge [is needed] to see that such errors are not random" (p. 67). In this way, data are testimony to the local conditions of their production that need to be linked across space and time in future processes of re-identification. If data quality problems and uncertainty are facts of life[@keulenManagingUncertaintyRoad2012], then organizations need to cope with this uncertainty in practices of re-identification.

The technical mechanisms for (re-)identifying if two or more persons represented by data records actually refer to the same real world persons or not is itself known by many names: object identification, entity resolution, record linkage, data matching [@batiniObjectIdentification2016]. Basically, these techniques will compare attributes of data records and use classification methods to determine matches [@christenDataMatchingConcepts2012]. Numerous classification techniques exist: some may be based on following specific rules, while others may use more probabilistic methods. Metrics can, for example, calculate the similarity of two sequence of characters based on the number of operations that would be required to transform one into the other. In this way, the names "Sam" and "Pam" may be considered closely related (was it a typo?). Other approaches may even calculate such similarities by comparing how names are pronounced (in English). Rules based matching may include rule such as to ignore honorifics and titles (e.g., Mr., Ms., Dr.) when matching personal data. In this way, knowledge, techniques, rules, methods for matching identity data are assembled into sociotechnologies that inform organizations and practices of identification.

Data deduplication is commonly used to refer to the problem of data matching when dealing with duplicate records from a single data source. A migrant may, for example, be mistakenly registered more than once in a system due to technical issues. Generally, a deduplication process will then periodically compare records with every other record in the database to score its likelihood of matching. After determining the set of possible matches, a domain expert will need to be involved to decide on found matches. A final step of the data matching is then to fuse the multiple records referring to the same entity into a single record. More interesting is how the process of ‘normalizing’ duplicates can be ‘a key to learning about the heterogeneity of data infrastructures’ [@loukissasAllDataAre2019]. Therefore, following Loukissas, deduplication can give insights into the heterogeneous practices of identification in migration and border control.

This section has attempted to provide a brief summary of the literature relating to identification in relation to street- and system-level bureaucracies, the performativity and materiality of devices and practices of identification, and problems and solutions for dealing with data uncertainties. Essentially, I find that a compelling addition to this debate could come from the literature on street- and system-level bureaucracy. In this view, the everyday bureaucratic practices of re-identification and use of discretion would amount to identification policy in practice. For this reason, it is crucial to investigate how particular sociomaterialities give shape to (re-)identification encounters.  The next section describes the empirical case and methods used to investigate these issues.

<!-- ### Data practices and technologies in security contexts -->

<!-- While the impact of new information technologies on the workplace has been the subject of long debates in fields such as CSCW [see for example, @bowkerSocialScienceTechnical1997; @leonardiMaterialityOrganizingSocial2012; @orlikowskiSociomaterialityChallengingSeparation2008; @pollockSoftwareOrganisationsBiography2009], only recently these effects have started to gain more attention in practices of migration and border security. In essence, many theories and frameworks have examined and theorized about technologies’ (in)ability of transforming organizations and the coordination of work [@pollockSoftwareOrganisationsBiography2009]. While some scholars have theorized the introduction of information technology as a force of organization change [e.g., @kallinikosWorkControlComputation2009], others instead emphasized the struggles encountered by actors of fitting global software to local circumstances [e.g., @hansethDevelopingInformationInfrastructure1996]. In addition, social constructivists have stressed the tentative meanings and specific contexts and actors in which artefacts emerge [@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. Critical studies of technological innovations and knowledge practices in migration management, borders, and security have thus also incorporated these aspects when examining how devices, practices, actors are mutually constituted and entangled. -->

<!-- ### Configuring data matching -->

<!-- STS scholars have devised different approaches to examine and understand the relations between designers, users and technologies [@oudshoornHowUsersMatter2003], and thus how expectations of designers can be inscribed into devices of migration and border security. For example, the approach of script analysis was advanced to examine the assumed competences of users and affordances that are embedded in artefacts [@akrichDescriptionTechnicalObjects1992]. The script of an artefact then requires users to adopt designer’s envisaged behaviours and actions to interact with an artefact. This approach also allows for accounting for situations when those assumptions by artefact designers don’t match the actual uses and practices. In the context of border security, for instance, a biometric fingerprint scanner might have certain assumptions about bodies to make them legible [@kloppenburgSecuringIdentitiesBiometric2020a]. @woolgarConfiguringUserCase1990 furthermore noted how system designers might attempt to ‘configure the user’ of the system by incorporating the user into the sociotechnical system. The method to understand such configuration is by treating the computer systems as a text that is read by users and can be interpreted differently. Designers may thus attempt to anticipate and delimit this flexibility. In these views, a sociotechnical system functions well when users and the system are successfully configured together. -->

<!-- While notions such as ‘script’ and ‘configuring the user’ have been generative in understanding relations between users and technologies, they tend to overemphasize the role of certain actors such as designers [@oudshoornHowUsersMatter2003]. More recent approaches have therefore suggested to, for example, take a more longitudinal approach to examine important moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. Suchman [-@suchmanHumanmachineReconfigurationsPlans2007] furthermore argued that the inscriptions of users and uses are never that coherent, and that a more open-ended and indeterminate approach towards artefacts is required. Analyses of sociotechnologies of migration and border security should therefore include a wider range of actors and moments of design, development, deployment, use. -->

<!-- Configuration can, in accordance with Suchman’s view, also sensitize us to the material and discursive elements of software for dealing with data uncertainties. She distinguishes two broad uses: ‘First, as an aid to delineating the composition and bounds of an object of analysis, in part through the acknowledgement that doing so is integral not only to the study of technologies, but to their very existence as objects’ (ibid., p. 48). The software tools for searching and matching data, for instance, structures the way officials can find and access information about people, and set boundaries between them, such as their capacities for acting on searches and results. The second use she describes is in ‘in drawing our analytic attention to the ways in which technologies materialize cultural imaginaries, just as imaginaries narrate the significance of technical artefacts’ (ibid., p.48). In the case of data management such expectations may be in the way data quality uncertainties are thought to be resolved. For example, either by focusing attention on activities to detect and correct data issues, or by making systems directly cope with uncertainty in data [@keulenManagingUncertaintyRoad2012]. -->

<!-- Following these approaches, this research aims to answer the question of what kinds of expectations and materialities are joined together in technologies for data management and reducing semantic uncertainty. Analysing and comparing the technical designs with actual uses, and thus also where they mismatch, can give evidence of the various expectations and assumptions at play. Yet, at the same time we should be both more specific to the situatedness of the design and keep a more open-ended and indeterminate view of how such technologies are put to use. -->

<!--chapter:end:sections/02-literature.Rmd-->

## Empirical case and methods {#empirical-case}

The research presented in this chapter draws on data from fieldwork conducted digital and in person from July 2020 to July 2021. In this research, I investigated how particular forms of searching for identity data provided by a data matching solution — such as taking for granted uncertainty about data and the incommensurability of different naming practices — embed particular expectations that shape users, their work practices, and the organizations where it is deployed in particular ways. I made use of standard methods (observations, interviews, document analysis) to situate and analyse the interactions between technologies, workplaces, and work practices [@ackermanResourcesCoevolutionArtifacts2008; @luffWorkplaceStudiesRecovering2000; @suchmanHumanmachineReconfigurationsPlans2007]. The fieldwork was informed by an initial understanding of the field and the use of this technology in transnational information infrastructures for migration and border control. As such, the research design can be considered a kind of "strategic ethnography" [@pollockEInfrastructuresHowWe2010] with specific choices regarding the setting and the scope of the research. Overall, the research is part of a more comprehensive endeavour to examine co-evolutions of development and deployments of a software solution deployed in EU and Member State identity management systems and practices of identification.

During the fieldwork period I joined the Netherlands-based company WCC Group, which created a software product for dealing with data matching and data deduplication, and which is used, for example, in the exchange of visa data in the central system of the EU Visa Information System (EU-VIS), and in the case management system of the Netherlands’ immigration and naturalization service.[^wcc].  More specifically, I joined the "Identity Team" (ID Team) of WCC (an organizational unit handling the company’s "Identity and Security Solutions" segment) to investigate the design, use, and evolution of their software solution for searching and matching identity data. Following the research design, it was crucial to be able to examine, on one hand, the design and development of the software, and, on the other, its use by institutional actors in practices of identification.  As a temporary member of the ID team, I visited the company’s headquarters in Utrecht (The Netherlands), had access to relevant documentation, carried out individual interviews with people, and attended some of the team’s joint meetings.

[^wcc]: Previously, the company was named WCC Smart Search & Match.

The research focused on comparing technical details of WCC’s software system called the "ELISE ID platform" (ELISE) — previously known as "ELISE Smart Search & Match" — with its use in processes of identification. The software products for searching and matching data are, among others, used by government agencies with some forms of identity management systems, such as for searching, matching, linking data information about clients. Briefly stated, the ELISE software provides advanced functionalities for searching and matching of information on customer’s existing databases. The novelty of the technology comes from using various kinds of algorithms to match identity data. For instance, by taking into account birth of date data may be incomplete or switched values for the date and month. Essentially, a search for a client’s data (e.g., based on their name, nationality, date of birth) will return a set of records with values indicating the probability that two identity data records match, rather than only exact matches.

Stated differently, the software is designed to deal with inexact searches and matches arising from difficulties of matching between personal data from different locales, scripts, cultural contexts, and so forth. Furthermore, the various algorithms expect that there can be mistakes in both the database and the parameters of a search. The starting point of this research was that such a subtle approach to identity data would shape identification practices through the tool’s specific characteristics and capabilities. The research assumes that in order for such search to work well, for (street-level) bureaucrat to effectively use the search and interpret the results, both the system and its users need to be attuned [@suchmanHumanmachineReconfigurationsPlans2007; @woolgarConfiguringUserCase1990]. Focusing on the kinds of frictions users encounter while using the search and match functionalities in the IND systems was therefore thought to highlight the underlying assumptions and expectations in the sociotechnical configuration.

For this chapter, I draw on the analysis of the integration of the ELISE ID platform with the systems of the immigration and naturalization service of the Netherlands (hereafter, IND). The IND is responsible for, among others, processing the applications from people who want to stay in The Netherlands or who want to become Dutch nationals. Practically, the IND case management system uses the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system and to deal with data issues such as duplicate records. Conceptually, the use of the ELISE search and match solution at the IND is relevant to understand how these heterogeneous elements are formed together and shape bureaucrat’s practices of identification.

To begin, I sought to find evidence of frictions between design and use of the tools by comparing how different organizational actors at the IND use the data matching capabilities in their daily tasks when searching for identities. Such comparison was thought to help discover how users’ expectations about data — and uncertainty surrounding data — is influenced by the software solutions. That is, how these expectations may be the source of tensions in expected and actual use of the search and match functionalities. Under such circumstances, frictions can also give evidence of discrepancies between users and the system of how to search and what constitutes good data. As I will explain further, there are indeed differences between the designed probabilistic identity matching, and user’s understanding of, for instance, the search results of the data matching process.

### Document analysis

The design and expected uses of the software were analysed through documents related to, first, the more generic technical details of the ELISE ID platform. And second, through documents related to the specific implementation of the platform at the IND. Members of WCC’s ID Team provided me these documents — such as technical design documents and meeting minutes —, as well as additional context and updates on current developments. My analysis of documents was furthermore grounded in understandings developed by STS of documents and practices of documenting as objects of study. As @shankarRethinkingDocuments2016 explain, different STS scholars broadened the understanding of documents as artefacts that don’t just document and stand for something in the world. Rather, documents play a role in social contexts — such as accounting for and coordinating of workplace activities — and cannot be separated from the practices through which they are produced.

Analysing the technical documents enabled, on one hand, to form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents gave insights in the genealogy of the package and the practices of configuring, deploying, designing the software. For example, by looking at different versions of documents (changes within the documents, added annotations, etc.). In addition to the more technical documents, public communications and reports helped gain contributory insights into the specific context of developments of the IND information systems. The outcome of the document analysis served as input for developing the interview protocols.

### Interview

The next step of the research was to gain insight into the actual development and use of the search and match tools, and to contrast this with the designed and expected use. For this, I conducted semi-structured interviews with a diverse set of actors. This approach was thought to contrast views on the development, expectations, uses of the tools for searching and matching identities. Overall, my interview approach revolved around two main groups of themes and participants. The first group resolved around people at the IND whose tasks involve the searching and matching of identities in their databases — and which thus involves the use of the ELISE ID platform. The second group centred around people at WCC who are, more generally, involved in some way or another in the development, deployment, (pre-)sales of their software. This approach made it possible to examine the mutual adaptations of the ELISE technology, WCC, and the IND.

Members of the ID Team facilitated establishing first contacts with the IND organization. Due to the Covid-19 pandemic related measures it was not possible to have any in person interviews. For this reason, participants were contacted to schedule an online meeting or phone call. All things considered, a benefit of these online meetings may have been that it was in fact easier to set up interviews — neither me nor participants needed to travel. On the other hand, not all communication may have come across in the same way, and it may have hampered the possibility for networking and scheduling more interviews. After the first contacts, a kind of snowball sampling was used for reaching out to more users willing to contribute to the research. In total, I conducted five interviews regarding the use of the search and match tools at the IND and seven interviews with people from WCC. Each interview lasted around one hour. However, I had little control over the sampling size of respondents and the reliance on their networks furthermore lead to a bias in who was included — in this case more senior people of the IND and WCC.

### Data coding and analysis

For analysing the fieldwork data, I followed standard methods for coding and analysing qualitative data. After collecting and preparing data from documents and interviews (including transcription), I coded and analysed the data using the computer-assisted qualitative data analysis software ATLAS.ti. After an initial inductive coding based on common themes and ideas, I reviewed and gathered similar codes to find patterns, processes, and typologies. For example, I developed a non-exhaustive typology of data frictions for alphanumeric identity data attested by the IND interview data. This typology then included types such as: frictions resulting from human errors during data entry, ambiguities and incommensurabilities in transliterations, differences in identification policies, and so forth. In the next section, I will present the principal findings of the comparing the differences in design and use of the search and match tools in practices of re-identification.

## Findings

The IND information infrastructure is characterized as a relatively complex environment influenced/determined by

* interactions with other (government) organizational links in the migration chains in which the IND works.
* interaction between components of the IND systems itself

### (Re-)identification at the IND and beyond

The IND's bureaucratic re-identification processes and practices are best understood in the context of the agency's information infrastructure. Through a formal collaboration between various governmental and non-governmental organizations, the IND is one link in what is (metaphorically) called the "migration chain" (_migratieketen_, hereafter MK). Each link in this chain (called a "chain partners" _ketenpartners_,  KP) joins together processes that foreign nationals in The Netherlands go through: from entering the country, to obtaining a residence permit, to naturalization, to departure/expulsion. The IND is specifically defined as an "identifying chain partner" (_identificerende ketenpartner_), which means that the agency can determine and register personal data of foreign nationals in the common registers, as well as change existing personal data. There are some differences between different identifying chain partners. Most importantly, only the _Vreemdelingenpolitie_ (national police) and _Koninklijke Marechaussee_ (national gendarmerie) can allocate identities in cases of undocumented migrants or suspected identity fraud. (Re-)identification in bureaucratic procedures of the IND should be understood in relation to this wider collaboration and coordination of migration policy and information exchange.

A crucial component in the information infrastructure that enables collaboration and coordination of the affiliated KP is the _Basisvoorziening Vreemdelingen_ (BVV) system. The BVV's data and issuance of uniquely identifying numbers (called _v-number_) constitutes a kind of "single source of truth" to share and consult information about foreign nationals between KP. Simultaneously, the BVV database is filled and changed from the KP's systems with identity data, information about travel and identity documents, biometric characteristics and status data (e.g., the outcome of asylum application). That is, even though each KP has its own database and information about migrants, data are kept aligned via the BVV [@InformatievoorzieningVreemdelingenketen2015]. For instance, INDiGO users searching for person matches always need to first consult the BVV. In practice, re-identification involves an interplay of multiple systems and practices. It can thus be suggested that the BVV's unique identification of migrants can be considered a kind of "boundary object" [@starInstitutionalEcologyTranslations1989] that makes it possible maintain a common identity across the MK while also allowing for the specific contexts of the partner organizations.

Various mechanisms are in place to ensure that "unique, unambiguous personal data of optimal quality are available in the migration chain" [@PIL2022, p. 9]. An important element is a protocol for standardizing the work of identifying and registering foreign nationals in the MK[@ministerievanjustititieenveiligheidProtocolIdentificatieLabeling2020].The protocol also describes data governance processes such as how identity data may be modified or when data must be destroyed. As such, the document describes the various partners, their links in the information architecture and role in the MK. As mentioned in the literature review, this dissertation sees identity as the outcome of practices involving heterogeneous actors. Hence, the number of organizations involved in the MK would lead to what @pelizzaIdentificationTranslationArt2021 calls "a proliferation of sociotechnical ‘spokespersons’" (p. 2) that would complicate future re-identification. As a result, the protocol for ensuring uniquely identifying data and enabling re-identification can be thought of as a way to reduce the multiplicity of possible identities.

In an interview with an IND employee, I was given an illustrative example of the proliferation of identities and the subsequent problems of re-identification (Interview 2021-01-29). The example recounts a scenario in which a migrant applies for residency at a municipality, which must then notify the IND in order to update the applicant's data. In the following interview quote, the interviewee explains how an automated message exchange can fail to re-identify the applicant due to differences in naming and identification practices and policies:

> In principle, the municipality only registers applicants who submitted such an application [for a residence permit] to the IND. A condition for registering with the municipality is that applicants must identify who they are. So that can be done, for example, with a birth certificate, a copy of a passport, or an identity document, or other documents, so to speak. The municipality does have a different kind of policy on identification than, for example, the IND. They have a different ranking of pieces that they, well, consider important to have.

> For example: we — the IND — see a copy of a passport sufficient, or an ID card, or even a laiser-passer. The last one is a kind of document issued by the embassy if the client does not have a passport or ID card. But the municipality… for them, the most important document to register someone is actually a birth certificate. And then you sometimes have differences, because, for example, applicants from, well, for example from Ukraine. They have, say, a name, and then a patronymic. That [patronym] actually refers to the name of their father. And then the family name. And, well, that patronym is often included in the registration of the municipality.

> But the IND, on the other hand, does not necessarily register on the basis of the birth certificate data; because those data were once given at birth, but of course they may have changed after many years. Because it is possible, by the way, that you take your marriage name, for example. So, if the client submits a passport with the marriage name, the IND will register the client on the basis of the passport data. While the municipality uses the birth certificate data. So you already have a difference. And we may then receive an automatic message [from the municipality], which the system cannot automatically link to a client.

This example demonstrates an important difficulty of the IND's re-identification processes and practices in determining whether or not a client is already known to the IND. Generally speaking, (re-)identification can happen in processing various information streams of the IND. The IND can, for instance, receive new applications, such as an application for a residency permit. In this case the applicant is commonly not yet in the IND system and a new client may need to be inputted into the system. However, the applicant may also already be known in the MK/BVV. In that case the client data needs to be linked(Interview 2021-01-29). As the following interviewee remarks about the process, there may be nonetheless by differences in the data records of the IND and BVV which will need to be investigated and corrected:

> Well, we actually search first on the system called BVV [...] We click on a button and then a search is made for the personal details that then appear. Well, if we have a hit, it means, for example, that either the Royal Netherlands Marechaussee, or Foreign Affairs, or the police have ever registered the client. Well then the data only occurs on the system called BVV. And if so, well, we'll make a link. Then we click on a button, and then there is a connection between the data from the BVV with the data we have received from the municipality. And if that is not the case, but for example that you can find the client in the BVV, but also in our IND system. That's when you press another [search] button. And when it turns out that the client appears in the BVV and in the Indigo system. Well, then we check in the Indigo system whether the names match completely, for example. In case of small changes in the name data, we also look further into the file. And if we do come to the conclusion: this is the same person. Then we also make the connection, so we register the client. We link the data together. Well, then you just have 1 client file, and then nothing is wrong. However, because there is a difference in personal details, for example, we have to report this in the system.

So far, this section has examined three aspects of re-identification in relation to the IND's information infrastructure. First, that (re-)identification in bureaucratic procedures of the IND should be understood in relation to the wider collaboration and coordination efforts. Second, that different mechanism for ensuring uniquely identifying data and enabling re-identification can be thought of as a way to limit the proliferation of identities from the various partner organizations. Third, that nonetheless these various mechanisms, the IND faces difficulty in determining whether or not a client is already known. In any case, whether entering a new client or adding new information to an existing client's file, the systems' search and match tools appear to be crucial for re-identifying clients. Let us therefore now look more closely to the integration of the ELISE search and match software package with IND's information system INDiGO.

### A brief history and overview of INDiGO

The IND's information infrastructure is an example of a system-level bureaucracy [@bovensStreetLevelSystem2002], with decisive roles for technologies, automation, information management for processing digital dossiers. The IND systems were designed to separate policy implementation, such as applying business rules related to the Dutch Aliens Act, from information management, such as data storage, searching, and matching [@kpmgitadvisoryAuditINDiGOWillen2011]. Technically, the case management system (called INDiGO) achieves this separation by utilizing a Service Oriented Architecture (SOA) design, which is a software architecture that aims to compartmentalize system functions into (more-or-less) independent services.

The design decision essentially aims to separate what INDiGO documentation refers to as the "flow" from the "know." By isolating aspects related to the application of laws, regulations, and domain knowledge, a rule management system "steers" the processing of digital dossiers with automated support (Figure \@ref(fig:landscape)).[^indigo-delays] This design addresses a common issue with rigid (governmental) IT systems, in which domain knowledge and business rules are tightly entwined with the system's operation. As a result, the IND rule management is easier to manage by the IND itself and better able to adapt to changes in legislation and regulations [@blankenaINDKanNieuwe2013]. Yet, how can we understand re-identification in relation to the interactions between IND system components? In this context, the ELISE software package is critical as the INDiGO component that supports client data searching and matching.

[^indigo-delays]: The SOA architecture of the INDiGO system received praise during its initial development in 2009 [@toetIndigosysteemVanIND2009], but the rollout of the new system in subsequent years did face a number of delays and problems [@bergsmaSysteemINDDuurder2013].

Based on my analysis, there are at least three uses of the ELISE software package for re-identification of IND clients. The ELISE software was initially used during the transition from the old IND information system (INDIS) to the new INDiGO. During a transition period when INDIS and INDiGO were running in parallel, the IND used the ELISE software to migrate legacy data by re-identifying matching client identities between the two systems.[^indis-indigo] Second, INDiGO makes use of the ELISE software to make client data searches easier. In place of a "traditional" search, which can fail to return results when search criteria are too strict or contain errors, the software's fuzzy search algorithms provide more advanced and reliable searching capabilities. Third, the ELISE software is used on a regular basis to search the database for possible duplicate client data. Broadly speaking, the software attempts to match all recently created clients to all other clients in the database. Potential duplicate matches that meet certain criteria will then be flagged and investigated further. In all three uses, clients are re-identified using match scores calculated by the software based on the likelihood that a client in the database meets the given search criteria. Let us now look more closely at the various types of searching and matching.

[^indis-indigo]: This is technically possible by running the software's algorithms on data that has been _replicated_ from both INDIS and INDIGO (i.e., regularly copying data from both data sources into the ELISE in-memory database). The initial rollout of the INDiGO system started in 2009 and was completed in 2013.

```{r landscape, echo=FALSE, fig.cap="A rough visualization of the IND’s information infrastructure supporting the various tasks of administering interactions with applicants, processing their applications, and collaborating between different organizational units and MK partners."}
knitr::include_graphics("figures/ind-landscape.pdf")
```

### Different types of searching and matching

The ELISE searching and matching is designed to function generically and decontextually as a separate component in the architecture of the INDiGO system. The system's technical documentation describes how queries from IND end-user applications are sent to the ELISE service, which then executes fuzzy matching algorithms and returns the results for display in the graphical user interfaces. As a result, the searching and matching service has little information about where in the INDiGO system and process it is called, or who is querying the system. This first finding contradicts much STS research on the co-construction of users and technologies [for example, @hyysaloNewProductionUsers2016; @oudshoornHowUsersMatter2003, @woolgarConfiguringUserCase1990]. To provide a generic search and match software package, the ELISE service does not consider specific users. And, in contrast to the process described by @pollockFittingStandardSoftware2003, there has been little back-and-forth between a "generic user" from the software package and a more "specific user" for the IND system. What does the lack of specific users imply for the IND's re-identification practices?

The interviews showed that different types of searches and matching can be distinguished. According to my analysis of the responses I received from IND staff members, different searches require different levels of verification and substantiation. The most basic and widely used method of searching for client data is to use rather precise personal data such as a name or identification number. These IND staff, for example, must process new information pertaining to an application. The employee may interact with the applicant directly at a font office counter or process documents at their back office desk. A distinguishing characteristic of these general uses of the search is that little interpretation of the input data (identification numbers, personal data) is typically needed to query the system.

To begin, search input may become more ambiguous and open to interpretation — and thus more error-prone. IND staff members who deal with phone calls or handwritten documents may fall into this category. In these circumstances, comprehending or piecing together the information to formulate a search query may require more effort. For example, the client's name may be misspelled or difficult to read in a handwritten document. Some search tool features may also become more relevant. In the case of phone calls, for instance, ELISE features such as taking into account different phonetic versions of a name or the possibility of numbers or letters being accidentally switched may be more relevant. While users working with handwritten documents may find functionalities that account for transcription mistakes and name variations more relevant.  In these re-identification situations, the ELISE system can be thought of as a mediating link in the chains of "translations into legible identities of individuals" [@pelizzaIdentificationTranslationArt2021, p.1]. Some examples of data frictions for the search input are given in Figure \@ref(fig:search-input), including errors in strings, integers, and dates, challenges with understanding and transcribing personal data, and methods used for inputting the data.

```{r search-input, echo=FALSE, fig.cap="This diagram ."}
knitr::include_graphics("figures/code-groups/data-friction-input.pdf")
```

Another distinction between search and match tool uses is the need to interpret and scrutinize search results. There are cases that require extensive interpretation of results despite having clear-cut input. For instance, there are important automated information exchanges with other MK partners, that sometimes necessitate manual intervention if the process fails to automatically identify the correct client to link the data. As previously mentioned, municipalities in The Netherlands, for example, use an automated information exchange to send residency data to the IND. In most cases, the search and match software will identify the corresponding IND (the match with the highest match score). If no good match is found, an IND employee will be required to identify the corresponding client in the IND's system. Furthermore, such examples provide a different perspective on the discussion of discretion in relation to automation in the public sector [@petersenRoleDiscretionAge2020] by being specific about the technologies [@monteiroSocialShapingInformation1996]. In practice, re-identification encounters are carried out in relation to particular sociotechnical contexts, sometimes automatically, other times requiring intervention.

Finally, there are times when the input is ambiguous and the search results must be scrutinized closely. For example, the first registration and start-up of a procedure for a client, particularly if it involves processing written documents. Documents, as previously stated, may be unclear and contain errors. Simultaneously, there is more uncertainty and a requirement to carefully investigate if a person is not already registered in the IND or MK databases. For example, an applicant may have been previously registered under their name before marriage. Some examples of data frictions for the search output are given in Figure \@ref(fig:search-output), including dealing with too many results, or none at all.

```{r search-input, echo=FALSE, fig.cap="This diagram ."}
knitr::include_graphics("figures/code-groups/data-friction-output.pdf")
```

Based on these findings, it is clear that there are gaps between the expected, decontextualized designed use of the ELISE system and the difficulties IND personnel face in interpreting search inputs and/or results. Because of the amount or types of information available for processing an application, users may have specific needs and expectations from the search. Some users, for example, can search for a person's record based on an existing identification, whereas others, such as those working with postal pieces, must conduct more difficult searches based on ambiguous handwriting and incomplete data. This disparity could be one of the reasons why some interviewees reported having difficulty using the search. For example, they stated that they must conduct multiple searches using various combinations of search criteria. I propose categorizing these uses based on the degree of complexity INDiGO users face when interpreting search input and output. As a result, four combinations of the needs to interpret personal information to search (input) and the needs to interpret search results (output) can be identified (Figure \@ref(fig:matrix) summarizes these four combinations and findings as a matrix).

```{r matrix, echo=FALSE, fig.cap="This visualization compares the needs for interpreting the input with needs for interpreting the results in the form of a matrix."}
knitr::include_graphics("figures/ind-users-matrix.pdf")
```

### Use and understanding of search functionalities

The ELISE system's expected use is actually very similar to the previously mentioned automatic search processes. In that case, when a client takes up residency at a municipality, an automatic update is sent to the IND with their personal information, triggering a search query in INDiGO. Ideally, the client can be re-identified using the calculated match criteria, and their status in the IND database is updated. As a result, it doesn't (or shouldn't) matter for INDiGO how match results are calculated, and ELISE algorithms are thus (by design) a black-box component. This automated process, I believe, demonstrates how ELISE searching and matching algorithms are indifferent to the context in which they are used. Next, I'll show how this indifference contrasts with INDiGO users' more adaptive use and understanding of the search tools during re-identification processes.

Methodologically, I propose to detect conflicting expectations that impede successful re-identification by contrasting the design of ELISE with the INDiGO users' knowledge of search functionalities. Prior research has found that alignment between users and systems is required for these configurations to function properly [@hyysaloNewProductionUsers2016; @oudshoornHowUsersMatter2003; @woolgarConfiguringUserCase1990]. Similarly, I want to demonstrate that alignment between the use and understanding of search functionalities is required for the search to function properly and allow IND personnel to re-identify clients. Otherwise, this lack of knowledge can impede effective use of the tools, resulting, for example, in the creation of duplicate records when the correct record could not be found using the search tools.

In all cases, the informants reported difficulties with interpreting search results when matches are based on clients' data from "historical fields." This is a specific element of the IND's data model The confusion for INDiGO users is that only the "primary fields" are presented in the search results. Talking about this issue an interviewee explained the problem users face as follows:

> "And what is actually very interesting in [the case of the IND] is that someone does not just have 1 address, but can have several addresses, for example. Or even several names. And, he may have changed his name, for example. So then the old name is also saved. You actually have a primary field, for example for name or for address. And you have historical fields. And they are all searched with ELISE. [...] So we actually have the history of every field. That can contain one value, but it can also contain 10 values. And if you match that. I think that there is also an interesting point with user expectations. [...] I think they're not always aware of that. That if they find someone, it can also be based on an old date of birth, which has been entered incorrectly or. Or based on an old name." (Interview 2020-08-05)

This system peculiarity can be understood as the _materiality_ of the data (model) shaping the system's operation [@dourishStuffBitsEssay2017]. Besides, it is well known that users modify their routines to work around system flaws [@gasserIntegrationComputingRoutine1986]. According to one interviewee, they would usually be able to identify such cases based on their experience. The user's workaround in this case is to examine the details of the results to determine why the match was included:

> "[...] experience has taught me that often the name has been changed. So there is also a history; so if someone has a different name, if a name is changed — and that is sometimes changed considerably — then you will indeed get it as matching. And when clicking through on the history of the name, you see that the logic comes from there, that it knows from history that it was called differently. And that is why it is shown. That is my experience." (Interview 2020-11-10)

Unlike in the case mentioned by @pollockWhenWorkaroundConflict2005, users were unable to shape the system's design to fix the workarounds. Rather than attempting to resolve such frictions, interviewees appear to rely on experience to understand results and re-identify the right client. Another interviewee stated, when asked about more advanced search functionalities, that users may not always understand how the search works but can usually re-identify clients:

> [N]ow and then it is very hazy how [the search] exactly works. For example, sometimes a letter seems to be more important than other times. Depending on where it is located. But you usually see if you misspell such a letter that you then do not get a hundred percent hit.But then you still get sixty percent or so. In some cases it is also higher than that percentage. But you usually find [the client]. (Interview 2020-08-05)

In general, interviewees indicated that they are familiar with different fuzzy search features. INDiGO users are aware, for example, that the matching system considers differences names spellings, name variations based on transliterations, and switched numbers such as in date of births or identification numbers. Matching based on name initials or name variations (for example, "Aleksandra" vs. "Ola"), (de)compounding names (for example, "Van Der Lei" vs. "Vanderlei"), or matching based on affinity matrices (for example, soft matching of year of birth within an acceptable range) did not appear to be well known. To some extent, this implies that the ELISE system enacts such knowledge as implicit or of minor importance to the users. According to this logic, what makes ELISE "smart" is its ability to take control of the search away from the user. In this sense, there is evidence of knowledge and expertise about re-identification being redistributed between employees and systems.

It should be noted that these problems could also be attributed to ELISE configuration issues in INDiGO. ELISE is actually intended to allow you to change the behaviour and importance of search criteria. This is not the case with the INDiGO implementation, and the graphical user interface clearly does not appear to assist inexperienced users in formulating effective queries. Technologically, the ELISE component has no control over the INDiGO GUI — neither the query specification nor the display of results. Most INDiGO users mainly learn how to use the tools by asking their coworkers for help or through trial and error. As a result, different users and departments within the IND organisation may have varying levels of familiarity with the tools' search and match features.

### Interpreting the search results and search strategies

> [...] on the BVV you can sometimes really get an error message with the result that no hits have been found because you are actually providing too many search options. Limiting the search options will yield results. While in INDIGO it is indeed that too; it always gives results. But, same effect; so the more data you sometimes provide, the more risk of match distribution, so that the customer is not always at the top that you are looking for.



> If you have more data, you look at what more you can put in it. So you're actually trying to make it as broad as possible. If you have a date of birth, you have a street name, or you have something else, to increase the matching percentage. And then you actually also look — if there are multiple search results — then you actually look first at the highest matching percentage.

> So then you try to see if you cannot find another way to find the customer. It may sound a bit strange, but sometimes you can see that… to which lawyer it was submitted, via which lawyer it was ever submitted. And then you can look through the lawyer, which client he has under him and that way you can also indirectly find out which client it is. But that is the difficulty when it comes to finding customers.

A third source of re-identification friction is how users interpret the system's search results, which are ranked by match percentage, in order to re-identify the correct client. The top results, those with the highest match percentage, are supposed to represent the most relevant matches. For example, a query based on a family name and a date of birth would return a list of results with data that roughly matches those fields of data, even if the name or date is slightly different. In theory, the matching personal data will be easily identified among the top results. In practise, this is not always the case. It became clear from the interviewees' responses that users have developed various techniques for making sense of results and exceptional cases.  "play" with the search tools until the re-identify the right client:

> "[...] what you often see in how they work is that hey, they use it first with one type data. And if they still get too many results, or they don't see it, they try with an extra piece of data. Or they try it with another kind of data. So you see, to find a person, they sometimes do five searches in a row. Also, a little, OK they could enter everything at once, but you can see they play with that a little bit." (Interview 2020-08-05)

In general, the search is designed to work best by including as much information as possible in the search query. The ELISE matching engine can then, ideally, use all of this information at once to calculate match scores. But, as the comment below demonstrates, there was a sense amongst interviewees that giving more input data does not necessarily lead to better results:

> "My own experience with searching for personal data is: the more data you enter, the more difficult the result will be. And the worse the result actually gets. So I often build it up. I do less data and if necessary I add some data if there are too many results." (Interview 2020-11-10)

The interviewee thus alluded to the use of a search strategy they came up with. Unlike the expected use of ELISE, entering too much query information may worsen the results and introduce uncertainty in re-identifying the correct client. As the comment below shows, users may even reduce uncertainty in re-identification by trying out different combinations of data or leaving out some information that can be used to cross-check the results:

> "And there is also a kind of self-check in [the search process]. So I often search by first name, last name; to start with. [...] But I often try not to do too much and see if that result is there. And on that basis, okay, the date of birth also matches the date of birth that I have. So I don't always deliver what I have available as information. But I also partly use it as a checkpoint for the search result that I then get to the top. It also works a bit more efficient for me. Because it makes no sense to enter much more data. Because you can find the customer anyway, also sufficient on the basis of first and last name. And you get insight so that you immediately know that you have the right one." (Interview 2020-11-10)

As a result of this conflict, users seem to have come up with their own "search strategies" (see Table \@ref(tab:strategies) for some examples). Strategies include withholding some information that can be used to cross-check the results or manually trying different combinations of data. These two strategies, in particular, contradict the search engine's purported optimal operation, which is to provide the system with as much information as possible. 

As one interviewee put it, users are "actually trying to fine tune [the search] so you can get the right person up." (Interview 2020-11-02) This expression of "getting the right person up" I think correctly captures the problems of re-identification that users face to try and get the correct client among the top results of the search results.

```{r strategies, echo=FALSE, ft.align="center"}
path = file.path(DATA_PATH, "search_strategies.csv")
data = read.csv(path, header = TRUE)

data$Code = substring(data$Name, first=34)

block_table(data[,c("Code", "Comment")], header = TRUE, properties = pt)
```

So far, the findings suggest that the ELISE software package for searching and matching identity data has become inextricably linked with data practises for re-identifying clients throughout the IND's bureaucratic procedures. On the one hand, the software package facilitates re-identification work by limiting common problems and uncertainties associated with personal identity data, such as transliteration variations and typos. The current search design, on the other hand, turns the searching and matching algorithms into a black box that can be opaque and difficult to use at times. Next, we will consider how these re-identification issues may be contributing to the INDiGO system's persistent problem with duplicate identity data.

### Duplicates and deduplication

Duplicates, or the presence of multiple (unlinked) records for the same person in a database, are a common issue that organizations need to address [@christenDataMatchingConcepts2012; keulenManagingUncertaintyRoad2012]. There are a number of scenarios in which a person could have two separate identity records, such as when two different systems are integrated. However, more often than not, duplicate records are added inadvertently because of other factors, such as work pressure or a lack of (automated) checks that make sure a person's data is not already present in the database. The following two interview quotes illustrate how duplicate identity records may be created at the IND:

> Yes, specific departments within the IND have that [problem of creating duplicates], which can indeed create a duplicate client more often. For example, counter staff can do that. Due to lack of, well yes, just having less experience with the system. At least with search keys. Of course, they have to work quickly because they have the client in front of them, so to speak. So maybe there is a bit more time pressure. And besides, my department, which is more trained on searching. So maybe we are generally searching a little better in the system. We also have those tools, those control tools, namely also in the system for that. And then we also have the postal department, which is called DRV, digital registration and preparation. And of course, they have many more clients on a daily basis, so there is of course a good chance that they will create a duplicate client. But also my department, even though we are very trained in this. We are also still creating duplicate clients. More than we'd like. (Interview 2021-01-29)

> We [the IND] also have contact with [MK] chain partners such as the Vreemdeling Politie [national police], the Koninklijke Marechaussee [national gendarmerie] and the like. They can also create clients themselves. Buitenlandse Zaken [foreign affairs] can also create clients. And it often happens that sometimes a client has already been created by the Vreemdelingen Politie and then [the client] comes to our counter, that it [the data] is created again and that it has been created again in a different way. (Interview 2020-11-02)

> So the moment someone creates a client and then a new document arrives. And the person after that, that could be a year later, that could be five years later, he's looking and actually finding nothing. But the client still comes first. And if he then creates it again, he often does not immediately realize it himself, but I get that back or from the decision process, from the client occurs twice. Or someone else also receives a document that then comes in and starts looking for the client, only to find out that it occurs twice. Then we have a deduplication process that then starts to turn the 2 clients into 1 client again. (Interview 2020-11-10)

These excerpts highlight three important themes related to the extistence of duplicates in the INDiGO system: how duplicates may be introduced, the potential risk of working with duplicates, and how duplicates are removed again. First, interviewees indicated that, in general, recurrent causes for duplicates to be introduced in the INDiGO system are, among others, related to time constraints, a lack of familiarity with the search tools, insufficient training, knowledge gaps, and the system's integration with other organizations. Second, having multiple client records for the same person raises the risk of the IND making wrong decisions. Third, what is known as the "deduplication process," is required to find duplicate records and turn the multiple client records into one again.

The ELISE search and matching system is also used to detect and remove duplicates. As mentioned in an earlier excerpt, interviewees cited a lack of automated checks as a significant reason for the creation of some duplicates. However, according to documentation, IND decided not to use automated checks to see if a client is already in the database when searching for or creating a client record.[^offline-online] Unfortunately, the reasoning behind this decision is not entirely clear. But as a result, the IND's deduplication service only uses automated database queries to scan for potential duplicates on a periodic basis. In general, the service looks for similarities between every new person record added to the database after a certain date and every other person record already in the database. For matching duplicates with ELISE, the IND has established specific rules for calculating the match score. If the match score between two records is greater than or equal to a threshold, both records are flagged as potential duplicates and must be investigated.

[^offline-online]: The INDiGO's technical design documents refer to these two approaches to finding possible duplicates as online and offline deduplication.

In addition to the automated query, "deduplication requests" are another, more direct method by which the "Titles and Identity" (T&I)  is informed of potential duplicates. Such requests may be sent by other IND departments or KP organisations, stating the data records and the justification for the request for deduplication. As an example, suppose an IND employee is processing an application and tries to re-identify a client. If the IND employee then sees that the client shows up twice in the list of results, they can fill out a form by hand and send it to T&I to ask them to merge the two records into one. The comment below shows how the "deduplication request memo" (_ontdubbelverzoek memo_) was made to standardise these kinds of requests within the organisation:

> Internally within the IND, yes, such a deduplication memo must be sent. And we have there on those requests, we have a kind of standard analysis. So basically an analysis based on system facts. For example, what comes in it is: we want to know if there is an identity difference of the customer. Because we've actually been working with that since — let's see — October 2019, when we also focused on identity differences. We call this "identiteitvraagstrukken" [identity question/problem] internally. That needs to be taken care of, and that was never done before. Until we got a case with a difference that we were aware of, hey, if we actually deduplicate files, we choose to make certain identity leading while this is not correct. Because my department just doesn't have that authority. And that authority is laid down in policy documents, among other things. (Interview 2021-01-29)

After the system identifies potential matches, domain experts evaluate duplicates and "deduplicate" records as necessary. During this deduplication process, users of the "Titles and Identity" (T&I) department will gather evidence to determine whether or not the records refer to the same person. If IND personnel determine that the records are duplicates, one client record will be designated as "leading." This means that instead of merging the two database records, one is made inactive. Some of the evidence gathered by users to make such decisions may be a combination of "weak evidence" (for example, identical addresses) rather than "strong evidence" (for example, identical identification documents). Given the importance of the decision to deduplicate, great care is taken to document the entire process and decision in a document known as a "memo." This way, others can see how the decision was made. Deduplication at the IND, as I see it, consists of four stages: (1) locating duplicates, either manually or through the query; (2) alerting users to duplicate records, either through the system or via email; (3) gathering evidence through T&I to decide whether or not to deduplicate the records; and (4) performing the deduplication in the system and logging the memo (sketched in Figure \@ref(fig:deduplication)).

> [...] you choose who… we call it “the survivor.” You choose who will be "the survivor" and "the loser." It sounds very hard, but that's how we choose who will be the survivor. And all data from the survivor remains leading. And you can possibly still find somewhere — if you search very well — some personal details, data of when he was born and the like. But certain data, file documents. You really can't do any more; unless it is really stated in the file document under which customer it was. But other than that you really can't figure it out any more. Cases, only if file documents are linked to them and that file document contains a case number. But otherwise you really wouldn't know any more. There is nowhere that a copy is stored of this was the situation. None of that is there. That is also the reason why people fill out that memo. So that you at least — should it go wrong — that you notice a little, that you can figure out a little. (Interview 2020-11-02)

```{r deduplication, echo=FALSE, fig.cap="The deduplication process at the IND."}
knitr::include_graphics("figures/deduplication-diagram.pdf")
```

What is also remarkable about the process of resolving duplicates is that it demonstrates the interconnectedness of the MK's various information systems as well as their transnational connections. Documents describing the various reasons for deciding to deduplicate frequently mention verifying identity records in other national and international systems. One example of strong evidence mentioned is when the National Police discovers a link between records by using information from the EU information systems, Eurodac or EU-VIS. In such cases, it is clear that other national or even international systems play a role in resolving local uncertainties about personal identity data.

In conclusion, duplicates in the INDIGO system are a recurring and significant issue for the IND. When everything goes well, tools like ELISE smart search and match assist INDiGO users in finding records and avoiding mistakes (mediated by the fuzzy search). However, respondents acknowledged that duplicates may still be created due to factors like time constraints and varying levels of familiarity with the tools. Thus, duplicates that still made it into the system are periodically searched for by a deduplication service. In an ideal world, checks would be performed immediately upon an INDiGO user attempting to add a new client record. Indeed, the IND has already implemented such automated checks in their online application processes. Nonetheless, these electronic procedures can also result in the creation of duplicate records, even if great care is taken to define the criteria and match score thresholds for potential duplicates. When taken as a whole, these findings shed light on how re-identification occurs within a system-level bureaucracy. Most semi-, non-, and fully automated bureaucratic processes depend on being able to correctly re-identify clients. However, the findings suggest that sometimes it might be simpler to produce duplicate data that needs to be cleaned again later rather than stopping a bureaucratic process when re-identification fails.

## Discussion

The findings suggest that the IND's re-identification practices are influenced in a number of ways by its use of the ELISE software package for searching and matching clients. In the first place, re-identification is influenced by the software's expectation that there is always some degree of uncertainty at play when searching through databases. In other words, these tools are built around the idea that both search queries and database records can have errors. In this way, this probabilistic method of handling data mediates every search query. Instead of returning a single value in response to a database lookup, the system returns a set of client records ranked by how closely they match the query. Moreover, the INDiGO system's fuzzy search algorithms and probability based ranking of matches mediate all client searches, whether they are performed by humans or automated systems.

The ELISE search and matching features are built around the idea that human error and semantic ambiguity are unavoidable. When entering names into a database or search query, IND employees sometimes make typos, reverse the order of fields like first and last name, and other similar errors. However, even if there are mistakes and incompatibilities in naming practises, the fuzzy matching aims to deal with these data uncertainties. The tool takes into account a variety of factors, such as the fact that there is no universally accepted method of transliterating names and that people may adopt their spouse's surname after marriage. As a result, the ELISE search can be characterised by its attempt to compensate for, and eventually replace, knowledge of all such variations and uncertainties in ways that no single user could. However, evidence from the various ad hoc search strategies indicates that these expectations do not always match actual use. For INDiGO users, search appears to be a black boxed system that they have learned to use through trial and error.

This chapter has focused on re-identification using alphanumeric data, such as a nationality or date of birth, rather than biometric data, such as fingerprints. Initially, the INDiGO system was designed to use ELISE's fingerprint matching to check if a client was already known by comparing their fingerprints to the entire database. Currently, such "one to many" matching is no longer done because fingerprints are only used to confirm someone's identity. According to one participant, matching fingerprints only "one to one" simplified re-identification for the IND:

> Yes, the work processes have changed at the IND. So it wasn't that it [ELISE search and match] didn't work. Because, yes, we still use the same algorithm to match fingerprints, so to speak. That was already included in ELISE, actually on top of the software. But now it's always 1 on 1 finger match. So we get the fingerprint of the customer who says who they are. So someone says his number, for example, his v-number. And then the fingerprint is collected there and compared with what he puts on the counter in terms of fingerprint, on the scanner. (Interview 2020-08-05)

The decision to use only fingerprints for authentication appears to contradict the widely held belief that biometrics are a more reliable form of identification. In my discussions with WCC's tech staff, for example, the use of biometrics was consistently cited as being simpler and more objective than other methods of identification. According to them, determining, for example, whether two fingerprints match is more precisely defined in biometrics in terms of mathematics or computation. When matching on less precisely defined alphanumeric data, context plays a much larger role. As a result, biometrics are frequently regarded as a means to reducing identification uncertainty. But this overlooks the reality that local conditions also affect biometrics. It takes effort to get a good fingerprint scan, such as knowing where to place one's hand on the machine, keeping fingerprints moist, and so on [@kloppenburgSecuringIdentitiesBiometric2020]. It can be argued that these "materialities of information" [@dourishStuffBitsEssay2017], influence re-identification systems and practises. The findings, expanding on the work of Dourish, indicate that re-identification is intrinsically linked to the shape that data takes. In other words, the materialities of more loosely defined biographic data versus more strictly defined biometric data facilitate and constrain re-identification in different ways.

Finally, the architecture of the system-level bureaucratic INDiGO shapes the IND's re-identification encounters. As mentioned previously, the service-oriented architecture separates the application's searching and matching functionality. This decoupling from other parts of the business logic, such as the rules engine used to implement policy, seems to have some advantages and disadvantages. On the one hand, the search component can be efficiently run and maintained because it is not tightly coupled with other aspects of the IND system. On the other hand, the search does not take into account the context of where and by whom it is performed. Yet, Interactions with the system and context can be seen as mutually constitutive [@dourishWhatWeTalk2004; @suchmanHumanmachineReconfigurationsPlans2007]. A possible implication of the current design is a redistribution of competences and knowledge, as users are enacted as passive participants, with ELISE's operation replacing their knowledge of migrant identity data and re-identification.

The current findings are significant in at least two ways. In the first place, the results shed light on less well-known practises and technologies of reidentifying people across data infrastructures, thereby contributing to debates on the materiality of identification. By concentrating on re-identification, we were able to see how identifying people on the move involves more than just first registration and biometric identification encounters. Second, the findings show how data matching technologies have altered the way routine bureaucratic re-identification is carried out in a system-level bureaucratic organization. According to the findings, the technologies' incorporation of previously dispersed bodies of knowledge, such as rules and methods for matching identity data, results in a redistribution of competence and expertise among both human workers and machines. More research, however, is needed to understand how technologically mediated re-identification influences bureaucratic discretion and, by extension, the (back office) enactment of identification policies [see, for example, @ustek-spildaStatisticiansBackofficePolicymakers2020].

<!-- Implications of findings for design of appropriate CSCW -->

<!-- * In the implementation at the IND the data cleaning efforts are still kept at the backstage -->

<!-- User experience of using the search tools -->

<!-- * Separation of concerns and the loss of context -->
<!-- * Making more prominent  -->

<!--chapter:end:sections/05-discussion.Rmd-->

## Conclusion

This study set out to investigate everyday bureaucratic practices and technologies of re-identifying clients across the data infrastructure of the Netherlands' immigration and naturalization service. The findings of this investigation showed the interconnectedness of a software package for matching identity data with IND data practices for re-identifying clients throughout bureaucratic procedures. On one hand, this software package supports the work by the system's affordances to mitigate data quality problems and uncertainties. If, for example, IND staff make mistakes in registering or querying client data, it should still be possible to retrieve the correct client. On the other hand, it was shown that the system's design transforms search into a black box that incorporates and redistributes diverse re-identification knowledge between humans and machines.

Although the current study is based on a small sample of participants, the findings suggest several practical implications. First, the way ELISE computes and presents results highlights the materialities of personal identity data [@dourishStuffBitsEssay2017] and ranked results [@pollockRankingDevicesSociomateriality2012]. On the one hand, the materialities of INDiGO's data models and alphanumeric/biographic data shape how data can be used for re-identification. It is possible, for instance, to match on previous names, but it may be challenging for a user to comprehend when the system presents such a match on historical data in the results. The presentation and ranking of search results, on the other hand, influences how users make sense of the matches and, as a result, shapes re-identification practises. Overall, the interviewees' responses indicated a need for a more deliberate design on how to best guide users in formulating queries and explaining results. Additionally, INDiGO users may have different needs in searching and matching identity data in contrast to a generic "view from nowhere" [@suchmanWorkingRelationsTechnology1993].

Several questions remain unanswered when it comes to conceptualising identification in the context of system-level bureaucracies and device materiality/performativity. It is clear that the ELISE software package has a specific history in the way it brings together previously disparate bodies of knowledge, techniques, rules, and methods for matching identity data. Additionally, the generic design of ELISE might actually be seen as a software vendor's strategy for "generification" [@pollockSoftwareOrganisationsBiography2009] of (re-)identification. However, much current research on the design and use of such (security) devices ignores the roles of software vendors and their broader market strategies. The investigation into how this software has changed over time and while moving between organisations is the focus of the following chapter.
