# Research design and methods {#ch-method}

## Research objectives: Applying and testing the conceptual framework for tracing data matching in transnational security data infrastructures

## Fieldwork site and access

### WCC ELISE — A technology for matching identity data

### Deployments at the IND and EU-VIS

## Methods for data collection

### Traces of data models

### Document analysis

My analysis of documents related to the deployment of ELISE is grounded in understandings developed by STS of documents and practices of documenting as objects of study. As @shankarRethinkingDocuments2016 explain, different STS scholars broadened the understanding of documents as artefacts that don't just document and stand for something in the world. Rather, documents play a role in social contexts — such as accounting for and coordinating of workplace activities — and cannot be separated from the practices through which they are produced. For this document analysis, I therefore looked not only as documents that help explain how the software packages function and how they were configured and deployed. Instead, the documents also "refer to the practices, objects, rules, knowledge, and organizational forms that produced them" (ibid., p. 62). Such a document analysis can thus contribute an understanding of the key biographical moments in the life of this software package.

At a first stage, I began the research by studying documents related to, first, the more generic technical details of the ELISE ID platform. And second, documents related to the specific implementation of the platform at the IND. WCC provided these documents — such as technical design documents and meeting minutes — during several on-site visits at their head office in Utrecht, The Netherlands. Members of the WCC ID team provided me additional context for the documents along with updates on the current status of the implementation at the IND.

In general, I distinguished between two broad aims of the document analysis, corresponding to two different research questions and hypotheses. A first aim was to find evidences of frictions between design and use of the tools by comparing how different users at the IND use the biographic matching and multi-cultural name matching capabilities in their daily tasks when searching for identities. Such a comparison was thought to help answer RQ3 by understanding how users' expectations of data — and uncertainty surrounding data — is influenced by the software solutions. That is, how these expectations may be the source of tensions in imagined and actual use of the search and match functionalities. As I will explain further in Chapter \@ref(ch-dm-within-org), there are indeed differences between the designed probabilistic identity match, and user's understanding of, for example, the search results.

The second aim of the document analysis was to unearth important moments in the trajectory of the software package. Key insights from scholars in this regard are that instead of looking at single versions of document, we should pay attention to the processes of documenting [@shankarRethinkingDocuments2016]. For example, by looking at different versions of the document — changes within them, added annotations, and so forth. In the case of the deployment of ELISE at the IND, I found different versions of the design document, documents describing updates to the package, and meetings notes discussing changes to the configuration. However, documents generally do not have a clear provenance. In encountered plenty of ambiguity and uncertainty in knowing why certain documents were created. This especially the case for ELISE at the IND. Most of the documents date back a decade ago and people involved with the project are no longer part of the organization.

In order to understand how knowledge and technologies for matching identity data travel and circulate, documents should be understood as assemblages of different authors and sources. As such, documents do not have an easily identifiable origin but are "assembled from multiple sources [where] content often flows from application to application and document to document, constantly recycled, reworked, and repackaged" (ibid., p. 63). Such flows are easy to recognize in the ELISE documentation for the IND. Technical documents incorporate generic information about the package from other company documents with details specifically for the IND implementation. In this way the genealogy of the package becomes evident — how it has moved and evolved when moving across domains and organizations.

Finally, an important — but less visible — as aspect of documentation is missing or unavailable documents. During my fieldwork it became evident that documents for the deployment of ELISE in the EU Visa Information System would not be available for me. This is due to specific configuration of actors in the development of the EU-VIS systems. In this configuration WCC acted as the technology supplier and worked together with a technology integrator, Accenture. This meant that relevant documents are in the hand of this integrator. What's more, the technology integrator acts as sort of gatekeeper which makes it impossible to access such materials. This lack of documents therefore also is revealing of the practices of deploying such packages.

In short, studying the documents allowed me to, on one hand, form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents give insights in the genealogy of the package and the practices of configuring, deploying, designing the software. Table \@ref(tab:documents) gives an overview of some these most important documents. In addition to the more technical documents, public communications and reports helped me gain contributory insights into the specific context of developments of the IND information systems. The outcome of the document analysis served as input for developing the interview protocols.

```{r documents, echo=FALSE, ft.align="center"}
path = file.path(DATA_PATH, "documents.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Document", "Author", "Year", "Description")], header = TRUE, properties = pt)
```


### Interviews

In order to learn more about the design and use of the search and match tools, I conducted semi-structured interviews with a diverse set of actors. Overall, my interview approach revolved around two main groups of themes and participants. The first group resolved around people at the IND whose tasks involve the searching and matching of identities in their databases — and which thus involves the use of the ELISE ID platform. The second group centred around people at WCC who are, more generally, involved in some way or another in the development, deployment, sales of their software package for identity matching in the security and identity market segment.

More specifically, the interview approach focused on individual interviews, which allowed for more time and possibilities to discuss topics in detail. However, it should be noted that compared to other approaches opportunities to observe group dynamics and discussion were not possible. Having said that, individual user interviews were the easiest to set up and provided a high degree of detail of individual experiences. I developed a protocol for the interviews — which included questions and probes to use for follow-up — based on the research questions and insights from the document analysis. The Appendix includes a sample of these interview's questions.

In the case of the IND interviews, participants were not immediately accessible due to my unfamiliarity with the organization and the need to conduct research digitally. Members of the "ID Team" at WCC Group therefore facilitated establishing the first contacts with the IND organization. Next, I sent a mail to the contacts I received and invited them for an online interview. After these first contacts, I attempted to use a kind of respondent-driven sampling for reaching out to more users willing to contribute to the research. On the whole, this approach allowed me to include participants that were not known before the start of the research, but could be included through the use of their networks. However, I had little control over the sampling size of respondents. It should be noted that the WCC organization only had one contact at the IND to begin with. The reliance on their networks furthermore necessarily lead to a bias in who was included, in this case more senior people in the organization.

The need to conduct the research digitally was due to the Covid-19 pandemic related measures, which made it not possible to have any in person interviews. For this reason, I contacted participants by email to schedule an online meeting or a phone call. All things considered, a benefit of these online meetings and phone calls may have been that interviews were in some sense easier to set up — neither me nor participants needed to travel. On the other hand, not all communication may have come across in the same way, and it may have hampered the possibility for networking and scheduling more interviews. The need to resort to phone interviews was due to limitations encountered by participants in IT access. For the online meetings I mainly relied on the secured Microsoft Teams installation of the WCC company. However, not all participants — who themselves were working from home — actually had the rights to install this application on their corporate laptop.

Another hypothesis was that users involved with different tasks at the IND would use the tools for searching and matching identities in different ways. For this reason, I reached out to users of different organizational units at the IND (see Table \@ref(tab:ind-departments) for an overview of these units). Indeed, the interviews clearly showed that there are distinct uses and knowledge of the tools. Unfortunately it was not possible to conduct interviews with users from all units. Information about the use of the search and match tools in the other units did however emerge during other interviews. In total, I conducted five interviews regarding the use of the search and match tools at the IND, each lasting around one hour. The "Yes" or "No" values in the fourth column of the Table indicate if a user from this unit was included or not.

```{r ind-departments, echo=FALSE, ft.align="center"}
path = file.path(DATA_PATH, "ind-departments.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Org", "Unit", "Incl", "Description")], header = TRUE, properties = pt)
```

In parallel and following the IND interviews, I interviewed people from the company WCC. I structured these interviews rather differently and followed a more open-ended approach. The aim of the interviews was to shed light on important biographical moments of their identity matching software package. I therefore adapted my questions and probes on the person — i.e., their role, projects, and experiences in the company. For example, people having worked on the IND or EU-VIS project I could ask more direct question about the development and deployment of the systems. While people other profiles I could ask more generally about their relations with existing and potential customers in the security and identity market. Operationally then, I interviewed, on one hand, people I met during my fieldwork visit at their office. On the other, I relied on a kind of respondent-driven sampling to receive more contacts. This lead a group of participants from which I distinguish two clusters. First, there are the members of the "ID Team" at WCC, with different role titles such as consultant, pre-sales, solutions manager. Second, I interviewed more technical people, senior software developers and a UX designer. Similarly to the IND situation, all interviews were conducted online due to the restrictions related to the Covid-19 pandemic.

For all interviews, I requested permission to record the sessions using the informed consent form established by the Processing Citizenship project. The form guarantees anonymity and confidentiality for the participants, informs them of the project, and allowed them to specify how collected data may be used in the research. As per the protocol, I recorded only the audio with voice distortion. Furthermore, interviews were transcribed manually to ensure that no confidential data would be leaked via the use of automated transcription platforms. Table \@ref(tab:interviews) gives an overview of all interviews.

```{r interviews, echo=FALSE, ft.align="center"}
path = file.path(DATA_PATH, "interviews.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Number", "Organization", "Date", "Format", "Language", "Description")], header = TRUE, properties = pt)
```

### Events

I collected various relevant publications and participated in events which were organized and attended by a variety of representatives of industry, academia, Member States' authorities, and EU institutions. Table \@ref(tab:fieldwork-events) gives an overview and description of events I participated in, either in person or digitally. The purpose of attending such meetings was to understand how these various actors define and frame the problems and solutions of data quality of personal data in their systems. On one hand, I hypothesized that the presentations and discussions would include useful examples which could help understand the problems actual practitioners are facing. For example, during the Q&A of a panel on the implementation of interoperability at the eu-LISA conference in 2018 a member of the audience asked a question about how they would manage the large amount of false positives which might be triggered when the different databases are connected that contain sometimes old data. The answer given by one of the panellists was that, indeed, a large amount of man-hours will be needed to check all those false positives. This example is revealing of the otherwise background work that is needed to integrate distinct systems.

On the other hand, I hypothesized that the solutions proposed by industry representatives would make evident the kinds of imaginaries related to connecting and using data from different sources. For example, a common trope used to highlight the importance of tools for searching and matching data spread across different databases is based on presumed threats of terrorism. In one presentation, a software company used the real-life example of the Boston bombers who was, prior to the attack, added to police watch lists but with variations in his name and other personal information. The premise of these examples is that having such divergent information across databases makes investigations more difficult for officers, and could be potential blind spots.

During these events I made notes to capture such discussions and problems and solutions of data quality of people's data. In addition, I collected screenshots and other publicly available materials related to these events such as video recordings and handouts. From these notes and pieces of observations and others materials I selected the most relevant to be coded and used in the data analyses.

```{r fieldwork-events, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "fieldwork-events.csv")
data = read.csv(path, header = TRUE)

block_table(data, header = TRUE, properties = pt)
```

### Other observations

Other observations I use in the analysis are from news articles, press releases, and other articles which I collected to provide additional context and help me understand important moments in the biography of the WCC search and match tool and its deployment at the IND and EU-VIS. Generally, I collected articles which mention the company WCC and/or the of its software in the INDIGO system of the IND or the EU-VIS system. To do these queries I used the LexisNexis database and tool — which aggregates and makes it possible to query all major English and Dutch-speaking newspapers, online business news, and other related publications. For example, I found several articles detailing the progress of the implementation of the INDIGO system that were published in IT business news outlines.

What is not included in the LexisNexis database are some blog posts and announcements by companies publishing information related to the design, development, deployment, and use of the systems. To access such material, I relied on more conventional search engines. For example, I found that a competitor of WCC publishes frequent blog posts on Natural Language Processing such one titled "Understanding Dari and Pashto Names: A Challenge to Intelligence Gathering in Afghanistan" [@rosetteUnderstandingDariPashto2012]. From all such material I selected only the most relevant to be further coded and analyses. This means that, for the analysis, I included only those articles which either directly deal with topics of data quality, data matching, or other relevant discussing surrounding the functioning of the technical systems.

Together these different kinds of materials — news articles, blog posts, press releases — can assist in making a more finished picture of the biography of the software artefacts for searching and matching identity data.

## Techniques of data analysis
