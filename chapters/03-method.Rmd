# A conceptual framework for analyzing data matching in transnational infrastructures {#ch-method}

\chaptermark{Research design and methods}

This chapter develops a conceptual framework for making visible the multifaceted phenomenon of linking and matching identity data in transnational security infrastructures. The following sections introduce three methodological strategies for using data infrastructures as a research topic and resource to direct attention to specific facets of data matching. Infrastructure, associated practices, and tools often fade into the background when everything runs smoothly. The conceptual framework, therefore, draws upon three methodological strategies from the literature that devised ways of making parts of infrastructures more visible. The three methodological strategies are named after and inspired by @bowkerSortingThingsOut1999's original work on "infrastructural inversions" [see also @bowkerScienceRunInformation1994]. The conceptual framework unifies three such infrastructural inversion strategies into a visual tool that directs the analysis of identity data matching in transnational security infrastructures, detailed in the succeeding chapters.

## Infrastructures as topics and resources of research

What methods and frameworks are best suited to trace the developments of matching identity data in border and migration control? In recent years, there has been an increasing amount of literature on Europe’s technological systems of border security and migration control as infrastructures [for example, @amelungTechnologiesInfrastructuresMigrations2020; @dijstelbloemBordersInfrastructureTechnopolitics2021; @olivieriTemporalitiesMigrationTime2023; @pelizzaProcessingAlterityEnacting2019; @potzschEmergenceIBorderBordering2015; @pollozekInfrastructuringEuropeanMigration2019; @trauttmansdorffInfrastructuralExperimentationCollective2021; @vanrossemOntologyExplorerMethod2022]. Large technical systems or infrastructures have been a long-term concern for scholars studying the processes, actors, and tensions in the historical developments of technologies [@hansethDevelopingInformationInfrastructure1996; @hughesNetworksPowerElectrification1983; @starStepsEcologyInfrastructure1996]. The research on border security and migration control as infrastructures draws on this earlier scholarship and recognizes the critical role infrastructures play in organizing and shaping large-scale phenomena.

A fundamental premise of this dissertation is that social research can use digital infrastructures as both a research topic and resource [@marresDigitalSociologyReinvention2017; @pelizzaDevelopingVectorialGlance2016; @pelizzaScriptsAlterityMappingunderreview]. In other words, we can either study infrastructures as distinct research topics or use infrastructures as resources for examining other areas of social research. Similarly, this dissertation examines, on the one hand, infrastructures supporting the linking and matching of identity data as a standalone subject. On the other hand, examining infrastructures can open up opportunities for studying the internationalization, commercialization, and securitization of identification. Nonetheless, both options must address the elusive issue of defining what infrastructure is and where and when it begins and ends.

Typically, researchers get around the problem of a priori delineating infrastructure by studying infrastructure ethnographically [@karastiStudyingInfrastructuringEthnographically2018; @starEthnographyInfrastructure1999]. To this end, the researcher as ethnographer can approach infrastructure in a self-reflexive manner, building it “by every choice the ethnographer makes in selecting, connecting, and bounding the site and via the interactions through which s/he engages with the material artifacts and the people who define the field” [@blombergReflections25Years2013, p. 389]. While studying infrastructure ethnographically is a valid means of defining it, it places considerable responsibility on the researcher. Methodologies that put less responsibility on the researchers have also been developed. For example, not all agree that researchers necessarily have to be present with the actors. @beaulieuResearchNoteColocation2010 notes how we can rely on various traces of infrastructural mediation. This chapter takes cues from both approaches to create a conceptual framework that does not define infrastructures at the outset but instead follows practices and actors by examining the finer technical details of matching and connecting identity data.

Therefore, the rationale for investigating the technical aspects of matching and linking identity data in security infrastructures is twofold. One reason is that, as discussed in Chapter 1, research has yet to thoroughly investigate the mechanisms for matching and linking identity information in identification. For instance, linking identities across systems and databases with various informational representations must be better understood. Second, as noted by @pelizzaDevelopingVectorialGlance2016, focusing on the “technical minutiae” of such systems' and databases' interoperability can be “strategic sites” that can highlight more significant “institutional shifts” [see also @bowkerSortingThingsOut1999]. Similarly, looking at how identity data are matched and linked could show how these technical mechanisms also change how actors relate to each other, such as between governments and security companies. The following section outlines the three methodological strategies that form the conceptual framework's basis to invert the embedded and less obvious technical details of matching and linking identity data in data infrastructures. The conceptual framework will thus operationalize these strategies to examine the matching of identity data in EU and member state systems by using data infrastructures as both a research topic and resource.

## “Infrastructural inversions” for matching identity data

A well-known result of the overlapping between infrastructures and practices is that infrastructures tend to be taken for granted and “remain as invisible backdrops to social action” [@harveyIntroductionInfrastructuralComplications2016, p. 3]. Studies on infrastructures have consequently provided different methodological strategies to invert this tendency of infrastructures to disappear and to make visible the interconnections between technical minutiae and the politics of knowledge production [e.g., @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Methods proposed by these authors to invert the tendency of infrastructure to disappear include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019]. Likewise, I suggest that by inverting infrastructure [@bowkerScienceRunInformation1994], we can learn more about the interplay between technological systems for identity matching and the politics of identification in transnational security contexts. As indicated by @bowkerSortingThingsOut1999:

> "Infrastructural inversion means recognizing the depths of interde­pendence of technical networks and standards, on the one hand, and the real work of politics and knowledge production on the other. It foregrounds these normally invisible Lilliputian threads and further­ more gives them causal prominence in many areas usually attributed to heroic actors, social movements, or cultural mores." (p. 34)

The quote alludes to a crucial aspect of infrastructural inversions developed further in their book: that classification schemes and standards are essential infrastructural elements that underlie many practices. For example, analyzing the database data models is a promising methodological option to understand how identity is linked and matched across data infrastructure. Data models are classification systems that can be used to identify individuals. The specific categories of data included in these models can determine the technical feasibility of matching and connecting various biographic and biometric information. Depending on how the data is categorized, some individuals may be more easily identifiable and visible, potentially leading to different treatment based on their perceived risk or other factors [see for example, @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. To better understand the implications of identity data matching on people, it is crucial to critically examine the data models and categories utilized, as they play an essential role in how individuals are represented and enacted.

Moreover, data models institutionalize particular views about individuals, yet, the underlying politics and decisions in designing such representations and models tend to fade into the background [@bowkerSortingThingsOut1999]. For all these reasons, data models constitute valuable methodological resources. Consequently, accessing data about choices concerning data models and simultaneously comparing and contrasting how authorities match them is an important research endeavour.

### First inversion strategy: Comparing data models

Data models are embedded in digital infrastructures and practices and institute specific ways of knowing and working [@bowkerSortingThingsOut1999; @hineDatabasesScientificInstruments2006; @lamplandStandardsTheirStories2009; @timmermansWorldStandardsNot2010]. As such, data models designed to represent phenomena can be considered a technology that enacts particular kinds of knowledge, organizations, and practices [@bloomfieldVisionsOrganizationOrganizations1997]. The politics in designing these informational representations shape how information is represented and affect what becomes visible and, consequently, what turns invisible [@bowkerSortingThingsOut1999]. Applied to border and migration management, this implies that data models represent people at the border and make them knowable in specific ways [@pelizzaScriptsAlterityMappingunderreview]. For instance, family composition fields that assume a heteronormative family model might only include spouses and children. Identifying people through fingerprints instead of study degree or family tree shrinks the possibilities of how they can be known [@pelizzaIdentificationTranslationArt2021]. However, how individuals are enacted through the interplay between different systems has received comparatively less attention and is a type of inquiry that this thesis will address.

Scholars have usually paid particular attention to how database technologies make people at the border visible by allowing states to collect — and connect — different biographic and biometric data to distinguish between populations for different treatments [e.g., @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. Hence, much of the mentioned research on database technologies on the alleged invisibility of people shows a tendency to take for granted the embedded data models. Less attention has thus been given to the invisibility of those informational representations and the links between these representations in infrastructures for managing and controlling mobility, migration and borders. One objective of this dissertation is to render the data models used in infrastructure visible and to trace how they represent and embody particular forms of knowledge.

One of the problems in analyzing data models concerns their “thinness.” Data models provide little information on their own because their meaning is often only revealed through comparison, such as when one data model has (a set of) categories that another does not [@vanrossemOntologyExplorerMethod2022].[^comparison] By comparing different data models for similar phenomena, we can discover what was lost or made invisible while developing data models and standards. For example, @cornfordRepresentingFamilyHow2013 illustrate this sort of comparison by contrasting digital data standards created by public service initiatives in the United Kingdom to represent family relationships. Their comparison highlighted "the kinds of family relationships that are recorded and those that are not recorded or harder to record, any hierarchies, implicit or explicit, for family forms or relationships and the implicit and explicit assumptions that underlie the terms and classifications used" (p. 8). Therefore, the first infrastructure inversion strategy is to compare the data models in different government agencies' migration and border control information systems.

[^comparison]: As semiotics has theorized, meaning emerges from comparison. For example, @latourReassemblingSocialIntroduction2005 explains the significance of not defining groups a priori because "whenever some work has to be done to trace or retrace the boundary of a group, other groupings are designated as being empty, archaic, dangerous, obsolete, and so on. It is always by comparison with other competing ties that any tie is emphasized." (p. 32). Similarly, @pelizzaCommunityTextBack2010 recalls that "the situations where the social is made visible and graspable are those where meaning emerges from comparison" (p. 67).

Yet, comparing data models and standards from distinct systems is not the only type of analysis, and it mostly makes sense when systems are not integrated. When systems are integrated, it is often only through the interaction of various databases that people are enacted as specific subjects. Research by @ruppertNotJustAnother2013, for instance, has looked into UK information systems that share data on juveniles at risk of (re-)offending to facilitate intervention. In her study, she demonstrates how combining various data sources, such as biographical information from multiple agencies, produced specific subjects targeted for intervention. To better comprehend the processes of matching and linking identity data, it would be helpful to develop a method to recover the embedded data models, which will give insight into the relations between various systems.

In short, the politics of data model design shape how information is represented, influencing what becomes visible and, as a result, what becomes invisible [@hansethInscribingBehaviourInformation1997; @bowkerSortingThingsOut1999]. However, these data models and their choices for representing people tend to become invisible in infrastructures. In light of this, the first infrastructural inversion strategy suggests comparing the informational representation of infrastructures. A comparison of this kind would allow us to recover choices made in these technical standards, which quickly become invisible while still being involved in processes of enacting populations, territories, and borders. The second infrastructural inversion strategy will consider the even less visible work required when data only occasionally match up with these data models and the issues this mismatching creates for identity linking and identification within and across organizations.

### Second inversion strategy: data practices

Data are never "raw" because they are always the outcome of some combination of human decisions and technical constraints [@bowkerSortingThingsOut1999; @gitelmanRawDataOxymoron2013]. As seen in the first infrastructural inversion strategy, data are not raw because of their representational formats or models. The second infrastructural inversion strategy proposes to focus on the various and often invisible work of managing and cleaning data that are part of the "cooking". As such, the second strategy suggests using _data practices_ as a methodological lens to make the work of identifying and managing data in data infrastructures visible.

Data practices should be understood in the context of "practice theory," which, from a methodological perspective, treats social practices — rather than technologies, people, structures or other actors — as the main object of inquiry [@reckwitzTheorySocialPractices2002; @schatzkiIntroductionPracticeTheory2005; @shoveDynamicsSocialPractice2012]. For instance, investigating routine social practices like cooking [@rinkinenColdChainsHanoi2017] or taking daily showers [@handExplainingShoweringDiscussion2005] can draw attention to the material configurations and social orderings of energy consumption. Following @schatzkiIntroductionPracticeTheory2005, practice theory approaches, on the one hand, give accounts of the interconnected practices of a subdomain of human activity. On the other hand, practice theory considers practices "the place to study the nature and transformation of their subject matter". This latter view is supported by @shoveDynamicsSocialPractice2012, who proposed that "the dynamics of social practice" can help understand social changes by looking at the evolution and stability of practices. Practice theory has been successful in explaining how routine social activities are situated. However, the theory has only recently begun to explore how social practices travel, how they are interconnected, and how they relate to other "large phenomena" such as infrastructure, as highlighted by more recent works by @shoveMattersPractice2016 and @nicoliniSmallOnlyBeautiful2016.

The "critical data studies" literature has stressed how crucial it is to examine how data are produced, disseminated, and used [@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. Similarly, infrastructure studies emphasize that data practices are critical for understanding the barely visible coordination work that goes on behind the scenes of data infrastructures [@leeHumanInfrastructureCyberinfrastructure2006; @ribesSociotechnicalStudiesCyberinfrastructure2010; @monteiroArtefactsInfrastructures2013]. For instance, in the context of scientific work, scholars have demonstrated the hardly visible work required to make data sets shareable and usable by others [e.g., @edwardsScienceFrictionData2011; @kervinBackstageWorkData2014; @plantinDataCleanersPristine2019]. An example of data practices related to identity matching could involve linking, resolving, and deduplicating data from one or more databases to accurately identify whether different records refer to the same individual based on data categories such as name, birth date, and nationality. Therefore, examining the routine data practices of how identity data is linked and matched across various infrastructures can provide valuable insights into how these data are produced, disseminated, and used across data infrastructures.

Therefore, I suggest that these insights from practice theory and data practices apply to identity matching and linking procedures. Unfortunately, scholars have paid scant attention to the work and techniques used to match and link identity data, such as probabilistic record linkage, fuzzy matching, and data deduplication. And yet data matching activities and technologies are similar to those that make data discoverable, shareable, and connectable for collaborative work. Furthermore, a data practice approach would allow empirical investigation of how identity data is matched and linked across different agencies and organizations (RQ2). Therefore, the opportunity to make visible practices and technologies that pursue the matching and linking of identity data within and across data infrastructures promises to shed light on how people are re-identified in data infrastructures, such as applicants at various points in their bureaucratic procedures.

The literature on practice theory and infrastructure provides a valuable methodological insight for investigating the dynamics of data practices involved in matching and linking identity data. This literature emphasizes the interconnectedness of practices and how they can change and stabilize over time [@huiNexusPracticesConnections2016]. For instance, information used for such identification is frequently based on data gathered about people by multiple organizations or government agencies. As such, the tools that aid in matching identity data from multiple sources typically mediate the practices. For instance, a data matching system may incorporate a rule for determining if two names should be considered the same. This rule could be based on exact matches of first and last names or consider nicknames, alternative spellings, or cultural naming conventions. Such name-matching expertise is developed and included into data matching systems that are the deployed at various settings. The inclusion of such name-based expertise within the system's algorithms can significantly influence the identification of individuals, demonstrating the system's ability to shape data practices.

Practice theory emphasizes the interconnectedness between materiality and practices as "aspects of practice-arrangement nexuses" [@schatzkiMaterialitySocialLife2010]. In the context of data matching, this means that the techniques and technologies used to, match identity data are integral to the practices themselves. These tools may, for example, flag individuals and their data as irregular, which can have significant consequences. By examining data matching practices, we can gain insight into the performativity of data practices in relation to technological artifacts and infrastructures [@ruppertDataPracticesMaking2021]. This approach recognizes that the materiality of these tools and the practices in which they are used are mutually constitutive, and both must be considered in understanding how data is processed and analyzed.

Using data practices as a second infrastructural inversion strategy also fits in with a trend toward finer-grained analyses of knowledge production in managing migration and borders and how practices simultaneously enact populations and states, among others [@mcharekTopologiesRaceDoing2014; @jeandesbozSmarteningBorderSecurity2016; @glouftsiosGoverningCirculationTechnology2018]. For instance, contributions from two recent Special Issues [@cakiciPeoplingEuropeData2020; @scheelEnactingMigrationData2019] took a practice-based approach to analyze the mechanisms through which knowledge production enacts migration. Before that, @pelizzaProcessingAlterityEnacting2019 had offered a framework to consider the variety of ways non-European populations are enacted when they arrive in Europe and the simultaneous enactment of institutions through data practices and infrastructures. Data practices, which are an integral part of infrastructures, can help shed light on the phenomena to which security infrastructures give rise.

Drawing on these insights, the second infrastructural inversion strategy for data matching in transnational security infrastructures suggests investigating routine data practices. Focusing on the behind-the-scenes practices of identity matching and linking can illuminate how sociotechnical practices shape, transform and stabilize identification within data infrastructures. The intertwining of technology and practice is a central tenet of practice theory. However, challenges arise when understanding how these interconnected technologies have evolved and proliferated in different settings and within various organizations. The third inversion strategy will propose examining how technology has changed over time as another way to make data matching visible in data infrastructures.

### Third inversion strategy: sociotechnical change

Data infrastructures, including those for migration and border control, connect different contexts and timescales into large-scale networks to exchange and use information collected at multiple times and places [@dijstelbloemBordersInfrastructureTechnopolitics2021]. The networks that connect and shape relations between many actors, such as individuals, states, companies, and technologies, are often sources of tensions in the developments toward specific goals [@edwardsUnderstandingInfrastructureDynamics2007; @ribesLongNowInfrastructure2009]. The first infrastructural inversion strategy notes how decisions made while designing data models and systems generally tend to disappear in infrastructures and how we can recover such decisions through comparison. Tracing these technological changes over time can be a way to make visible the technical choices made in identification systems in dealing with the infrastructural tensions in matching and linking identity data. The third infrastructural strategy is thus founded on insights from sociotechnical change, which emphasizes how technology is developed over time and is influenced by contingent processes occurring in various contexts and involving multiple actors.

Several methodological tools exist for bringing technological design choices to light and for comparing how users, designers, system builders, and technological artefacts interact with and influence one another. For instance, a researcher's task in the script analysis methodology is to recover the scenarios designers incorporate into artefacts and describe the successes and failures of adapting to actual use [@akrichSummaryConvenientVocabulary1992; @akrichDescriptionTechnicalObjects1992]. Scenarios inscribed in objects imagine a mise-en-scène of the worlds where the actors and objects interact, much like a film script. Similarly, @woolgarConfiguringUserCase1990 suggested that researchers treat devices as text to be “read”. His methodological approach focused on how designers encourage specific readings by encoding user and system expectations in technological artefacts, much like the relationship between writers and readers. In addition to Woolgar's notion of configuration, it is crucial to consider the evolution of technology and design over time by adopting a longitudinal perspective to understand the various processes that shape and are shaped by them.

STS scholars have long documented the choices and paths of designing technological artefacts and systems using various methodological approaches to examine and understand the nexus between users, technologies, and designers. Rather than technology dictating human activity, social constructivists have argued that technological artefacts emerge from complex human processes involving multiple choices and unforeseen outcomes [@mackenzieSocialShapingTechnology1985; @pinchSocialConstructionFacts1984]. Along with theoretical underpinnings, scholarship like Social Construction of Technology has provided research methodologies that empirically show how the construction of technologies is tentative and shaped by the particular contexts and actors from which artefacts emerge[@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. As such, concepts such as “interpretative flexibility” [@pinchSocialConstructionFacts1984] and “boundary objects” [@starInstitutionalEcologyTranslations1989] are practical as heuristic methods that bring to light how various social actors and groups ascribe varying meanings to artefacts and make varied use of technologies. Consequently, these methodologies and concepts on technological change form the basis of the third infrastructural inversion strategy: examining how technology has developed over time thanks to the agency not only of designers but also of users and how it is influenced by contingent processes occurring in different contexts and involving numerous actors.

Understanding the morphing and shaping of software technology as a part of infrastructures necessitates moving beyond studies of configuring technology and users at single time and places, as @williamsMovingSingleSite2012 convincingly argue. Technological development should not be studied in a vacuum but take a longitudinal approach to examine the dynamic processes at various timeframes, settings, and actors [@pollockSoftwareOrganisationsBiography2009]. However, clear analytical criteria for identifying key moments in the many interactions of multiple actors in the co-construction of sociotechnical problems and solutions for linking and matching identity data are still missing and need to be developed.

Chapter 6 will therefore draw on the literature and sociotechnical change and infrastructure studies to further develop two methodological heuristics for detecting pivotal moments in technology development. The first heuristic will draw upon the concepts of “interpretative flexibility”, “closure mechanisms”, and “social groups” from the Social Construction of Technology [@pinchSocialConstructionFacts1984]. These three concepts can highlight the emergence and standardization of infrastructure by looking at moments when the meanings of technologies are challenged, changed, or closed down. The second heuristic proposes understanding how large-scale identification infrastructures emerge by tracing pivotal moments when previously separate systems are connected. This heuristic draws upon infrastructure studies' use of the gateway concept [@edwardsUnderstandingInfrastructureDynamics2007]. Gateways are essential when there are several independent systems; it is frequently uncertain which system will succeed or whether other technological and social compromises are necessary for systems to work together. Hence, gateways are essential at this stage of infrastructure development as previously incompatible systems may be able to work and communicate with one another through the gateway. As a result, these heuristics are methodological devices that help identify pivotal moments in the sociotechnical change of infrastructure for linking and matching identity data.

This section has reviewed infrastructural inversions to provide three methodological strategies for using data infrastructures as research topics and resources. These methods can guide reversing infrastructure's technical aspects of matching and linking identity data, which tend to be background activities. The next step is to combine these three infrastructural inversion strategies into a conceptual framework that we can operationalize to simultaneously make visible the technical and institutional dimensions of matching and linking identity data in transnational security infrastructures.

## A conceptual framework for analyzing data matching in transnational infrastructure

### Working model for operationalizing the infrastructural inversions

This section combines the three infrastructure inversion strategies into a conceptual framework that will direct the analysis of matching and linking identity data of particular transnational security data infrastructures. The three inversions of the infrastructure each seeks to answer one of the research questions reported in Chapter 2. Combining infrastructural inversions aims to comprehensively study various data matching dimensions and technologies used in transnational security systems. To visually schematize the analysis of the pertinent aspects of data matching, the conceptual framework further draws inspiration from computer science views on relational data models.

The conceptual framework explores data matching practices and technologies in transnational security infrastructures from three distinct perspectives, each of which corresponds to a research question:

1. Comparing data models can reveal what kinds of information various organizations and systems collect. In addition, such comparison can yield insights into how organizations search for, match, and use data.
2. Data practices can reveal the searching, matching, and linking of identity data within and across organizations.
3. Sociotechnical change can reveal how data matching knowledge, technologies, and practices travel and circulate over time and across organizations.

The accompanying table describes the connections between the research question, the infrastructural inversion strategy, and the data collection and analysis techniques (discussed in the following sections).

```{r tab.cap="Conceptual model", tab.id = "conceptualModel", tab.cap.style = "Table Caption", echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "conceptual_model.csv")
data = read.csv(path, header = TRUE)

block_table(data, header = TRUE, properties = pt)
```

The conceptual framework can also be represented graphically in three dimensions. (See \@ref(fig:method-axes-visualization)). This visualization is similar to a relational model in that data is represented as tables with relationships between them. The three axes represent different data dimensions: _data models_, _data categories_, and _data values_. Next, we will discuss how investigating these three axes and their interrelationships can enhance our understanding of data matching across and within organizations.

```{r method-axes-visualization, echo=FALSE, fig.cap="The conceptual framework is represented graphically in three dimensions in this visualization.", out.width="100%", fig.align="center"}
knitr::include_graphics("figures/method-axes-visualization.pdf")
```

The axis for _data models_ (axis `z`) represents the database and other informational models that standardize the kinds of information about people collected by different organizations' IT systems. An example of such a data model could be the schemas specifying data collected about migrants by a government agency. A single system and database can have its own unique data models. Data models can also be standardized formats for data exchange or interoperability between various systems. Next, data models define various _data categories_ (axis `x`), which are the types of information collected about people. For example, the data categories of the example asylum agency’s data model may include "name", "place of birth", and "date of birth". 

Whether stated or unstated, connections between databases can be found in both scenarios; for instance, one data model may use the "surname" data category while another uses the "family name" data category. Thus, the two data models attempt to capture the same real-world concept. Finally, there are the actual _data values_ (axis `y`) in systems’ databases for a data model and its data categories. For example, a value for the "place of birth" data category may be "Brussels". Additionally, there might be similarities between the actual data values in various databases. For example, in another database, "Bruxelles" might be the data value for a similar category. Let us now consider how these relationships can be conceptualized as combinations of these axes to represent three aspects of data matching.

#### Comparing authorities' data models

The first pair _(data models `z`, data categories `x`)_ represents the identification of semantic similarities and differences between different data models and their data categories. Connections between data categories in distinct data models may exist explicitly or not. For example, one government agency’s data model may include the data category “family name.” In contrast, another agency may use “surname” as a data category. Although the names of the data categories differ, they both refer to the same referent. In this case, a part of someone’s name indicates their family. Alternatively, suppose that while one data model categorizes all languages as “language,” another differentiates between “native language” and “spoken languages.” This discrepancy can tell us something about what designers expect. Following @pelizzaScriptsAlterityMappingunderreview, data models will be conceptualized as scripts, as they similarly encode and enact designers’ expectations regarding people on the move. Comparing these scripts can thus reveal the expectations of various actors regarding data collected about people and how data can be searched, matched, and used.

It is significant to note that, similar to what we want to achieve, researchers in computer science who study knowledge engineering, linked data, and natural language processing have come up with some technical solutions to compare data models. For example, finding relationships between different data categories is closely related to the outcome of schema or ontology matching processes, which automatically find correspondences between concepts of schemas or ontologies [@euzenatOntologyMatching2007; @kementsietsidisSchemaMatching2009]. Furthermore, analyzing a corpus of documents within a given domain of discourse is one of these (semi-)automatic mechanisms' primary means of extracting concepts and relationships between concepts within that domain. The recovery of knowledge representations from a discursive domain and examining the relationships between data models are thus common concerns. However, fundamental differences make it challenging to use such methods to support a discursive analysis that can answer our research questions.

Knowledge engineering methods are generally more concerned with developing technologies that help machines better understand data as a means for systems and integrating different sources of information to work together. On the other hand, the method for this research was expected to contribute to Processing Citizenship's goal of supporting discursive analysis that uses differences in representations as a starting point to analyze the imaginaries of various organizations and authorities and combine the analysis with ethnographic observation [@pcProcessingCitizenshipDigital2017]. That is why it is crucial to have methods to aid both qualitative and quantitative analyses of formalized data models. First, to assist analyses of information systems that specify their data models, even if these systems are rarely comparable. Second, to systematically and quantitatively support discursive analysis of "thin" data models by detecting differences and absences between systems and turning them into "thick" data.

This dissertation considers data models as "thin" traces after Geiger and Ribes [-@geigerTraceEthnographyFollowing2011]. This thinness refers to data models not conveying much information as relatively standardized schemas made of categories and values. However, the use of data models in various infrastructures can be a great place to start when examining how they connect geographically dispersed sites similarly to standards [see also, @latourReassemblingSocialIntroduction2005]. That is why it is crucial to consider transforming these "thin" data into "thick" data when developing an analysis strategy. For instance, @burnsWhereDatabaseDigital2020 developed a similar approach called "database ethnography" to use traces left behind by databases as sites to examine shifts in the social meanings of phenomena over time. Although researchers investigating classification have previously used and combined ad hoc approaches, there is now a need for systematic and reusable approaches to retrace the work of seemingly mundane data models.

The conceptual framework developed in this dissertation addresses several aspects of matching people's identity data. The first aspect it addresses is the relationships between different information systems' data models. This is an important consideration because when identity data is collected and stored in different information systems, it is often fragmented and inconsistent, making it challenging to match individuals across systems. Therefore, understanding how various information systems' data models relate to each other is essential for understanding identity matching. Yet, there is a method that wishes to capture the scripts embedded in data models that should proceed by comparing them to detect meaningful differences and absences as information systems define their data models in ways that are not immediately comparable. Chapter 4 will develop such a method for extracting, analyzing, comparing, and visualizing data models from heterogeneous sources.

The conceptual framework developed in this dissertation addresses several aspects of matching people's identity data. One key consideration is the relationships between different information systems' data models. When identity data is collected and stored in multiple systems, it often becomes fragmented and inconsistent, which poses challenges for matching individuals across systems. Yet, information systems often define their data models in ways that are not immediately comparable, making it necessary to have a systematic method for comparing them. Therefore, Chapter 4 will propose a method for extracting, analyzing, comparing, and visualizing data models from heterogeneous sources to detect meaningful differences and absences. Using this method will help us understand how data models represent identity information and how they shape identity matching practices.

#### Data matching within and across organizations

The second pair _(data categories `x`, data values `y`)_ associates data categories with their actual values in databases, which may not always correspond to the data models' expectations. Suppose, for instance, that a database contains first and last name values that are inadvertently switched. Also, it is common for databases to have duplicate entries for the same person. In both instances, identification procedures will have to handle inconsistent data. Also, these data problems could be addressed with the help of new technologies. For example, in many administrative procedures, such as naturalization or residency applications, it is standard practice to use some technology to help establish or verify a client's identity at various points. Using the second inversion strategy, we will examine routine data practices for identifying people and the use of data matching technology, such as when bureaucrats need to identify applicants in bureaucratic migration procedures based on ambiguous personal data.

The third pair _(data models `z`, data values `y`)_ is concerned with data matching across organizations and agencies. In the databases and computer systems of various businesses and governmental organizations, identity data records referring to the same person may exist. In this context, data matching refers to data practices related to identifying and potentially linking or merging such identity data spread across multiple databases. A significant challenge for data matching techniques is that unique identifiers are only sometimes available; thus, matching relies on personal information that is only sometimes complete or accurate. For example, a woman may have used her married surname in one system but her maiden name in another after marriage. Additionally, typos and other variations are common in personal data, like names and dates of birth. To understand how people are identified across data infrastructures, we will also use the second infrastructural inversion strategy to bring such less-known data practices and technologies to light.

We can extract at least three essential data matching practices to focus on from technical literature: batch data matching, real-time data matching, and data deduplication [e.g., @christenDataMatchingConcepts2012]. Batch data matching involves an offline batch processing of prepared data collections to find matching data entities. For example, an organization may enhance the data about individuals in their database by matching and integrating data from different sources. In comparison, real-time data matching addresses the problem of immediately retrieving data via direct search queries. For example, police officers may need to query databases using the data categories "name", "nationality", and "date of birth" to see if there are any approximate matches for those personal details. Generally, data matching will find almost identical identity data that experts need to evaluate rather than finding only exact matches. Lastly, data deduplication employs data matching technology to identify and combine multiple records in a database that, for instance, refer to the same person.

Empirically, the research focuses on these data matching practices by analyzing the data matching practices and technologies for identifying people throughout the data infrastructure of the Netherlands' immigration and naturalization service. On the one hand, the analysis will focus on how this government service identifies applicants and deals with data issues. On the other hand, the research expands to highlight how applicants are identified throughout the broader Netherlands immigration data infrastructure. By focusing on these data practices, we will see how the identification at the agency is closely connected to a software package for matching identity data. Chapter 5 will introduce the concept of "re-identification" to capture the processes by which applicants of bureaucratic procedures are re-identified throughout data infrastructures.

#### Travelling data matching software and expertise

Another issue when considering data matching within and across organizations is how data matching expertise and technology spread over time across multiple organizations. The underlying premise is that businesses develop and deploy tools to aid customers' data matching efforts in various locations and times. In addition, expertise in related areas, such as, for example, comparing identity records containing typographical errors and name variants, may similarly circulate over time from one institution to another. As a result, this axis combination employs the third infrastructural inversion strategy to focus on sociotechnical change to make visible how knowledge and technology for matching identity data travels.

According to the literature on the relationship between technology design and use, the dynamics and evolution of data matching can become visible in examining moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. For instance, the "Biography of Artefacts and Practices" (BOAP) method describes the intricate process of creating such technologies and related practices over time. In accordance with the BOAP methodology, the emphasis of this aspect of the conceptual framework is to comprehend the history of data matching software packages. This approach can highlight the diverse actors involved in developing such security technology and how the software travels and evolves over time and between different sites. BOAP goes beyond the narrow focus of the user-design-technology nexus used in the second axis to examine how technology has changed over time and by a broader range of actors at different sites.

Practically, Chapter 6 will propose two heuristics for selecting crucial moments in the lifecycle of identification technologies.
The first heuristic uses the Social Construction of Technology's concept of “interpretative flexibility” to pick out significant moments when social groups challenge, change or close down the meanings of identification practices and technologies. The second heuristic employs the infrastructure studies' concept of “gateway technologies” to pick out moments when heterogeneous identification software systems and infrastructures intersect. The combination of methods is thus used to understand the long-term development of identification systems and infrastructures.

### Fieldwork identification

#### Software package for matching identity data

The next step of the conceptual framework is to situate and position the dissertation research project and the researcher in relation to, on the one hand, case- and site-specific elements. On the other hand, the conceptual framework links up with the research goals of the overarching Processing Citizenship project. The following description is of the fieldwork site selection for examining data matching within and between organizations and the travelling data matching software. Following that, a description of how the systems' data models were chosen for comparison with authorities' data models will be provided.

The dissertation project is linked to Processing Citizenship's goal to examine how identity management systems support the creation of knowledge about non-European populations [@pelizzaProcessingAlterityEnacting2019; @pcProcessingCitizenshipDigital2017]. To gain a better understanding of knowledge production processes related to identity and security, one useful approach is to examine the database software and technology used by EU and Member State government agencies, as well as international organizations involved in these issues. Investigating the development and implementation of such software can provide insights into the challenges associated with cross-organizational data matching and identity management in security settings. By studying the specific features and functions of these systems, it becomes possible to identify the ways in which they shape and are shaped by data practices and infrastructures, and how they can impact the identification and tracking of individuals.

The first, unfortunately unsuccessful, attempt at opening up a potential fieldwork site concentrated on the problems and technological solutions EU institutions face with identity data in border security and migration management. Establishing contacts with eu-LISA, the European Commission agency managing several EU border management information systems was possible. With the support of the PC project' Principal Investigator, it was possible to participate in the 2018 annual EU-LISA conference and meet with the head officer for research and development to discuss a potential traineeship on-site at the agency's office. After an initial agreement, I applied for a traineeship with a proof of concept proposal based on probabilistic databases, an active area of research that uses a database model based on working with uncertain data [@keulenProbabilisticDataIntegration2018]. Such a proof of concept was thought to potentially give insight into data uncertainties that the EU and MS encounter. Unfortunately, the proposal had to be abandoned by August 2019 due to issues surrounding the confidentiality of the data that the research would collect. As such, the study faced common barriers to qualitative secrecy research; the proposal could not pass the “gatekeepers” that could permit access to the fieldwork site [@degoedeSecrecyMethodsSecurity2020].

The second attempt at opening up a potential fieldwork site was successful by focusing instead on a more low-key supplier of identity-matching software. By looking at other technology companies that work with eu-LISA, the research pinpointed the company WCC Group, which develops technology for matching identity data in border security and migration management. Indeed, the EU Visa Information System uses WCC's software, the ELISE ID platform, to search and match identity and visa data. In short, ELISE provides fast and advanced data matching, such as querying based on inexact data. The novelty of the technology comes from using various kinds of fuzzy logic algorithms for data matching that consider that data may be incomplete or inaccurate. The term “fuzzy logic” in this context refers to a type of mathematical logic that computes truth variables using probabilities rather than boolean “true” or “false” values. The company and its software were thus deemed as excellent opportunities to empirically observe the problems and solutions of matching and linking identity data in security settings. Indeed, next to using ELISE in VIS, WCC has various other customers who use the software in border security and migration management settings.

A meeting was set up in December 2019 with WCC at the company's headquarters in Utrecht to discuss a research project that would meet the needs of the Processing Citizenship and the PhD project's research and live up to WCC and its clients' expectations. Sometime after, we proposed a project for on-site research focusing on the deployments of ELISE in the Visa Information System and the Netherlands' immigration and naturalization government agency, which WCC later approved. Unfortunately, at the same time, the Covid-19 virus spread worldwide and became a global pandemic. As a result, the government of The Netherlands announced in March 2020 that people should work from home. We decided against starting remotely and instead pushed back the start date because we believed the research would benefit more from face-to-face interaction. In May and June of 2020, the government of the Netherlands loosened up some restrictions on working on-site and taking public transportation. Therefore, we decided to start the fieldwork in a hybrid way, with remote access to relevant resources and a few on-site visits to the Utrecht office when the staff members were present. The summer months provided a more relaxed environment, which meant fewer people were present, and it coincided with holiday schedules. However, by September 2020, the government had once more tightened Covid-related regulations, making it necessary to conduct subsequent fieldwork online. A more in-depth discussion will be found in later sections and chapters.

#### Data matching within and across the organizations

We can map the fieldwork choices onto the conceptual framework and infrastructural inversion strategies. Initially, an emphasis was placed on data practices to comprehend how the ELISE identity-matching software influences organizations' identification processes and vice versa. I expected that the embedded assumptions in the tools regarding the uncertainty of identity data influence, to a certain extent, work practices regarding the identification of individuals. Due to the company's limited knowledge of how customers were using ELISE, this focus on data practices related to identifying people was expected to be of interest to both the researcher and WCC, who could use this knowledge to improve the software's user experience. The research study documented the results of analyzing a customer's process of matching identity data, which were presented in an online company webinar and compiled into a report. For instance, the report highlighted an issue with identifying whether someone already exists in the system when creating an application, which the company was examining to explore possible improvements. Therefore, the study's insights and recommendations were valuable to varying degrees for the researcher, company, and customer.

During the fieldwork, I examined users' everyday data practices to learn how they use biographic matching and multicultural name-matching tools. Additionally, comparing the software's design and actual use was helpful for understanding the challenges of identifying people from ambiguous data in relation to the technology's affordances. As a result, the study looked into data practices related to the ELISE software. Indeed, as we will see in Chapter 5, tensions in the imagined and actual use of the search and match functionalities were evidence of difficulties with ambiguous identity data. Therefore, the first goal of the fieldwork was to examine data practices and answer the second research question, which is how organizations that collect information about people-on-the-move search for and match identity data in their systems, as well as how data about people-on-the-move is matched and linked across different agencies and organizations.

The second fieldwork goal was to answer the third research question, or how knowledge and technology for matching identity data circulate and travel across organizations. The research identified pivotal moments in developing and deploying the software package. For example, the fieldwork research investigated system builders’ work to make identity data matching work across different contexts and organizations. Looking at sociotechnical change revealed the changing interpretative flexibility of the software and some closure mechanisms leading to the current data matching solutions. Furthermore, investigating how the ELISE software connects disparate systems made it possible to conceptualize it as a gateway, a crucial technology in infrastructure building. In this way, it was possible to understand technological development as a complex and dynamic process that involves numerous actors across time and locales. Therefore this goal corresponds to the conceptual framework’s aspect of knowledge and technologies for matching identity data travel and circulate and uses sociotechnical change as an infrastructural inversion strategy.

#### Deployments of ELISE at the IND and EU-VIS

The initial focus of the conceptual framework was on ELISE software deployments at two WCC customers of WCCs, one in a national setting and the other in a transnational setting. First, the use of ELISE in national settings was investigated through the software's integration with the systems of the Netherlands' immigration and naturalization service (IND). Second, the use of ELISE in transnational settings was investigated through the software's integration into the EU Visa Information System (VIS). However, we should be careful in distinguishing between the national and the transnational. The social research methods employed for this inquiry not only describe realities but are a part of the “ontological politics” of enacting certain kinds of realities [@lawEnactingSocial2004]. We should therefore be careful to avoid reproducing fixed distinctions between national, transnational, or international. Instead, following Law and Urry, the question is, on the one hand, to which realities we want to contribute. On the other hand, researching data infrastructures can reveal the enactment of social orders [@pelizzaProcessingAlterityEnacting2019].

The ELISE search and match solution's use in those two contexts was deemed relevant to understand how the tools shape the practices of knowledge production related to linking and matching identity data. In the case of the IND, the agency uses ELISE throughout its bureaucratic processing of applications from people who want to stay in The Netherlands or become Dutch nationals. The agency's information system thus leverages the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system. In the case of the EU VIS system, ELISE has a narrower scope in its use of biographic matching and multicultural name-matching capabilities for its search engine. At the same time, due to its transnational dimensions, the amount of records in the EU-VIS database is much higher.[^eu-vis-capacity] As we will see in Chapter 6, there are some connections between the deployment of ELISE in these two systems. Therefore, examining the data practices and sociotechnical change related to the ELISE deployments in these two settings was deemed suitable to make the matching and linking of identity data in transnational security infrastructures visible.

[^eu-vis-capacity]: The “Report on the technical functioning of the Visa Information System (VIS)” [@eu-lisaReportTechnicalFunction2020] notes a database capacity of 60 million records.

The methodological choice of tracing the development and deployment of the ELISE software package for these two systems necessarily has some “framing effects” [@hyysaloMethodMattersSocial2019], which refers to how researchers' choices about their study's research design have consequences for how their studies frame the technology under investigation. Although WCC's clientele includes both public and private entities from various sectors (such as employment agencies), the "Identity and security" market segment was singled out for analysis in this dissertation's framing. Furthermore, not all identity and security customers were possible to research due to issues such as confidentiality. At first, WCC's work in other sectors beyond identity and security seemed unimportant for the research. However, links with customers from other sectors repeatedly emerged during the fieldwork and interviews in discussions about the genericness of the search and match. This finding will be important in Chapter 6, which investigates the origins of the search and match software's ability to be applied in various contexts. Chapter 6 will also show how other actors, directly and indirectly, influence the company and its products beyond the research's initial framing, such as research centres, competitors and potential customers. The investigation gradually introduced these ancillary actors by attending industry events and relying on other sources, as detailed in the subsections \@ref(events) and \@ref(other-observations).

#### Choice of authorities for comparing data models

I compared data models of EU and Member State authorities. First, three important information systems developed by European Commission agencies for border security and migration management can represent the EU as one authority. These systems are Eurodac, the Schengen Information System (SIS), and the Visa Information System (VIS). All three systems use different data models to support policing tasks involving travel, cross-border crime, or irregular migration. Eurodac (_European Dactyloscopy_) aims to support the identification of asylum seekers through fingerprints and to ascertain the Member State responsible for processing their asylum applications within the context of the Dublin System.[^dublin-iii] The database was created in 2003 to store asylum seekers' fingerprints and other basic information. It is currently operational in the Member States of the European Union (plus Norway, Iceland, Switzerland, and Liechtenstein). Law enforcement agencies and Europol also use the system, which has significantly expanded its original scope since its inception [@ajanaAsylumIdentityManagement2013]. New proposals aim to include more biographic and biometric data, including a facial image. [^recast]

[^dublin-iii]: The Dublin System (Regulation No. 604/2013; also known as the Dublin III Regulation) establishes the criteria and mechanisms for determining which EU Member State is responsible for examining an asylum application.

[^recast]: Procedure 2016/0132/COD, a recast of the Eurodac Regulation.

The Schengen Information System (SIS II) aims to support external border control and law enforcement cooperation in the European Union. It supports this task by storing alerts which contain information on persons and objects, and instructions on what to do when officers encounter such persons or objects. In addition, the SIRENE network can exchange this information between law and border enforcement authorities. Finally, the Visa Information System (VIS) allows exchanging of visa data (including personal and biometric data). In this way, it aims to maintain a common EU visa policy. Authorities use VIS data in the context of border identification procedures. Access is thus different from the Eurodac and SISII systems, as authorities can only access VIS data on a case-by-case basis. Officers at the external border of the European Union also have access for purposes such as “verifying the identity of the person, the authenticity of the visa or whether the person meets the requirements for entering, staying in or residing within the national territories” [@europeancommissionVisaInformationSystem]. Furthermore, asylum authorities use VIS data to determine which EU Member State is responsible for examining the asylum application.

Concerning national systems, the analysis includes the systems of the Hellenic and German Register of Foreigners. These systems were chosen as part of the broader Processing Citizenship's task plan of examining actual identification practices in "processing alterity" [@pelizzaProcessingAlterityEnacting2019]. For this reason, it is necessary to view the analysis of data models within the context of the PC project's task of investigating how national and European authorities formalize knowledge about individuals in data models used for managing migration. The PC project's script analysis methodology thus aimed to produce a typology of “intended migrants”, which is the formalized knowledge of migrant identities inscribed in information systems [@pelizzaScriptsAlterityMappingunderreview]. On the one hand, comparing data models aided this dissertation's research in further understanding the authorities' capabilities and restrictions in linking and matching identity data. On the other hand, the analysis could be helpful to ground PC researchers' analysis of actual practices at the border by migrants and officers [@olivieriTemporalitiesMigrationTime2023].

The Hellenic Register of Foreigners is the primary system used at border zones in Greece to identify and register persons who arrive at the border without the required documents. In addition, officials use the system to support various tasks during the identification and asylum procedures, such as retrieving migrants' biographic and biometric data, conducting screening and asylum interviews, and assessing health conditions. Therefore, the systems' users include police, administrative personnel, and asylum officers.

The German Register of Foreigners (GRF) is a system containing a large amount of personal information of foreigners in Germany who have or had a residence permit, as well as those who seek or have sought asylum or are recognized asylum seekers [@bvaAuslanderzentralregister]. Various partner authorities and organizations in asylum, migration, and border control access this central register. The XAusländer standard, a data exchange format which formalizes and enables data exchange between the immigration authorities in Germany, describes the data sent to the GRF during the first registration [@bamfStandardXAuslaender]. According to the description of the standard’s motivation, it aims to facilitate the exchange of such data between authorities in Germany to reduce data re-entry and enable the reuse of such data by the authorities. However, all the authorities' information systems described above define their data models and the kinds of people they aim to represent in ways that are not immediately comparable due to formatting differences. The next chapter will thus introduce a digital method called "The Ontology Explorer" used to extract, analyze, compare and visualize those diverse data models.

This section outlined how this dissertation makes data matching in specific transnational security data infrastructures visible by combining three infrastructural inversion strategies into a conceptual framework and operationalizing it. First, the ability and limitations of the data models to link and match identity data are compared, along with the authorities' knowledge of individuals in the EU and MS. Second, data practices can demonstrate how identity information is linked and matched within and across organizations. Third, the dissemination of data matching knowledge and tools is examined using a social constructivist understanding of sociotechnical change. The following section details the types of data collected through field and desk research and the various methods used to analyze this data.

## Methods for data collection

The conceptual framework guided the analysis of data matching in global security data infrastructures. Additionally, various techniques were used to gather and analyze qualitative data to support the investigation, including document analysis and user interviews. Naturally, these methods informed each other at various points throughout the research.

### Traces of data models

To investigate the matching of identity data across organizations, it is useful to understand how the data models of different systems correspond. Records referring to the same person can be challenging to identify without knowledge of the underlying data models of the datasets being matched, as the same person may be represented differently across different datasets. The first step in data matching typically involves finding correspondences and similarities in the types of information stored in the respective databases. For instance, two systems may hold a combination of a person's last name, date of birth, and nationality. However, to compare these individual identifiers, it is necessary to deal with the different data models that each has their labeling, types, and formats of attributes. These data models and their data categories, which describe a state that can assume different values, are typically specified in various documents such as database schemas, design documents, or regulations. Thus, investigating the development and implementation of database software used in identity and security-related settings can provide valuable insights into cross-organizational data matching and identity management.

Because data models are defined in various formats, some of which may not always be accessible, such as confidential technical database specifications, the term "traces" is used in this dissertation. This broader term is utilized to encompass other forms of data model description, such as screenshots of a graphical user interface. These traces were then used to reconstruct the data models, allowing for a comprehensive comparison of the data models used by the different authorities. In general, desk research or fieldwork enabled the collection of such data model traces. Chapter 4 details the method to compare diverse descriptions of data models through additional steps to code, harmonize, and group all documents, categories, and values.

Regarding the traces of those models, the research draws on data collected during fieldwork conducted in the context of the Processing Citizenship project at border zones in Europe. In addition, given linguistic constraints and the PC project’s task plan organized as a matrix, some documents were collected by other researchers employed as collaborators in the Processing Citizenship project.[^pc-team] Overall, the data collection efforts included desk research of European regulations over Eurodac, VIS, and SISII, technical documents made available by European and German authorities, systems screenshots collected at border zones in the Hellenic Republic, interviews and ethnographic observation with IT developers and users in Italy and Greece, and technical documents collected during fieldwork in The Netherlands.

[^pc-team]: Over the years, PC researchers have been: A. Bacchi, E. Frezouli, Y. Lausberg, C. Loschi, L. Olivieri, A. Pelizza, A. Pettrachin, S. Scheel, P. Trauttmansdorff.

### Documents and documentation practices

The document analysis related to the deployment of ELISE is grounded by the STS understanding of documents and practices of documenting as research objects. However, the extensive literature review of @shankarRethinkingDocuments2017 shows that various STS scholars have broadened our conception of documents as objects that do more than just record information. Instead, scholars remarked on how documents play an active role in social contexts, such as accounting for and coordinating workplace activities. Documents, in this sense, are inseparable from the processes that generate them [@hullDocumentsBureaucracy2012; @rilesDocumentsArtifactsModern2006]. Therefore, the dissertation looked not only at documents to help explain how the software package functions. Instead, documents help understand the organizations as their records "refer to the practices, objects, rules, knowledge, and organizational forms that produced them" [ibid., p. 62]. In this way, the findings from the document analysis mainly contributed to an understanding of the biographical moments in the life of this software package.

Several on-site visits to WCC's headquarters in Utrecht, the Netherlands, made it possible to consult ELISE-related documents such as technical design documents, product brochures, and meeting minutes. On the one hand, documentation related to the more general technical particulars of the ELISE ID platform made it possible to become acquainted with the software suite and how WCC designed it. On the other hand, documents related to the platform's specific implementations at the IND made it possible to understand how WCC practically configures the software package for customers. In addition, WCC's “ID team” provided additional context for the documents and updates on the status of ELISE integration in the EU-VIS and INDiGO systems.

The document analysis had two broad aims, corresponding to two aspects of the conceptual framework. The first goal was to compare how different IND users use WCC ELISE's biographic matching capabilities in their daily identity searches to find friction between expected use and actual data practices. These software expectations may be the source of tensions in the imagined and actual use of the search and match functionalities. In other words, we can comprehend how IND staff members use the software, shaping expectations regarding identification and the degree of uncertainty surrounding identity data in the process. Chapter 5 shows how the designed probabilistic identity match shapes the user’s understanding of, for example, the search results.

The second aim of the document analysis was to unearth moments in the trajectory of the software package (i.e., the third inversion). Fundamental insights from scholars in this regard suggest that — instead of looking at single document versions — we should pay attention to the documenting practices [@shankarRethinkingDocuments2017]. For example, we can retrace documents' histories by looking at different document versions, such as changes within documents or added annotations. For example, the group of documents related to the deployment of ELISE at the IND included different versions of the design document, documents that described updates to the package, and meeting notes about changes to the configuration. However, most collected documents lacked a clear context. As a result, determining the motivations or source of a document can be challenging. For instance, the IND project documentation that WCC kept was an archive dating back more than ten years, and the staff members who worked on it were no longer with the company. As a result, recovering the context of some of these documents and their origins and motivations turned out complex, even for current WCC employees.

Rather than seeing individual authors, we should consider documents as assemblages of different authors and sources [see also @latourReassemblingSocialIntroduction2005]. That is why documents help understand how knowledge and technologies for matching identity data travel and circulate. Documents often lack a clear origin and are instead “assembled from multiple sources [where] content often flows from application to application and document to document, constantly recycled, reworked, and repackaged” [@shankarRethinkingDocuments2017, p. 63]. In the ELISE documentation for the IND, such flows were readily identifiable. For example, technical documents include IND-specific implementation details alongside package-specific information from other company documents. Thus, documents and documenting practices can help answer how knowledge and technology for matching identity data circulates and travels across organizations (RQ3). The software's history, including its distribution, modification, and adoption by different groups, is documented in various forms throughout the project's lifecycle.

The absence or inaccessibility of documents is a notable, albeit less visible, aspect of documentation. For example, it became clear during fieldwork that records for the deployment of ELISE in the EU Visa Information System would be inaccessible. The specific arrangement of actors involved in developing the EU-VIS systems resulted in this lack of access. In this arrangement, WCC was the technology supplier and collaborated with Accenture, the technology integrator. This arrangement meant that the relevant documents were in the hands of the technology integrator. Moreover, even for WCC, the technology integrator acts as a gatekeeper, deterring access to such materials. Nevertheless, the absence or inaccessibility of documentation can still provide insights into the work of actors building these systems.

In short, the document analysis made it possible to form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents give insights into the package's genealogy and the practices of configuring, deploying, and designing the software. Table \@ref(tab:documents) overviews the documents consulted. In addition to the more technical documents, public communications and reports gave valuable insights into the particular context of the IND information system developments. The findings from the document review were used to inform the structure of the interviews.

```{r documents, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "documents.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data, header = TRUE, properties = pt)

# pander(dt)

# kbl(data)

# kbl(dt, booktabs = T) %>%
  # kable_styling(latex_options = c("striped", "scale_down"))

# block_table(data[,c("Document", "Author", "Year", "Description")], header = TRUE, properties = pt)
# ft <- flextable(data)
# ft <- add_header_row(ft,
#   colwidths = c(4, 2),
#   values = c("Document", "Author", "Year", "Description")
# )
# ft <- autofit(ft)
# ft <- theme_vanilla(ft)
# ft <- set_caption(ft, caption = "Documents")
# ft
```

### Interviews {#interviews}

Semi-structured interviews with a diverse range of actors provided additional insight into the design and implementation of the search and match tools. The interview strategy centred on two main themes and participant groups. The first group involved IND employees whose duties included looking up and matching identities using the ELISE search and match engine. The second set of interviewees were WCC employees with responsibilities related to the company’s identity matching software package, including design, implementation, and (pre-)sales.

More specifically, the interview approach focused on individual interviews, allowing more time and possibilities to discuss topics in detail. Individual user interviews were the easiest to set up and provided more detail of personal experiences. However, opportunities to observe group dynamics and discussion were missing compared to other approaches. The interview protocol included questions and probes for follow-up based on the research questions and insights from the document analysis. The Appendix contains a sample of the interview questions.

Due to unfamiliarity with the IND and the necessity of conducting research digitally, interviewees were only available after considerable time and effort. WCC's “ID Team” members helped set up the first contact with the IND organization. The search for IND employees willing to participate in the study using respondent-driven sampling grew after the initial contact. Email invitations were sent to the gathered contacts inviting them to participate in an online interview. The small-scale snowball method of WCC and IND staff networks did allow for the inclusion of participants who were unknown before the start of the research. However, there was little control over the sample size of respondents. It is also worth noting that WCC had only one point of contact within the IND. Dependence on these personal networks led to a bias in who was included, in this case, more senior members of the IND organization.

The Covid-19 pandemic made it tough to have in-person interviews, so much of the research was conducted online. Measures at the time restricted people's options to go to their place of work, so most IND and WCC staff were working from home, which limited the interview format to video and phone calls. These telephone and online conferences had the advantage that scheduling the interviews was more straightforward because meetings did not involve travel. On the other hand, not all communication may have come across the same way, which may have made it harder to network and schedule more interviews. The interviews used the WCC company's secure Microsoft Teams installation for meeting online. However, not all IND employees who worked remotely had the authority to install the software on their company's laptops. Participants' IT access limitations, therefore, necessitated phone interviews in some cases.

The study hypothesized that users performing different tasks at the IND would use the tools for searching and matching identities differently. For this reason, the participants included members of the IND's various organizational units (for a list of these units, see Table \@ref(tab:ind-departments)). As we will see in Chapter 5, the interviews revealed vast differences in tool use and familiarity. It was, unfortunately, impossible to interview users from each unit. Fortunately, details about utilizing the search and match tools in the other departments did surface during the interviews. Participants described using the search and match tools at the IND in five interviews, each lasting approximately an hour. Table \@ref(tab:ind-departments) summarizes the various IND units and whether or not the members from the respective units were interviewed.

```{r ind-departments, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "ind-departments.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Org", "Unit", "Incl", "Description")], header = TRUE, properties = pt)
```

The research also included interviews with people from the company WCC, which took place concurrently with and after the IND interviews. These interviews followed a looser format than the ones conducted with IND employees. The interviews aimed to illuminate events in the history of their identity-matching software system. The participant's role, projects, and company experiences thus influenced the interview questions and probes. For example, the interview included questions about system development and deployment for people who worked on the IND or EU-VIS projects. This structure also allowed asking people with different profiles about their connections with current and potential customers in the security and identity market. In addition, a few WCC employees present during the fieldwork were invited to participate in an online interview. On the other hand, respondent-driven sampling expanded the number of participants. Based on their profiles, we can divide these participants into two clusters. The first cluster comprises WCC's “ID Team” members who hold consultant, pre-sales, and solutions manager positions. The second group consisted of the more technically minded; among them were a senior software developer and a user experience designer. The Covid-19 pandemic also made online interviews necessary in this case. Participants described their knowledge of building and deploying the company's software in seven interviews, each lasting approximately an hour.

Participants consented to record the sessions using Processing Citizenship's informed consent form. The form allowed participants to specify how the research would use their provided data while guaranteeing anonymity and confidentiality. For example, per protocol, the recording only included audio with a distorted voice for added anonymity. Furthermore, manual transcription of interviews ensured additional confidentiality by preventing confidential information from being leaked via automated transcription platforms. Table \@ref(tab:interviews) provides an overview of the interviews.

```{r interviews, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "interviews.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Nr", "Org", "Date", "Channel", "Language", "Description")], header = TRUE, properties = pt)
```

### Events {#events}

The analysis also relied on relevant publications from the grey literature and field notes from events where industry representatives, academics, Member State authorities, and EU agencies were all present. The events I attended, in person or online, are summarized and described in the table \@ref(tab:fieldwork-events). Participating in these gatherings aimed to gain insight into how different stakeholders in the industry define and frame the issues surrounding the quality of personal data used in their systems, as well as potential solutions for linking and matching such data. The study's premise was, on the one hand, that presenters and attendees would use real-world examples to depict the challenges they face in their work. For instance, at the 2018 eu-LISA conference, an audience member asked a panellist how they planned to handle the potentially high volume of false positives generated by integrating disparate databases, some of which may contain outdated information. One of the panellists responded that checking all those false positives would take time. This example demonstrates the value of attending such events, as they can provide insights into the often-invisible effort required to integrate various systems successfully.

Another assumption behind the study was that recommendations from security and identification experts would show people's and organizations' ideas about connecting and using data from different sources. Although not central to the dissertation, this aspect relates to the STS literature on “sociotechnical imaginaries”, which developed this concept to investigate how societal expectations of the future, sociotechnological projects, and social order are shaped by one another [@jasanoffSociotechnicalImaginariesNational2013]. For example, in Chapter 5, we come across a scenario that security companies use to illustrate why it is vital to have methods for searching and matching data stored in various databases by invoking the spectre of terrorism. In one presentation, a software company used the real-life example of authorities adding one of the “Boston bombers” to police watch list databases before the attack, but with different and invalid transliteration variations of his name. According to those security professionals, disparate information across databases makes it harder for officers to conduct investigations and may result in blind spots. Correspondingly, @trauttmansdorffInfrastructuralExperimentationCollective2021 demonstrated that such sociotechnical imaginaries enable or disable the development of specific forms of large-scale border security infrastructure.

```{r fieldwork-events, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "fieldwork-events.csv")
data = read.csv(path, header = TRUE)

block_table(data, header = TRUE, properties = pt)
```

### Other observations {#other-observations}

Other observations used in the analysis came from news articles, press releases, and other publications. These publications were collected to give more context and help understand moments in the history of the WCC search and match tool and its use at the IND and EU-VIS. The LexisNexis database and tool were used to search for articles that mentioned the company WCC and its software in the INDIGO system of the IND or the EU-VIS system. LexisNexis allows users to search for articles in newspapers, online business publications, and other sources written in English and Dutch. For example, several articles published in Dutch IT business news websites provided valuable information on IND's INDiGO system. This database also uncovered press releases concerning the WCC's participation in the MITRE multicultural name matching challenge (discussed in Chapter 6).

The document corpus also includes blog posts, and business press releases that disseminate information about the systems' creation, development, deployment, and utilization. These additional publications are discoverable via conventional search engines. For example, blog posts on WCC's and competitors' websites showed how experts in the field of name-matching devise and disseminate their knowledge. Take, for instance, the articles "Understanding Dari and Pashto names: a challenge to intelligence gathering in Afghanistan" [@scheersBiographicMatchingUMF2021] and "Biographic matching & UMF standards for EU interoperability" [@btcUnderstandingDariPashto2012]. Such publications give essential insights for Chapter 6's SCOT analysis on how companies, as relevant social groups, define problems and solutions of identification. A selection of these publications dealing with data quality, data matching, or other relevant discussions surrounding the operation of the ELISE, EU-VIS, and INDiGO systems was further coded and analyzed.

These various other publications (news articles, blog posts, and press releases) contribute to a more complete picture of the history of this identity data searching and matching software.

## Techniques of data analysis

Two distinct methods distinguish the data analysis of the variously collected data. This distinction stems from two aspects of the conceptual framework discussed above: the relationships between different data models and data matching within and across organizations. First, for Chapter 4, analyzing information systems' data models necessitated the development of a new method and a tool. Second, for Chapters 5 and 6, analyzing the other field notes, interview transcripts, and technical documents used well-known methods for coding and analyzing qualitative data.

Chapter 4 details the methodology for comparing data models named "The Ontology Explorer". The Ontology Explorer (OE) is a semantic method and JavaScript-based open-source tool to compare the data models collected in different formats and from diverse systems in two ways. First, it supports analyses of information systems which define their data models, even if these systems are only sometimes comparable. Second, it systematically and quantitatively enables discursive analysis of “thin” data models by detecting differences and absences between systems. The method extracts, analyses, compares and visualizes heterogeneous data models to achieve this. Applying this structured approach to the data models used by diverse organizations eventually made it possible to observe differences and similarities in the data models of diverse data infrastructures.

In this way, the OE analysis addresses the first aspect of matching identity data: the correspondence between data models of different EU and Member State systems. That is, how different data models for collecting information about people-on-the-move are situated in relation to one another and what this tells us about the organizations which developed and use these models to collect, search, and match identities. Data models of EU and Member States’ systems were analyzed using the methodology and tool of the ontology explorer. The results from this analysis showed that, for example, there are differences and similarities between all data collected in the systems and those used for searching and matching identities. Furthermore, these differences and similarities between authorities’ data models can show how knowledge of people circulates and shows divisions of responsibilities between actors [@pelizzaScriptsAlterityMappingunderreview]. A typology of data developed using the OE includes shared data categories necessary for searching and matching identities. In contrast, other distinctive categories give evidence of distinct responsibilities to those systems and actors.

Other qualitative fieldwork data analysis followed standard computer-assisted qualitative data analysis methods. As such, the data analysis relied on the software _ATLAS.ti_. Cleaned-up field notes and transcribed interviews were imported into the software. The data coding and analysis drew inspiration from the three interconnected steps of the "Noticing-Collecting-Thinking" (NCT) method by @frieseQualitativeDataAnalysis2014, which follows a standard qualitative data analysis but is tailored for the ATLAS.ti software package. First, labels were assigned to segments in the documents, i.e., codes, in the Noticing step. The research questions guided this coding step, but the coding was simultaneously open to inductive findings from the data. Second, those codes were reviewed and gathered into similar codes in the Collecting step. Third, among the developed codes, patterns, processes, and typologies were found in the Thinking step. Of course, in practice, the whole process proceeds by moving back and forth between the different steps of noticing, collecting and thinking. Figure \@ref(fig:data-analysis) summarizes this recursive data collection process and application of the NCT steps.

```{r data-analysis, fig.cap="Data analysis."}
knitr::include_graphics("figures/data-analysis.pdf")
```

Take, for example, the typologies developed based on analyzing data collected during fieldwork and from interviews with practitioners using the ELISE software to search and match data at the Netherlands' asylum and naturalization agency. Chapter 5 employs these typologies to develop the concept of "re-identification". For instance, a non-exhaustive typology of potential data frictions in alphanumeric identity data included ambiguity and incommensurability in transliterations, variations in identification policies, and frictions brought on by human errors during data entry. As a result, it was possible to theorize how algorithms for dealing with incomplete, inaccurate, and unreliable identity data deal with the problem of re-identifying people throughout the agency's bureaucratic procedures. Analyzing the findings of ELISE's implementation at the agency thus revealed how fictions about handling data uncertainties influence daily (re-)identification practices.

These examples demonstrate how, by drawing from various sources, it is possible to trace multiple aspects of data matching practices and the design and use of data matching technologies. The following chapter tackles the first aspect of the conceptual model to analyze authorities’ imaginaries of populations and the scripts through which they enact actual people. Then, the chapter will describe the development of a new method and tool that relies on the first infrastructural inversion strategy by comparing data models in the information systems of both EU and Member State authorities.
