# Towards a methodological framework for analyzing data matching in transnational infrastructures {#ch-method}

\chaptermark{Research design and methods}

This chapter aims to develop a methodological framework for analyzing data matching in transnational security infrastructures, thereby exploring the intertwined nature of internationalization, commercialization, security and infrastructure development in the realm of identification and their relationship with identity data matching. By uncovering the technical details involved in matching and linking identity data, the framework seeks to enhance our understanding of these processes. To achieve this, three distinct methodological strategies are introduced, utilizing data infrastructures as both a research topic and resource: 1) comparing data models, 2) analyzing data practices, and 3) tracing sociotechnical change. These strategies, influenced by the work on "infrastructural inversions" by @bowkerScienceRunInformation1994 and @bowkerSortingThingsOut1999, help to invert the tendency of infrastructure, associated practices, and tools to fade into the background when everything runs smoothly. Through comparing data models, we can gain insights into the information collected by various organizations and systems. Analyzing data practices allows us to understand the processes of searching and matching identity data within and across organizations. By examining sociotechnical change, we can trace the circulation of knowledge, technologies, and practices related to data matching over time and across organizations. The resulting methodological framework combines these three inversion strategies into a visual tool that directs the analysis of identity data matching in transnational security infrastructures. Subsequent chapters will delve into the empirical analysis in more detail.

## Infrastructures as topics and resources of research

What methods and frameworks are best suited to trace identity data matching in border and migration control? In recent years, a growing body of literature has viewed Europe's technological systems of border security and migration control as infrastructures [e.g., @amelungTechnologiesInfrastructuresMigrations2020; @dijstelbloemBordersInfrastructureTechnopolitics2021; @olivieriTemporalitiesMigrationTime2023; @pelizzaProcessingAlterityEnacting2019; @potzschEmergenceIBorderBordering2015; @pollozekInfrastructuringEuropeanMigration2019; @trauttmansdorffInfrastructuralExperimentationCollective2021; @vanrossemOntologyExplorerMethod2022]. For instance, the work of @dijstelbloemBordersInfrastructureTechnopolitics2021 highlights the benefits of conceptualizing borders as infrastructure, enabling the discernment of interconnections between large-scale networks and local contexts, the privileging of specific perspectives, the visibility or invisibility of certain phenomena and movements, and the process of state formation. Moreover, the recognition of sociotechnical infrastructures' involvement in the historical process of state formation through material means is increasing [@misaInventingEuropeTechnology2005a; @schipperInfrastructuralEuropeanismProject2011; @pelizzaTellingMoreComplex2023]. Additionally, this perspective sheds light on the active involvement of non-state actors and the performative effects arising from these infrastructures, as explored by @pelizzaProcessingAlterityEnacting2019. Therefore, exploring suitable methods and frameworks within this infrastructural paradigm can enhance our understanding of identity data matching in the context of border and migration control.

A central idea of this dissertation is that digital infrastructures can serve as both a research topic and a resource in social research [@marresDigitalSociologyReinvention2017; @pelizzaDevelopingVectorialGlance2016; @pelizzaScriptsAlterityMapping2023]. The dual nature of infrastructure allows for two distinct approaches: investigating infrastructures as an independent research topic and utilizing them as tools to explore other areas of social research. This dissertation examines the infrastructures underpinning identity data linking and matching as a standalone subject while exploring their implications for the internationalization, commercialization, and securitization of identification. However, both approaches face the challenge of defining the scope and boundaries of infrastructure, including when and where it begins and ends.

One approach to overcome the challenge of a priori delineating infrastructure is through the ethnographic study of infrastructure [@karastiStudyingInfrastructuringEthnographically2018; @starEthnographyInfrastructure1999]. To this end, the researcher, as an ethnographer, can approach infrastructure in a self-reflexive manner, building it “by every choice the ethnographer makes in selecting, connecting, and bounding the site and via the interactions through which s/he engages with the material artifacts and the people who define the field” [@blombergReflections25Years2013, p. 389]. However, this method places a significant responsibility on the researcher. Alternatively, some methodologies require less responsibility from the researcher. For example, not all agree that researchers necessarily have to be present with the actors. @beaulieuResearchNoteColocation2010 notes that we can rely on various traces of infrastructural mediation without physically co-locating with the actors. This chapter takes cues from both approaches to develop a methodological framework that does not define infrastructures at the outset but instead examines the specific technical intricacies of matching and connecting identity data while studying the practices and actors involved.

There are, therefore, two reasons why it is helpful to investigate the technical aspects of matching and linking identity data in security infrastructures. One reason is that, as discussed in Chapter 1, existing research has yet to thoroughly explore the mechanisms for matching and linking identity information in identification, including how to link identities across different systems and databases. Second, as highlighted by @pelizzaDevelopingVectorialGlance2016, focusing on the “technical minutiae” of such systems' and databases' interoperability can serve as “strategic sites” for uncovering significant “institutional shifts” [see also @bowkerSortingThingsOut1999]. Additionally, studying the matching and linking of identity data can provide insight into how these technical mechanisms impact the relationships between different actors, such as governments and security companies. The following section will discuss the three methodological strategies that form the basis of the methodological framework.

## “Infrastructural inversions” for matching identity data

A well-known result of the overlapping between infrastructures and practices is that infrastructures tend to be taken for granted and “remain as invisible backdrops to social action” [@harveyIntroductionInfrastructuralComplications2016, p. 3]. Studies on infrastructures have consequently provided different methodological strategies to invert this tendency of infrastructures to disappear and to make visible the interconnections between technical minutiae and the politics of knowledge production [e.g., @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Methods proposed by these authors to invert the tendency of infrastructure to disappear include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019]. Likewise, I suggest that by inverting infrastructure [@bowkerScienceRunInformation1994], we can learn more about the interplay between technological systems for identity matching and the internationalization, commercialization, and securitization of identification in transnational security contexts. As indicated by @bowkerSortingThingsOut1999:

> "Infrastructural inversion means recognizing the depths of interde­pendence of technical networks and standards, on the one hand, and the real work of politics and knowledge production on the other. It foregrounds these normally invisible Lilliputian threads and further­ more gives them causal prominence in many areas usually attributed to heroic actors, social movements, or cultural mores." (p. 34)

The quote alludes to an important aspect of infrastructural inversions developed further in their book: that classification schemes and standards are fundamental infrastructural elements that underlie many practices. For example, analyzing database data models is a promising methodological option to understand how identity is linked and matched across infrastructure. Data models are classification systems that aid in identifying individuals. The specific data categories included in these models can determine the technical feasibility of matching and connecting different biographic and biometric information. Based on how the data is classified, some individuals may be more easily identifiable and visible, which could result in differential treatment based on their perceived risk or other factors [see, for example, @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. To better understand the implications of identity data matching on people, it is crucial to critically examine the data models and categories utilized, as they play an essential role in how individuals are represented and enacted.

Data models play a crucial role in organizing information and can shape how individuals and groups are perceived and treated. However, designing these models involves ethical and political choices that may go unnoticed [@bowkerSortingThingsOut1999]. Nevertheless, it is possible to revisit these choices. For example, comparing and contrasting the models used by different organizations and authorities can provide valuable insights into how different actors view and categorize the world around us. As a result, data models are a helpful methodological tool for researchers looking to understand how information is organized and presented, which will be discussed in more detail as the first infrastructural inversion technique below.

### First inversion strategy: Comparing data models

Data models are embedded in digital infrastructures and practices and institute specific ways of knowing and working [@bowkerSortingThingsOut1999; @hineDatabasesScientificInstruments2006; @lamplandStandardsTheirStories2009; @timmermansWorldStandardsNot2010]. These models, designed to represent phenomena, can be regarded as technologies that actively shape knowledge, organizational structures, and practices [@bloomfieldVisionsOrganizationOrganizations1997]. The design process of data models is inherently political, determining how information is presented, what aspects are emphasized, and which elements may be marginalized [@bowkerSortingThingsOut1999]. This is particularly relevant in the context of border and migration management, where data models are used to categorize and identify individuals [@pelizzaScriptsAlterityMapping2023]. For example, a family composition field that assumes a traditional nuclear family structure may only account for spouses and children, while biometric identification methods may prioritize fingerprints over other personal characteristics, such as someone's educational background [@pelizzaIdentificationTranslationArt2021]. However, how individuals are enacted through the interplay between different systems has received comparatively less attention and is a type of inquiry that this thesis will address.

While scholars have extensively examined how database technologies enhance the visibility of individuals at the border by facilitating the collection of diverse biographic and biometric data for population differentiation [e.g., @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015], they have often overlooked the critical role of the connections between data models embedded within multiple systems. As a result, these data models, instrumental in matching and linking data across different systems, remain relatively invisible within the infrastructures that govern mobility, migration, and border control. Consequently, there has been limited exploration of the interconnections between data models and their impact on the enactment of individuals across diverse systems. This dissertation aims to address this oversight by shedding light on the significance of data models in infrastructure, tracing their representation, and elucidating how they embody specific forms of knowledge.

An inherent challenge in analyzing data models is their "thinness." Data models provide little information on their own because their meaning is often only revealed through comparison, where one model may possess categories that another lacks [@vanrossemOntologyExplorerMethod2022].[^comparison] By comparing different data models for similar phenomena, we can discover what was lost or made invisible while developing data models and standards. For example, @cornfordRepresentingFamilyHow2013 conducted a notable comparison of digital data standards created by public service initiatives in the United Kingdom to represent family relationships. Their study shed light on "the kinds of family relationships that are recorded and those that are not recorded or harder to record, any hierarchies, implicit or explicit, for family forms or relationships and the implicit and explicit assumptions that underlie the terms and classifications used" (p. 8). Thus, as part of the first infrastructural inversion strategy, this dissertation will examine and compare data models employed in migration and border control information systems across different government agencies.

[^comparison]: As semiotics has theorized, meaning emerges from comparison. For example, @latourReassemblingSocialIntroduction2005 explains the significance of not defining groups a priori because "whenever some work has to be done to trace or retrace the boundary of a group, other groupings are designated as being empty, archaic, dangerous, obsolete, and so on. It is always by comparison with other competing ties that any tie is emphasized." (p. 32). Similarly, @pelizzaCommunityTextBack2010 recalls that "the situations where the social is made visible and graspable are those where meaning emerges from comparison" (p. 67).

However, it is essential to note that comparing data models and standards is not the sole form of analysis, and its significance is more pronounced when systems are not integrated. In integrated systems, the enactment of individuals as specific subjects relies on the interaction among various databases. For instance, a notable study by @ruppertNotJustAnother2013 examined UK information systems that share data on juveniles at risk of (re-)offending to facilitate intervention. Her research illustrated how the combination of diverse data sources, including biographical information from multiple agencies, generates specific subjects targeted for intervention. Therefore, to gain comprehensive insights into the processes of matching and linking identity data, it becomes crucial to develop a method that can uncover the embedded data models, shedding light on the interrelationships among different systems.

In short, the design of data models has a profound impact on how information is represented, influencing what becomes visible and what remains invisible [@hansethInscribingBehaviourInformation1997; @bowkerSortingThingsOut1999]. However, data models and their choices often go unnoticed within infrastructures. To address this challenge, the first strategy for infrastructural inversion proposes a comparative analysis of data models in population management across infrastructures. Such a comparison can unveil the hidden choices embedded in technical standards, which actively contribute to the enactment of populations, territories, and borders. Next, the second infrastructural inversion strategy focuses on the challenges that arise when data occasionally fail to align with these data models. This mismatch poses issues for identity linking and identification within and across organizations.

### Second inversion strategy: data practices

Data are never "raw" because they are always the outcome of some combination of human decisions and technical constraints [@bowkerSortingThingsOut1999; @gitelmanRawDataOxymoron2013]. As demonstrated in the first infrastructural inversion strategy, the format and models used for data representation contribute to their constructed nature. The second strategy delves into the often unseen labour involved in managing and cleaning data, which can be likened to the process of "cooking." This strategy proposes employing a methodological lens centred on data practices to unveil the work of identifying and managing data within data infrastructures. By adopting this lens, we gain insight into the interplay between human agency and technological constraints, shedding light on the dynamics that shape identification.

Data practices are expected to be best understood within the context of "practice theory," which prioritizes social practices as the primary focus of study rather than individual actors or technologies [@reckwitzTheorySocialPractices2002; @schatzkiIntroductionPracticeTheory2005; @shoveDynamicsSocialPractice2012]. For instance, by investigating routine activities such as cooking [@rinkinenColdChainsHanoi2019] or daily showering habits [@handExplainingShoweringDiscussion2005], attention can be drawn to the material configurations and social orderings of energy consumption. Following @schatzkiIntroductionPracticeTheory2005, practice theory approaches, on the one hand, give accounts of the interconnected practices of a subdomain of human activity. On the other hand, practice theory considers practices "the place to study the nature and transformation of their subject matter". This viewpoint aligns with the perspective put forth by @shoveDynamicsSocialPractice2012, which asserts that understanding social changes necessitates studying the evolution and stability of practices. Practice theory has been successful in explaining how routine social activities are situated. However, the theory has only recently begun to explore how social practices travel, how they are interconnected, and how they relate to other "large phenomena" such as infrastructure, as highlighted by more recent works by @shoveMattersPractice2016 and @nicoliniSmallOnlyBeautiful2016.

To further understand data practices and how they relate to practice theory, we must also consider the valuable insights offered by the "critical data studies" literature. [@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. This literature emphasizes the significance of examining the production, dissemination, and utilization of data. Similarly to infrastructure studies, attention is drawn to the often unseen coordination work underlying data infrastructures [@leeHumanInfrastructureCyberinfrastructure2006; @ribesSociotechnicalStudiesCyberinfrastructure2010; @monteiroArtefactsInfrastructures2013]. For instance, in scientific research, scholars have shed light on the meticulous efforts required to render data sets shareable and usable by others [@edwardsScienceFrictionData2011; @kervinBackstageWorkData2014; @plantinDataCleanersPristine2019]. This attention to data practices extends to identity matching, where tasks such as linking, resolving, and deduplicating data from multiple databases are undertaken to accurately identify individuals based on attributes like name, birth date, and nationality. Consequently, investigating the routine data practices involved in linking and matching identity data across diverse infrastructures holds significant potential in unravelling the processes of data production, dissemination, and utilization within data infrastructures.

Hence, applying the insights gained from practice theory and data practices to the realm of identity matching and linking procedures is imperative. Surprisingly, scant attention has been given to the work and techniques employed in matching and linking identity data, such as probabilistic record linkage, fuzzy matching, and data deduplication. Remarkably, these data matching activities and technologies share similarities with those utilized to make data discoverable, shareable, and interconnected for collaborative endeavours. Therefore, adopting a data practice approach enables empirical investigations into how identity data is matched and linked across diverse agencies and organizations (RQ2). Unveiling the practices and technologies involved in matching and linking identity data within and across data infrastructures holds the potential to provide valuable insights into how, for instance, individuals are identified throughout these infrastructures.

Practice theory and the study of infrastructure offer valuable methodological insights for investigating the dynamics of data practices involved in matching and linking identity data. Practice theory emphasizes the interconnectedness between materiality and practices as "aspects of practice-arrangement nexuses" [@schatzkiMaterialitySocialLife2010]. In data matching, this perspective underscores the integral role played by techniques and technologies employed in the matching process. These tools may, for example, flag individuals and their data as irregular, which can have significant consequences. By examining data matching practices, we can gain insight into the performativity of data practices in relation to technological artefacts and infrastructures [@ruppertDataPracticesMaking2021]. This approach recognizes that the materiality of these tools and the practices in which they are used are mutually constitutive, and both must be considered in understanding how data is processed and analyzed. The techniques and technologies used to match identity data are not separate from the practices; they are intrinsically intertwined. The inclusion of expertise within data matching systems, such as rules for determining name similarity, demonstrates how these systems shape and influence data practices. This lens allows us to comprehend the intricate interplay between materiality and practice, highlighting the ways in which technology and practice co-constitute each other throughout the data matching process and their potential for evolution and stability over time [@huiNexusPracticesConnections2016].

Using data practices as a second infrastructural inversion strategy also fits in with a trend toward finer-grained analyses of knowledge production in managing migration and borders and how practices simultaneously enact populations and states, among others [@mcharekTopologiesRaceDoing2014; @jeandesbozSmarteningBorderSecurity2016; @glouftsiosGoverningCirculationTechnology2018]. For instance, contributions from two recent Special Issues [@cakiciPeoplingEuropeData2020; @scheelEnactingMigrationData2019] took a practice-based approach to analyze the mechanisms through which knowledge production enacts migration. Before that, @pelizzaProcessingAlterityEnacting2019 had offered a framework to consider the variety of ways non-European populations are enacted in Europe and the simultaneous enactment of institutions through data practices and infrastructures. As integral components of infrastructures, data practices offer valuable insights into the phenomena arising from commercialized security infrastructures.

Building upon these perspectives, the second infrastructural inversion strategy emphasizes the exploration of routine data practices related to identity matching and linking. By focusing on these underlying practices, we can better understand how sociotechnical practices shape and stabilize identification within data infrastructures. Practice theory, recognizing the intertwined nature of technology and practice, is a foundational principle for this investigation. However, challenges arise in comprehending the evolution and proliferation of interconnected technologies across diverse settings and organizations. Therefore, the third inversion strategy proposes examining how technology has transformed over time, providing further avenues to uncover the visibility of data matching within data infrastructures. By combining these methodological approaches, we can illuminate the intricate dynamics of data practices and their role in the functioning of transnational security infrastructures.

### Third inversion strategy: sociotechnical change

Data infrastructures, including those for migration and border control, connect different contexts and timescales into large-scale networks to exchange and use information collected at multiple times and places [@dijstelbloemBordersInfrastructureTechnopolitics2021]. The networks that connect and shape relations between many actors, such as individuals, states, companies, and technologies, are often sources of tensions in the developments toward specific goals [@edwardsUnderstandingInfrastructureDynamics2007; @ribesLongNowInfrastructure2009]. While the first infrastructural inversion strategy highlights the disappearance of design decisions within infrastructures and proposes their recovery through comparative analysis, another approach to revealing the intricacies of matching and linking identity data within these tensions is tracing technological changes over time. By examining the evolution of identification systems, we can elucidate the technical choices made in response to infrastructural challenges. Informed by the insights of sociotechnical change, the third infrastructural inversion strategy recognizes the developmental trajectory of technology over time, influenced by contingent processes unfolding within diverse contexts and involving multiple actors.

Several methodological tools exist for bringing technological design choices to light and comparing how users, designers, system builders, and technological artefacts interact with and influence one another. For instance, a researcher's task in the script analysis methodology is to recover the scenarios designers incorporate into artefacts and describe the successes and failures of adapting to actual use [@akrichSummaryConvenientVocabulary1992; @akrichDescriptionTechnicalObjects1992]. Scenarios inscribed in objects imagine a mise-en-scène of the worlds where the actors and objects interact, much like a film script. Similarly, @woolgarConfiguringUserCase1990 suggested that researchers treat devices as text to be “read”. His methodological approach focused on how designers encourage specific readings by encoding user and system expectations in technological artefacts, much like the relationship between writers and readers. While these approaches are valuable, they often provide static perspectives that cannot capture the full scope of technological changes and their implications for data matching and identity linking. By studying the long-term evolution, shifts, and transformations in technology and design, we can uncover the broader sociotechnical processes that shape and are shaped by data infrastructures.

STS scholars have long documented the choices and paths of designing technological artefacts and systems using various methodological approaches to examine and understand the nexus between users, technologies, and designers. Rather than technology dictating human activity, social constructivists have argued that technological artefacts emerge from human processes involving multiple choices and unforeseen outcomes [@mackenzieSocialShapingTechnology1985; @pinchSocialConstructionFacts1984]. Along with theoretical underpinnings, scholarship like Social Construction of Technology has provided research methodologies that empirically show how the construction of technologies is tentative and shaped by the particular contexts and actors from which artefacts emerge [@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. As such, concepts such as “interpretative flexibility” [@pinchSocialConstructionFacts1984] and “boundary objects” [@starInstitutionalEcologyTranslations1989] are practical as heuristic methods that bring to light how various social actors and groups ascribe varying meanings to artefacts and make varied use of technologies. Consequently, these methodologies and concepts on technological change form the basis of the third infrastructural inversion strategy: examining how technology has developed over time thanks to the agency not only of designers but also of users and how it is influenced by contingent processes occurring in different contexts and involving numerous actors.

To comprehensively understand the transformation and influence of software technology within infrastructures, it is imperative to transcend studies focused on configuring technology and users within isolated time and place contexts, as convincingly argued by @williamsMovingSingleSite2012. Instead, a longitudinal approach is required to examine the dynamic processes that unfold across different timeframes, settings, and actors involved in the co-construction of sociotechnical problems and solutions for linking and matching identity data [@pollockSoftwareOrganisationsBiography2009]. Nevertheless, developing clear analytical criteria to identify pivotal moments in these multifaceted interactions is currently lacking and calls for further research and refinement. By addressing this gap, we can gain deeper insights into the intricate dynamics of technology development and its implications for identification.

Chapter 6 builds upon insights from sociotechnical change and infrastructure studies to propose two methodological heuristics for identifying crucial moments in technology development. The first heuristic draws from the Social Construction of Technology framework and encompasses the concepts of "interpretative flexibility," "closure mechanisms," and "social groups" [@pinchSocialConstructionFacts1984]. By examining instances when the meanings of technologies are challenged, altered, or solidified, this heuristic sheds light on the emergence and standardization of infrastructure. The second heuristic focuses on the formation of large-scale identification infrastructures, pinpointing pivotal moments when previously independent systems become interconnected. Inspired by infrastructure studies' use of the gateway concept [@edwardsUnderstandingInfrastructureDynamics2007], this heuristic recognizes gateways as critical junctures where incompatible systems can collaborate and communicate. We can effectively identify pivotal moments in the sociotechnical evolution of identity data linking and matching infrastructure by employing these methodological devices.

This section has examined the concept of infrastructural inversions and proposed three methodological strategies for investigating data infrastructures in the context of matching and linking identity data. These strategies offer insights into the technical aspects of these processes that are often overlooked. The next crucial step is to integrate these infrastructural inversion strategies into a comprehensive methodological framework. This framework will enable us to effectively uncover and analyze the interconnected technical and institutional dimensions of matching and linking identity data within transnational security infrastructures. Moreover, by combining these strategies, we can better understand the dynamics at play and shed light on the multifaceted nature of these infrastructural practices.

## A methodological framework for analyzing data matching in transnational infrastructure

### Working model for operationalizing the infrastructural inversions

This section combines the three infrastructure inversion strategies into a methodological framework that will direct the analysis of matching and linking identity data of particular transnational security data infrastructures. Each of the three inversions corresponds to a research question outlined in Chapter 2, allowing for a comprehensive examination of different dimensions and technologies involved in data matching within these infrastructures.

The methodological framework explores data matching practices and technologies in transnational security infrastructures from three distinct perspectives, each of which corresponds to a research question:

1. Comparative analysis of data models: This perspective reveals the types of information collected by different organizations and systems, shedding light on how organizations search for, match, and utilize data.
2. Examination of data practices: This perspective focuses on the actual uses in searching, matching, and linking identity data within and across organizations, providing insights into the underlying data practices employed.
3. Investigation of sociotechnical change: This perspective uncovers the dynamics of data matching knowledge, technologies, and practices as they evolve and circulate over time and across organizations.

The accompanying table describes the connections between the research question, the infrastructural inversion strategy, and the data collection and analysis techniques (discussed in the following sections).

```{r tab.cap="Methodological model", tab.id = "methodologicalModel", tab.cap.style = "Table Caption", echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "conceptual_model.csv")
data = read.csv(path, header = TRUE)

block_table(data, header = TRUE, properties = pt)
```

The methodological framework can be visually represented in a three-dimensional graph, as illustrated in Figure \@ref(fig:method-axes-visualization). This graphical representation resembles a relational model, where data is organized into tables with interrelationships. The three axes of the graph represent distinct data dimensions: _data models_, _data categories_, and _data values_. To better understand these dimensions, we will first explore each axis in more detail, their characteristics, and implications for data matching. After that, subsequent sections will explore the interconnections between these axes and how they correspond to infrastructural inversion strategies.

```{r method-axes-visualization, echo=FALSE, fig.cap="The methodological framework is represented graphically in three dimensions in this visualization.", out.width="100%", fig.align="center"}
knitr::include_graphics("figures/method-axes-visualization.pdf")
```

The axis for _data models_ (axis `z`) represents the database and other informational models that standardize the kinds of information about people collected by different organizations' IT systems. An example of such a data model could be the schemas specifying data collected about migrants by a government agency. Each system and database may have its own unique data models, while standardized formats can also exist for data exchange and interoperability between different systems. Moving to the next axis, _data categories_ (axis `x`) refer to the specific types of information captured about individuals within a data model. For instance, in the example of the asylum agency's data model, data categories may include "name," "place of birth," and "date of birth."

Connections between data models can often be identified, whether explicitly stated or implied. For example, one data model may utilize the data category "surname," while another may use "family name" to represent the same concept. These variations demonstrate attempts to capture the same real-world information within different data models. Finally, we have the axis of _data values_ (axis `y`), which pertains to the actual values stored in databases corresponding to a particular data model and its associated data categories. For instance, a data value for the "place of birth" category could be "Brussels." It is worth noting that similarities between data values can exist across different databases. For instance, another database might contain the data value "Bruxelles" for a similar category. With these three axes in mind, we can now explore how interrelationships between these aspects can be conceptualized as combinations of these axes to represent three aspects of data matching.

#### Comparing authorities' data models

The first pair _(data models `z`, data categories `x`)_ represents the identification of semantic similarities and differences between different data models and their data categories. Connections between data categories in distinct data models may exist explicitly or not. For example, one government agency’s data model may include the data category “family name.” In contrast, another agency may use “surname” as a data category. Although the names of the data categories differ, they both refer to the same referent. In this case, a part of someone’s name indicates their family. Alternatively, suppose that while one data model categorizes all languages as “language,” another differentiates between “native language” and “spoken languages.” This discrepancy can tell us something about what designers expect. Following @pelizzaScriptsAlterityMapping2023, data models will be conceptualized as scripts, as they similarly encode and enact designers’ expectations regarding people on the move. Comparing these scripts can thus reveal the expectations of various organizations regarding data collected about people and how data can be searched, matched, and used.

It is significant to note that, similar to what we want to achieve, researchers in computer science who study knowledge engineering, linked data, and natural language processing have come up with some technical solutions to compare data models. For example, finding relationships between different data categories is closely related to the outcome of schema or ontology matching processes, which automatically find correspondences between concepts of schemas or ontologies [@euzenatOntologyMatching2007; @kementsietsidisSchemaMatching2009]. Furthermore, analyzing a corpus of documents within a given domain of discourse is one of these (semi-)automatic mechanisms' primary means of extracting concepts and relationships between concepts within that domain. The recovery of knowledge representations from a discursive domain and examining the relationships between data models are thus common concerns. However, fundamental differences make it challenging to use such methods to support a discursive analysis that can answer our research questions.

Knowledge engineering methods are generally more concerned with developing technologies that help machines better understand data as a means for systems and integrating different sources of information to work together. On the other hand, the method for this research was expected to contribute to Processing Citizenship's goal of supporting discursive analysis that uses differences in representations as a starting point to analyze the imaginaries of various organizations and authorities and combine the analysis with ethnographic observation [@pcProcessingCitizenshipDigital2017]. That is why it is crucial to have methods to aid both qualitative and quantitative analyses of formalized data models. First, to assist analyses of information systems that specify their data models, even if these systems are rarely comparable. Second, to systematically and quantitatively support discursive analysis of "thin" data models by detecting differences and absences between systems and turning them into "thick" data.

This dissertation considers data models as "thin" traces after Geiger and Ribes [-@geigerTraceEthnographyFollowing2011]. This thinness refers to data models not conveying much information as relatively standardized schemas made of categories and values. However, the use of data models in various infrastructures can be a great place to start when examining how they connect geographically dispersed sites similarly to standards [see also, @latourReassemblingSocialIntroduction2005]. That is why it is crucial to consider transforming these "thin" data into "thick" data when developing an analysis strategy. For instance, @burnsWhereDatabaseDigital2020 developed a similar approach called "database ethnography" to use traces left behind by databases as sites to examine shifts in the social meanings of phenomena over time. Although researchers investigating classification have previously used and combined ad hoc approaches, there is now a need for systematic and reusable approaches to retrace the work of seemingly mundane data models.

The methodological framework developed in this dissertation addresses several aspects of matching people's identity data. The first aspect it addresses is the relationships between different information systems' data models. This is an important consideration because when identity data is collected and stored in different information systems, it is often fragmented and inconsistent, making it challenging to match individuals across systems. Therefore, understanding how various information systems' data models relate to each other is essential for understanding identity matching. We have thus developed a method to capture the scripts embedded in data models: it proceeds by comparing them to detect meaningful differences and absences, although information systems define their data models in ways that are not immediately comparable. Chapter 4 will describe such a method for extracting, analyzing, comparing, and visualizing data models from heterogeneous sources.

The methodological framework developed in this dissertation addresses several aspects of matching people's identity data. One key consideration is the relationships between different information systems' data models. When identity data is collected and stored in multiple systems, it often becomes fragmented and inconsistent, which poses challenges for matching individuals across systems. Yet, information systems often define their data models in ways that are not immediately comparable, making it necessary to have a systematic method for comparing them. Therefore, Chapter 4 will propose a method for extracting, analyzing, comparing, and visualizing data models from heterogeneous sources to detect meaningful differences and absences. Using this method will help us understand how data models represent identity information and how they shape identity matching practices.

#### Data matching within and across organizations

The second pair _(data categories `x`, data values `y`)_ associates data categories with their actual values in databases, which may not always correspond to the data models' expectations. Suppose, for instance, that a database contains first and last name values that are inadvertently switched. Also, it is common for databases to have duplicate entries for the same person. In both instances, identification procedures will have to handle inconsistent data. Also, these data problems could be addressed with the help of new technologies. For example, in many administrative procedures, such as naturalization or residency applications, it is standard practice to use some technology to help establish or verify an applicant's identity at various points. Using the second inversion strategy, we will examine routine data practices for identifying people and the use of data matching technology, such as when bureaucrats need to identify applicants in bureaucratic migration procedures based on ambiguous personal data.

The third pair _(data models `z`, data values `y`)_ is concerned with data matching across organizations and agencies. In the databases and computer systems of various businesses and governmental organizations, identity data records referring to the same person may exist. In this context, data matching refers to data practices related to identifying and potentially linking or merging such identity data spread across multiple databases. A significant challenge for data matching techniques is that unique identifiers are only sometimes available; thus, matching relies on personal information that is only sometimes complete or accurate. For example, a woman may have used her married surname in one system but her maiden name in another after marriage. Additionally, typos and other variations are common in personal data, like names and dates of birth. To understand how people are identified across data infrastructures, we will also use the second infrastructural inversion strategy to bring such less-known data practices and technologies to light.

We can extract at least three essential data matching practices to focus on from technical literature: batch data matching, real-time data matching, and data deduplication [e.g., @christenDataMatchingConcepts2012]. Batch data matching involves an offline batch processing of prepared data collections to find matching data entities. For example, an organization may enhance the data about individuals in their database by matching and integrating data from different sources. In comparison, real-time data matching addresses the problem of immediately retrieving data via direct search queries. For example, police officers may need to query databases using the data categories "name", "nationality", and "date of birth" to see if there are any approximate matches for those personal details. Generally, data matching will find almost identical identity data that experts need to evaluate rather than finding only exact matches. Lastly, data deduplication employs data matching technology to identify and combine multiple records in a database that, for instance, refer to the same person.

Empirically, the research focuses on these data matching practices by analyzing the data matching practices and technologies for identifying people throughout the data infrastructure of the Netherlands' immigration and naturalization service (IND). On the one hand, the analysis will focus on how this government service identifies applicants and deals with data issues. On the other hand, the research expands to highlight how applicants are identified throughout the broader Netherlands immigration data infrastructure. By focusing on these data practices, we will see how the identification at the agency is closely connected to a software package for matching identity data. Chapter 5 will introduce the concept of "re-identification" to capture the processes by which applicants of bureaucratic procedures are re-identified throughout data infrastructures.

#### Travelling data matching software and expertise

Another issue when considering data matching within and across organizations is how data matching expertise and technology spread over time across multiple organizations. The underlying premise is that businesses develop and deploy tools to aid customers' data matching efforts in various locations and times. In addition, expertise in related areas, such as, for example, comparing identity records containing typographical errors and name variants, may similarly circulate over time from one institution to another. As a result, this axis combination employs the third infrastructural inversion strategy to focus on sociotechnical change to make visible how knowledge and technology for matching identity data travels.

According to the literature on the relationship between technology design and use, the dynamics and evolution of data matching can become visible in examining moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. For instance, the "Biography of Artefacts and Practices" (BOAP) method describes the intricate process of creating such technologies and related practices over time. In accordance with the BOAP methodology, the emphasis of this aspect of the methodological framework is to comprehend the history of data matching software packages. This approach can highlight the diverse actors involved in developing such security technology and how the software travels and evolves over time and between different sites. BOAP goes beyond the narrow focus of the user-design-technology nexus used in the second axis to examine how technology has changed over time and by a broader range of actors at different sites.

Practically, Chapter 6 will propose two heuristics for selecting crucial moments in the lifecycle of identification technologies.
The first heuristic uses the Social Construction of Technology's concept of “interpretative flexibility” to pick out significant moments when social groups challenge, change or close down the meanings of identification practices and technologies. The second heuristic employs the infrastructure studies' concept of “gateway technologies” to pick out moments when heterogeneous identification software systems and infrastructures intersect. The combination of methods is thus used to understand the long-term development of identification systems and infrastructures.

### Fieldwork identification

#### Software package for matching identity data

The next step of the methodological framework is to situate and position the dissertation research project and the researcher in relation to, on the one hand, case- and site-specific elements. On the other hand, the methodological framework links up with the research goals of the overarching Processing Citizenship project. The following description illustrates the fieldwork site selection for examining data matching within and between organizations and the travelling data matching software. Following that, a description of how the systems' data models were chosen for comparison with authorities' data models will be provided.

The dissertation project is linked to Processing Citizenship's goal to examine how identity management systems support the creation of knowledge about non-European populations [@pelizzaProcessingAlterityEnacting2019; @pcProcessingCitizenshipDigital2017]. To gain a better understanding of knowledge production processes related to identity and security, one useful approach is to examine the database software and technology used by EU and Member State government agencies, as well as international organizations involved in these issues. Investigating the development and implementation of such software can provide insights into the challenges associated with cross-organizational data matching and identity management in security settings. By studying the specific features and functions of these systems, it becomes possible to identify the ways in which they shape and are shaped by data practices and infrastructures, and how they can impact the identification and tracking of individuals.

The first, unfortunately unsuccessful, attempt at opening up a potential fieldwork site concentrated on the problems and technological solutions EU institutions face with identity data in border security and migration management. Establishing contacts with eu-LISA, the European Commission agency managing several EU border management information systems was possible. With the support of the PC project' Principal Investigator, it was possible to participate in the 2018 annual eu-LISA conference and meet with the head officer for research and development to discuss a potential traineeship on-site at the agency's office. After an initial agreement, I applied for a traineeship with a proof of concept proposal based on probabilistic databases, an active area of research that uses a database model based on working with uncertain data [@keulenProbabilisticDataIntegration2018]. Such a proof of concept was thought to potentially give insight into data uncertainties that the EU and MS encounter. Unfortunately, the proposal had to be abandoned by August 2019 due to issues surrounding the confidentiality of the data that the research would collect. As such, the study faced common barriers to qualitative secrecy research; the proposal could not pass the “gatekeepers” that could permit access to the fieldwork site [@degoedeSecrecyMethodsSecurity2020].

The second attempt at opening up a potential fieldwork site was successful by focusing instead on a eu-LISA supplier of identity-matching software. By looking at other technology companies that work with eu-LISA, the research pinpointed the company WCC Group, which develops technology for matching identity data in border security and migration management. Indeed, the EU Visa Information System uses WCC's software, the ELISE ID platform, to search and match identity and visa data. In short, ELISE provides data matching that aims to provide fast querying based on inexact data. The company's proprietary data matching technology uses various _fuzzy logic_ algorithms for data matching that consider that data may be incomplete or inaccurate. The term fuzzy logic in this context refers to a type of mathematical logic that computes truth variables using probabilities rather than boolean “true” or “false” values. The key difference between these approaches lies in how they handle data uncertainties and produce results.

For example, consider a scenario where the data matching technology is tasked with identifying potential duplicate records in a customer database. In a boolean search, the system would simply match exact values, such as names, email addresses, or phone numbers, and return either "true" (a match) or "false" (no match). In contrast, using fuzzy logic, the technology considers the possibility of minor variations or errors in the data, such as misspellings, nicknames, or incomplete information. It then calculates probabilities of matches based on the level of similarity between records. This means that records with slight discrepancies can still be identified as potential matches, with varying degrees of confidence in their accuracy. By employing fuzzy logic algorithms, the company's data matching technology can provide more comprehensive results by accommodating variations and uncertainties in the data. The company and its software were thus deemed as excellent opportunities to empirically observe the problems and solutions of matching and linking identity data in security settings. Indeed, next to using ELISE in VIS, WCC has various other customers who use the software in border security and migration management settings.

A meeting was set up in December 2019 with WCC at the company's headquarters in Utrecht to discuss a research project that would meet the needs of the Processing Citizenship and the PhD project's research and live up to WCC and its customers' expectations. Sometime after, we proposed a project for on-site research focusing on the deployments of ELISE in the Visa Information System and the Netherlands' immigration and naturalization government agency, which WCC later approved. Unfortunately, at the same time, the Covid-19 virus spread worldwide and became a global pandemic. As a result, the government of The Netherlands announced in March 2020 that people should work from home. We decided against starting remotely and instead pushed back the start date because we believed the research would benefit more from face-to-face interaction. In May and June of 2020, the government of the Netherlands loosened up some restrictions on working on-site and taking public transportation. Therefore, we decided to start the fieldwork in a hybrid way, with remote access to relevant resources and a few on-site visits to the Utrecht office when the staff members were present. The summer months provided a more relaxed environment, which meant fewer people were present, and it coincided with holiday schedules. However, by September 2020, the government had once more tightened Covid-related regulations, making it necessary to conduct subsequent fieldwork online. A more in-depth discussion will be found in later sections and chapters.

#### Data matching within and across the organizations

We can map the fieldwork choices onto the methodological framework and infrastructural inversion strategies. Initially, an emphasis was placed on data practices to comprehend how the ELISE identity-matching software influences organizations' identification processes and vice versa. I expected that the embedded assumptions in the tools regarding the uncertainty of identity data influence, to a certain extent, work practices regarding the identification of individuals. Due to the company's limited knowledge of how customers were using ELISE, this focus on data practices related to identifying people was expected to be of interest to both the researcher and WCC, who could use this knowledge to improve the software's user experience. The research study documented the results of analyzing a customer's process of matching identity data, which were presented in an online company webinar and compiled into a report. For instance, the report highlighted an issue with identifying whether someone already exists in the system when creating an application, which the company was examining to explore possible improvements. Therefore, the study's insights and recommendations were valuable to varying degrees for the researcher, company, and customer.

During the fieldwork, I examined users' everyday data practices to learn how they use biographic matching and multicultural name-matching tools. Additionally, comparing the software's design and actual use was helpful to understand the challenges of identifying people from ambiguous data in relation to the technology's affordances. As a result, the study looked into data practices related to the ELISE software. Indeed, as we will see in Chapter 5, tensions in the imagined and actual use of the search and match functionalities were evidence of difficulties with ambiguous identity data. Therefore, the first goal of the fieldwork was to examine data practices and answer the second research question, which is how organizations that collect information about people-on-the-move search for and match identity data in their systems, as well as how data about people-on-the-move is matched and linked across different agencies and organizations.

The second fieldwork goal was to answer the third research question, or how knowledge and technology for matching identity data circulate and travel across organizations. The research identified pivotal moments in developing and deploying the software package. For example, the fieldwork research investigated system builders’ work to make identity data matching work across different contexts and organizations. Looking at sociotechnical change revealed the changing interpretative flexibility of the software and some closure mechanisms leading to the current data matching solutions. Furthermore, investigating how the ELISE software connects disparate systems made it possible to conceptualize it as a gateway, a crucial technology in infrastructure building. In this way, it was possible to understand technological development as a dynamic process that involves numerous actors across time and locales. Therefore this goal corresponds to the methodological framework’s aspect of knowledge and technologies for matching identity data travel and circulate and uses sociotechnical change as an infrastructural inversion strategy.

#### Deployments of ELISE at the IND and EU-VIS

The initial focus of the methodological framework was on ELISE software deployments at two customers of WCC, one in a national setting and the other in a transnational setting. First, the use of ELISE in national settings was investigated through the software's integration with the systems of the Netherlands' immigration and naturalization service (IND). Second, the use of ELISE in transnational settings was investigated through the software's integration into the EU Visa Information System (VIS). However, we should be careful in distinguishing between the national and the transnational. The social research methods employed for this inquiry not only describe realities but are a part of the “ontological politics” of enacting certain kinds of realities [@lawEnactingSocial2004]. We should therefore be careful to avoid reproducing fixed distinctions between national, transnational, or international. Instead, following Law and Urry, the question is, on the one hand, to which realities we want to contribute. On the other hand, researching data infrastructures can reveal the enactment of social orders [@pelizzaProcessingAlterityEnacting2019].

The ELISE search and match solution's use in those two contexts was deemed relevant to understand how the tools shape the practices of knowledge production related to linking and matching identity data. In the case of the IND, the agency uses ELISE throughout its bureaucratic processing of applications from people who want to stay in The Netherlands or become Dutch nationals. The agency's information system thus leverages the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system. In the case of the EU VIS system, ELISE has a narrower scope in its use of biographic matching and multicultural name-matching capabilities for its search engine. At the same time, due to its transnational dimensions, the amount of records in the EU-VIS database is much higher.[^eu-vis-capacity] As we will see in Chapter 6, there are some connections between the deployment of ELISE in these two systems. Therefore, examining the data practices and sociotechnical change related to the ELISE deployments in these two settings was deemed suitable to make the matching and linking of identity data in transnational security infrastructures visible.

[^eu-vis-capacity]: The “Report on the technical functioning of the Visa Information System (VIS)” [@eu-lisaReportTechnicalFunction2020] notes a database capacity of 60 million records.

The methodological choice of tracing the development and deployment of the ELISE software package for these two systems necessarily has some “framing effects” [@hyysaloMethodMattersSocial2019], which refers to how researchers' choices about their study's research design have consequences for how their studies frame the technology under investigation. Although WCC's customers includes both public and private entities from various sectors (such as employment agencies), the "Identity and security" market segment was singled out for analysis in this dissertation's framing. Furthermore, not all identity and security customers were possible to research due to issues such as confidentiality. At first, WCC's work in other sectors beyond identity and security seemed unimportant for the research. However, links with customers from other sectors repeatedly emerged during the fieldwork and interviews in discussions about the genericness of the search and match. This finding will be important in Chapter 6, which investigates the origins of the search and match software's ability to be applied in various contexts. Chapter 6 will also show how other actors, directly and indirectly, influenced the company and its products beyond the research's initial framing, such as research centres, competitors and potential customers. The investigation gradually introduced these ancillary actors by attending industry events and relying on other sources, as detailed in the following subsections.

#### Choice of authorities for comparing data models

I compared data models of EU and Member State authorities. First, three important information systems developed by European Commission agencies for border security and migration management can represent the EU as one authority. These systems are Eurodac, the Schengen Information System (SIS), and the Visa Information System (VIS). All three systems use different data models to support policing tasks involving travel, cross-border crime, or irregular migration. Eurodac (_European Dactyloscopy_) aims to support the identification of asylum seekers through fingerprints and to ascertain the Member State responsible for processing their asylum applications within the context of the Dublin System.[^dublin-iii] The database was created in 2003 to store asylum seekers' fingerprints and other basic information. It is currently operational in the Member States of the European Union (plus Norway, Iceland, Switzerland, and Liechtenstein). Law enforcement agencies and Europol also use the system, which has significantly expanded its original scope since its inception [@ajanaAsylumIdentityManagement2013]. New proposals aim to include more biographic and biometric data, including a facial image. [^recast]

[^dublin-iii]: The Dublin System (Regulation No. 604/2013; also known as the Dublin III Regulation) establishes the criteria and mechanisms for determining which EU Member State is responsible for examining an asylum application.

[^recast]: Procedure 2016/0132/COD, a recast of the Eurodac Regulation.

The Schengen Information System (SIS II) aims to support external border control and law enforcement cooperation in the European Union. It supports this task by storing alerts which contain information on persons and objects, and instructions on what to do when officers encounter such persons or objects. In addition, the SIRENE network can exchange this information between law and border enforcement authorities. Finally, the Visa Information System (VIS) allows exchanging of visa data (including personal and biometric data). In this way, it aims to maintain a common EU visa policy. Authorities use VIS data in the context of border identification procedures. Access is thus different from the Eurodac and SISII systems, as authorities can only access VIS data on a case-by-case basis. Officers at the external border of the European Union also have access for purposes such as “verifying the identity of the person, the authenticity of the visa or whether the person meets the requirements for entering, staying in or residing within the national territories” [@europeancommissionVisaInformationSystem]. Furthermore, asylum authorities use VIS data to determine which EU Member State is responsible for examining the asylum application.

Concerning national systems, the analysis includes the systems of the Hellenic and German Register of Foreigners. These systems were chosen as part of the broader Processing Citizenship's task plan of examining actual identification practices in "processing alterity" [@pelizzaProcessingAlterityEnacting2019]. For this reason, it is necessary to view the analysis of data models within the context of the PC project's task of investigating how national and European authorities formalize knowledge about individuals in data models used for managing migration. The PC project's script analysis methodology thus aimed to produce a typology of “intended migrants”, which is the formalized knowledge of migrant identities inscribed in information systems [@pelizzaScriptsAlterityMapping2023]. On the one hand, comparing data models aided this dissertation's research in further understanding the authorities' capabilities and restrictions in linking and matching identity data. On the other hand, the analysis could be helpful to ground PC researchers' analysis of actual practices at the border by migrants and officers [@olivieriTemporalitiesMigrationTime2023].

The Hellenic Register of Foreigners is the primary system used at border zones in Greece to identify and register persons who arrive at the border without the required documents. In addition, officials use the system to support various tasks during the identification and asylum procedures, such as retrieving migrants' biographic and biometric data, conducting screening and asylum interviews, and assessing health conditions. Therefore, the systems' users include police, administrative personnel, and asylum officers.

The German Register of Foreigners (GRF) is a system containing a large amount of personal information of foreigners in Germany who have or had a residence permit, as well as those who seek or have sought asylum or are recognized asylum seekers [@bvaAuslanderzentralregister]. Various partner authorities and organizations in asylum, migration, and border control access this central register. The XAusländer standard, a data exchange format which formalizes and enables data exchange between the immigration authorities in Germany, describes the data sent to the GRF during the first registration [@bamfStandardXAuslaender]. According to the description of the standard’s motivation, it aims to facilitate the exchange of such data between authorities in Germany to reduce data re-entry and enable the reuse of such data by the authorities. It is important to stress that all the authorities' information systems described above define their data models and the kinds of people they aim to represent in ways that are not immediately comparable due to formatting differences. The next chapter will thus introduce a digital method called "The Ontology Explorer" used to extract, analyze, compare and visualize those diverse data models.

This section outlined how this dissertation makes data matching in specific transnational security data infrastructures visible by combining three infrastructural inversion strategies into a methodological framework and operationalizing it. First, the ability and limitations of the data models to link and match identity data are compared, along with the authorities' knowledge of individuals in the EU and MS. Second, data practices can demonstrate how identity information is linked and matched within and across organizations. Third, the dissemination of data matching knowledge and tools is examined using a social constructivist understanding of sociotechnical change. The following section details the types of data collected through field and desk research and the various methods used to analyze this data.

## Methods for data collection

The methodological framework guided the collection and analysis of data matching in European security data infrastructures. Notably, various techniques were used to gather and analyze qualitative data to support the investigation, including document analysis and user interviews. Naturally, these methods informed each other at various points throughout the research.

### Traces of data models

To investigate the matching of identity data across organizations, it is useful to understand how the data models of different systems correspond. Records referring to the same person can be challenging to identify without knowledge of the underlying data models of the datasets being matched, as the same person may be represented differently across different datasets. The first step in data matching typically involves finding correspondences and similarities in the types of information stored in the respective databases – i.e., data models. For instance, two systems may hold a combination of a person’s last name, date of birth, and nationality. To compare these individual identifiers, it is necessary to deal with the different data models that each has their labeling, types, and formats of attributes. These data models and their data categories, which describe a state that can assume different values, are typically specified in various documents such as database schemas, design documents, or regulations. Thus, investigating the development and implementation of database models used in identity and security-related settings can provide valuable insights into cross-organizational data matching and identity management.

Because data models are defined in various formats, some of which may not always be accessible, such as confidential technical database specifications, the term "traces" is used in this dissertation. This broader term is utilized to encompass other forms of data model description than technical specifications, such as screenshots of a graphical user interface. These traces were then used to reconstruct the data models, allowing for a comprehensive comparison of the data models used by the different authorities. In general, desk research or fieldwork enabled the collection of such data model traces. Chapter 4 details the method to compare diverse descriptions of data models through additional steps to code, harmonize, and group all documents, categories, and values.

Regarding the traces of those models, the research draws on data collected during fieldwork conducted in the context of the Processing Citizenship project at border zones in Europe. In addition, given linguistic constraints and the PC project’s task plan organized as a matrix, some documents were collected by other researchers employed as collaborators in the Processing Citizenship project.[^pc-team] Overall, the data collection efforts included desk research of European regulations over Eurodac, VIS, and SISII, technical documents made available by European and German authorities, systems screenshots collected at border zones in the Hellenic Republic, interviews and ethnographic observation with IT developers and users in Italy and Greece, and technical documents collected during fieldwork in The Netherlands.

[^pc-team]: Over the years, PC researchers have been: A. Bacchi, E. Frezouli, Y. Lausberg, C. Loschi, L. Olivieri, A. Pelizza, A. Pettrachin, S. Scheel, P. Trauttmansdorff.

### Documents and documentation practices

The document analysis related to the deployment of ELISE is grounded by the STS understanding of documents and practices of documenting as research objects. However, the extensive literature review of @shankarRethinkingDocuments2017 shows that various STS scholars have broadened our conception of documents as objects that do more than just record information. Instead, scholars remarked on how documents play an active role in social contexts, such as accounting for and coordinating workplace activities. Documents, in this sense, are inseparable from the processes that generate them [@hullDocumentsBureaucracy2012; @rilesDocumentsArtifactsModern2006]. Therefore, the dissertation looked not only at documents to help explain how the software package functions. Instead, documents help understand the organizations as their records "refer to the practices, objects, rules, knowledge, and organizational forms that produced them" [ibid., p. 62]. In this way, the findings from the document analysis mainly contributed to an understanding of the biographical moments in the life of this software package.

Several on-site visits to WCC's headquarters in Utrecht, the Netherlands, made it possible to consult ELISE-related documents such as technical design documents, product brochures, and meeting minutes. On the one hand, documentation related to the more general technical particulars of the ELISE ID platform made it possible to become acquainted with the software suite and how WCC designed it. On the other hand, documents related to the platform's specific implementations at the IND made it possible to understand how WCC practically configures the software package for customers. In addition, WCC's “ID team” provided additional context for the documents and updates on the status of ELISE integration in the EU-VIS and INDiGO systems.

The document analysis in this study had two primary objectives, which were integral to the methodological framework. The first objective aimed to compare the utilization of WCC ELISE's biographic matching capabilities by different users within the IND during their daily identity searches. This comparison sought to identify any discrepancies between the expected use of the software and the actual practices employed by IND users. By examining how the software is employed in real-world identity searching and matching, we gained insights into the frictions that arise from the disparity between anticipated and actual usage. This analysis provided valuable insights into how IND staff members shape their expectations regarding identification processes and the levels of uncertainty surrounding identity data. In Chapter 5, we explore how the designed probabilistic identity match influences the user's understanding of search results, highlighting the impact of the software on the user's perspective. Thus, addressing the first objective of the document analysis helps to answer the research question 2 on how organizations collect and search for identity data of individuals on the move, and how such identity data is matched and linked across different agencies and organizations.

The second objective of the document analysis was to uncover key moments in the evolution of the software package. Notably, scholars have highlighted the importance of examining documenting practices rather than solely focusing on individual document versions [@bowkerSortingThingsOut1999; @shankarRethinkingDocuments2017; sweeneyAmbiguousOriginsArchival2008]. This perspective offers fundamental insights into understanding the trajectory of the software package and its development over time. For example, we can retrace documents' histories by looking at different document versions, such as changes within documents or added annotations. For example, the group of documents related to the deployment of ELISE at the IND included different versions of the design document, documents that described updates to the package, and meeting notes about changes to the configuration. However, most collected documents lacked a clear context. As a result, determining the motivations or source of a document can be challenging. For instance, the IND project documentation that WCC kept was an archive dating back more than ten years, and the staff members who worked on it were no longer with the company. As a result, recovering the context of some of these documents and their origins and motivations turned out complex, even for current WCC employees.

Rather than seeing individual authors, we should consider documents as assemblages of different authors and sources [see also, for instance, @latourReassemblingSocialIntroduction2005; @venturiniOnceTextANT2012]. That is why documents help understand how knowledge and technologies for matching identity data travel and circulate. Documents often lack a clear origin and are instead “assembled from multiple sources [where] content often flows from application to application and document to document, constantly recycled, reworked, and repackaged” [@shankarRethinkingDocuments2017, p. 63]. In the ELISE documentation for the IND, such flows were readily identifiable. For example, technical documents include IND-specific implementation details alongside package-specific information from other company documents. Thus, documents and documenting practices can help answer how knowledge and technology for matching identity data circulates and travels across organizations (RQ3). The software's history, including its distribution, modification, and adoption by different groups, is documented in various forms throughout the project's lifecycle.

The absence or inaccessibility of documents is a notable, albeit less visible, aspect of documentation. For example, it became clear during fieldwork that records for the deployment of ELISE in the EU Visa Information System would be inaccessible. The specific arrangement of actors involved in developing the EU-VIS systems resulted in this lack of access. In this arrangement, WCC was the technology supplier and collaborated with Accenture, the technology integrator. This arrangement meant that the relevant documents were in the hands of the technology integrator. Moreover, even for WCC, the technology integrator acts as a gatekeeper, deterring access to such materials. Nevertheless, the absence or inaccessibility of documentation can still provide insights into the work of actors building these systems.

In short, the document analysis made it possible to form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents give insights into the package's genealogy and the practices of configuring, deploying, and designing the software. The table below overviews the documents consulted. In addition to the more technical documents, public communications and reports gave valuable insights into the particular context of the IND information system developments. The findings from the document review were used to inform the design of the interviews.

```{r documents, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "documents.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data, header = TRUE, properties = pt)

# pander(dt)

# kbl(data)

# kbl(dt, booktabs = T) %>%
  # kable_styling(latex_options = c("striped", "scale_down"))

# block_table(data[,c("Document", "Author", "Year", "Description")], header = TRUE, properties = pt)
# ft <- flextable(data)
# ft <- add_header_row(ft,
#   colwidths = c(4, 2),
#   values = c("Document", "Author", "Year", "Description")
# )
# ft <- autofit(ft)
# ft <- theme_vanilla(ft)
# ft <- set_caption(ft, caption = "Documents")
# ft
```

### Interviews {#interviews}

Semi-structured interviews with a diverse range of actors provided additional insight into the design and implementation of the searching and matching tools. The interview strategy centred on two main themes and participant groups. The first group involved IND employees whose duties included looking up and matching identities using the ELISE search and match engine. The second set of interviewees were conducted with WCC employees with responsibilities related to the company’s identity matching software package, including design, implementation, and (pre-)sales.

More specifically, the interview approach focused on individual interviews, allowing more time and possibilities to discuss topics in detail. Individual user interviews were the easiest to set up and provided more detail of personal experiences. However, opportunities to observe group dynamics and discussion were missing compared to other approaches. The interview protocol included questions and probes for follow-up based on the research questions and insights from the document analysis. The Appendix contains a sample of the interview questions.

Due to unfamiliarity with the IND and the necessity of conducting research digitally, interviewees were only available after considerable time and effort. WCC's “ID Team” members helped set up the first contact with the IND organization. The search for IND employees willing to participate in the study using respondent-driven sampling grew after the initial contact. Email invitations were sent to the gathered contacts inviting them to participate in an online interview. The small-scale snowball method of WCC and IND staff networks allowed for the inclusion of participants who were unknown before the start of the research. However, there was little control over the sample size of respondents. It is also worth noting that WCC had only one point of contact within the IND. Dependence on these personal networks led to a bias in who was included, in this case, more senior members of the IND organization.

The Covid-19 pandemic made it tough to have in-person interviews, so much of the research was conducted online. Measures at the time restricted people's options to go to their place of work, so most IND and WCC staff were working from home, which limited the interview format to video and phone calls. These telephone and online conferences had the advantage that scheduling the interviews was more straightforward because meetings did not involve travel. On the other hand, not all communication may have come across the same way, which may have made it harder to network and schedule more interviews. The interviews used the WCC company's secure Microsoft Teams installation for meeting online. However, not all IND employees who worked remotely had the authority to install the software on their company's laptops. Participants' IT access limitations, therefore, necessitated phone interviews in some cases.

The study hypothesized that users performing different tasks at the IND would use the tools for searching and matching identities differently. For this reason, the participants included members of the IND's various organizational units (for a list of these units, see Table \@ref(tab:ind-departments)). As we will see in Chapter 5, the interviews revealed vast differences in tool use and familiarity. It was, unfortunately, impossible to interview users from each unit. Nevertheless, details about the use of the search and match tools in the other departments surfaced during the interviews. Participants described using the search and match tools at the IND in five interviews, each lasting approximately an hour. Table \@ref(tab:ind-departments) summarizes the various IND units and whether or not the members from the respective units were interviewed.

```{r ind-departments, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "ind-departments.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Org", "Unit", "Incl", "Description")], header = TRUE, properties = pt)
```

The research also included interviews with people from the company WCC, which took place concurrently with and after the IND interviews. These interviews followed a looser format than the ones conducted with IND employees. The interviews aimed to illuminate events in the history of their identity-matching software system. The participant's role, projects, and company experiences thus influenced the interview questions and probes. For example, the interview included questions about system development and deployment for people who worked on the IND or EU-VIS projects. This structure also allowed asking people with different profiles about their connections with current and potential customers in the security and identity market. In addition, a few WCC employees present during the fieldwork were invited to participate in an online interview. On the other hand, respondent-driven sampling expanded the number of participants. Based on their profiles, we can divide these participants into two clusters. The first cluster comprises WCC's “ID Team” members who hold consultant, pre-sales, and solutions manager positions. The second group consisted of the more technically minded; among them were a senior software developer and a user experience designer. Participants described their knowledge of building and deploying the company's software in seven interviews, each lasting approximately an hour.

The number of interviews conducted in this study was lower than initially expected and fell short of the number initially foreseen by the Processing Citizenship project. Several factors contributed to this outcome. Firstly, due to the sensitive nature of their work in border control and security, it was challenging to gain access to additional customers beyond the software technology supplier and one of their customers, despite the initial intention to include them. Clearance and background checks required for individuals involved in these areas posed difficulties for the researcher in expanding the participant pool. Additionally, the Covid-19 pandemic further complicated the situation by limiting networking opportunities and hindering the ability to find additional interview participants, particularly in the case of IND interviews. As a result, the study had to adapt to conducting online interviews, which, although enabling data collection to continue, introduced constraints such as reduced rapport-building and limitations in gathering nuanced insights compared to face-to-face interactions. Despite these challenges, the study made the most of available opportunities and provided valuable insights within its defined scope, shedding light on the perspectives of the software technology supplier and their customer.

Participants consented to record the sessions using Processing Citizenship's informed consent form. The form allowed participants to specify how the research would use their provided data while guaranteeing anonymity and confidentiality. For example, per protocol, the recording only included audio with a distorted voice for added anonymity. Furthermore, manual transcription of interviews ensured additional confidentiality by preventing confidential information from being leaked via automated transcription platforms. Table \@ref(tab:interviews) provides an overview of the interviews.

```{r interviews, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "interviews.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Nr", "Org", "Date", "Channel", "Language", "Description")], header = TRUE, properties = pt)
```

### Events {#events}

In addition, data were collected through observations and field notes during events that involved a wide range of stakeholders, including industry representatives, academics, Member State authorities, and EU agencies. The selection of these events was based on two main criteria. Firstly, preference was given to events that focused on topics such as data quality, identification, and interoperability of EU identification systems. Secondly, preference was given to events involving WCC company's participation and their knowledge and viewpoints on identity and security matters. A detailed summary and description of these events, both attended in person and online, can be found in Table \@ref(tab:fieldwork-events). The primary objective of actively participating in these gatherings was to acquire invaluable insights into how different stakeholders within the industry define and approach the challenges associated with the quality of personal data used in their systems and explore potential solutions for linking and matching such data. These events not only offered an opportunity to delve into the challenges encountered by practitioners and the diverse roles undertaken by stakeholders such as industry representatives, academics, Member State authorities, and EU agencies but also facilitated an exploration of how knowledge and technology for matching identity data circulate and travel across organizations, thus addressing research question three.

An illustrative instance highlighting the significance of these events occurred during the 2018 eu-LISA conference, where an audience member raised a thought-provoking question concerning the management of a potentially high volume of false positives resulting from the integration of disparate databases containing outdated information. One of the panellists astutely acknowledged the time-consuming nature of addressing all these false positives. Such real-world examples, shared within the context of such events, shed light on the often-unseen efforts required to successfully integrate diverse identification systems, offering invaluable perspectives on the challenges practitioners confront in their work.

Hence, one assumption underlying this study was that recommendations from security and identification experts would reflect diverse perspectives on connecting and utilizing data from multiple sources. While not central to the dissertation, this aspect aligns with the STS literature on "sociotechnical imaginaries," which explores how societal expectations, sociotechnological projects, and social order shape one another [@jasanoffSociotechnicalImaginariesNational2013]. During my fieldwork, I came across an insightful example that I incorporated at the beginning of Chapter 5. This example was presented during an online session and effectively illustrates how security companies emphasize the significance of employing methods for searching and matching data stored in diverse databases, particularly in the context of counterterrorism efforts. The software company shared the case of authorities adding one of the "Boston bombers" to police watch list databases prior to the attack, but with inconsistent and invalid transliteration variations of his name. The security professionals delivering the presentation stressed that the existence of disparate information across databases presents considerable challenges when conducting effective investigations and may lead to potential blind spots for authorities. The work conducted by @trauttmansdorffInfrastructuralExperimentationCollective2021 provides evidence of the impact of such sociotechnical imaginaries on the development of large-scale border security infrastructure. These imaginaries can either facilitate or impede the creation of specific forms of infrastructure, shaping the trajectory of such projects.

```{r fieldwork-events, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "fieldwork-events.csv")
data = read.csv(path, header = TRUE)

block_table(data, header = TRUE, properties = pt)
```

### Other data {#other-data}

Other data used in the analysis came from news articles, press releases, and other publications. These publications were collected to give more context and help understand moments in the history of the WCC search and match tool and its use at the IND and EU-VIS. The Nexis Uni[^nexis] database and tool were used to search for articles that mentioned the company WCC and its software in the INDIGO system of the IND or the EU-VIS system. Nexis Uni allows users to search for articles in newspapers, online business publications, and other sources written in English and Dutch. For example, several articles published in Dutch IT business news websites provided valuable information on IND's INDiGO system. This database also uncovered press releases concerning the WCC's participation in the MITRE multicultural name matching challenge (discussed in Chapter 6).

[^nexis]: http://web.archive.org/web/20230606140543/https://www.lexisnexis.com/en-us/professional/academic/nexis-uni.page (Formerly LexisNexis Academic)

The document corpus also includes blog posts, and business press releases that disseminate information about the systems' creation, development, deployment, and utilization. These additional publications are discoverable via conventional search engines. For example, blog posts on WCC's and competitors' websites showed how experts in the field of name-matching devise and disseminate their knowledge. Take, for instance, the articles "Understanding Dari and Pashto names: a challenge to intelligence gathering in Afghanistan" [@scheersBiographicMatchingUMF2021] and "Biographic matching & UMF standards for EU interoperability" [@btcUnderstandingDariPashto2012]. Such publications gave essential insights for Chapter 6's SCOT analysis on how companies, as relevant social groups, define problems and solutions of identification. A selection of these publications dealing with data quality, data matching, or other relevant discussions surrounding the operation of the ELISE, EU-VIS, and INDiGO systems was further coded and analyzed.

## Techniques of data analysis

A range of techniques were employed to gain an understanding of the information collected from various sources, considering different aspects of the methodological framework. Data analysis involved examining the relationships between data models and investigating data matching within and across organizations. Chapter 4 introduces a novel method and tool specifically designed for analyzing data models of information systems. In Chapters 5 and 6, well-established techniques were utilized to code and analyze qualitative data obtained from field notes, interviews, and technical documents. Overall, the combination of these techniques provided a comprehensive analysis of the information collected, enabling a deeper understanding of data matching in transnational security infrastructures.

Chapter 4 details the methodology for comparing data models named "The Ontology Explorer". The Ontology Explorer (OE) is a semantic method and JavaScript-based open-source tool to compare the data models collected in different formats and from diverse systems in two ways. First, it supports analyses of information systems which define their data models, even if these systems are only sometimes comparable. Second, it systematically and quantitatively enables discursive analysis of “thin” data models by detecting differences and absences between systems. The method extracts, analyses, compares and visualizes heterogeneous data models to achieve this. Applying this structured approach to the data models used by diverse organizations eventually made it possible to observe differences and similarities in the data models of diverse data infrastructures.

In this way, the OE analysis explores an important aspect of matching identity data, namely the examination of correspondences between the categories of data in different EU and Member State systems' data models. This investigation sheds light on the interrelationships among data models for collecting information about people-on-the-move and provides insights into the organizations that have developed and utilize these models for data collection, searching, and identity matching purposes. The analysis of EU and Member States' data models was conducted using the Ontology Explorer methodology and tool. The results revealed notable differences and similarities in the data collected across the systems and the data used for identity searching and matching. Moreover, these disparities and commonalities in the data models offer insights into the circulation of knowledge about individuals and highlight divisions of responsibilities among various actors involved [@pelizzaScriptsAlterityMapping2023]. The findings from the Ontology Explorer analysis shed light on both shared data categories crucial for identity searching and matching and distinct categories that indicate specific responsibilities assigned to different systems and organizations.

Other qualitative fieldwork data analysis followed standard computer-assisted qualitative data analysis methods. As such, the data analysis relied on the software _ATLAS.ti_. Cleaned-up field notes and transcribed interviews were imported into the software. The data coding and analysis drew inspiration from the three interconnected steps of the "Noticing-Collecting-Thinking" (NCT) method by @frieseQualitativeDataAnalysis2014, which follows a standard qualitative data analysis but is tailored for the ATLAS.ti software package. First, labels were assigned to segments in the documents, i.e., codes, in the Noticing step. The research questions guided this coding step, but the coding was simultaneously open to inductive findings from the data. Second, those codes were reviewed and gathered into similar codes in the Collecting step. Third, among the developed codes, patterns, processes, and typologies were found in the Thinking step. Of course, in practice, the whole process proceeds by moving back and forth between the different steps of noticing, collecting and thinking. Figure \@ref(fig:data-analysis) summarizes this recursive data collection process and application of the NCT steps.

```{r data-analysis, echo=FALSE, fig.cap="Data analysis.", out.width="80%", fig.asp=.75, fig.align="center"}
knitr::include_graphics("figures/data-analysis.pdf")
```

Take, for example, the typologies developed based on analyzing data collected during fieldwork and from interviews with practitioners using the ELISE software to search and match data at the Netherlands' asylum and naturalization agency (see figures in Chapter 5). Chapter 5 employs these typologies to develop the concept of "re-identification". For instance, a non-exhaustive typology of potential data frictions in alphanumeric identity data included ambiguity and incommensurability in transliterations, variations in identification policies, and frictions brought on by human errors during data entry. As a result, it was possible to theorize how algorithms for dealing with incomplete, inaccurate, and unreliable identity data deal with the problem of re-identifying people throughout the agency's bureaucratic procedures. Analyzing the findings of ELISE's implementation at the agency thus revealed how fictions about handling data uncertainties influence daily (re-)identification practices.

These examples demonstrate how, by drawing from various sources, it is possible to trace multiple aspects of data matching practices and the design and use of data matching technologies. In this section, we provided a general overview of the data analysis methods employed. However, it is important to note that the specific coding and analysis processes were conducted on a case-by-case basis, and detailed descriptions of these processes can be found in each empirical chapter. The following chapter will focus on the first aspect of the methodological framework, which involves analyzing authorities' imaginaries of populations and the scripts through which they enact actual people. Additionally, the chapter will describe the development of the Ontology Explorer as a new method and tool that utilizes the first infrastructural inversion strategy by comparing data models in the information systems of both EU and Member State authorities.
