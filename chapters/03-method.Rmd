# Research design and methods {#ch-method}

\chaptermark{Research design and methods}

In this dissertation I deal with the multifaceted phenomena of matching people’s identity data, and how the development of technologies to make this possible shape and are shaped by transnational security infrastructures. Still, how can we get to know these facets of the technologies and practices involved, and how is what we know bound up with different methods and perspectives? This chapter develops a conceptual framework which integrates perspectives from primarily three strands of research into a visual tool to comprehensively analyse the matching of identity data in transnational security infrastructures. The chapter is composed of two parts. The first part introduces conceptualizations of systems of migration, border control as infrastructures and the well-known problem that infrastructures (and the technical choices made) tend to disappear. I therefore propose three bodies of literature that can each highlight different ways for making visible diverse facets of security infrastructures. There is a need for combining methods as most studies in the field have focused on either local interactions, or broad technical overviews.

Digital infrastructures can be both topics and resources of research of social inquiries [@marresDigitalSociologyReinvention2017]. Deciding of what an infrastructure is, the difficulty of demarcating where an infrastructure begins and ends, is often approached by studying an infrastructure ethnographically [@karastiStudyingInfrastructuringEthnographically2018; @starEthnographyInfrastructure1999]. An ethnographer reflexively approaches the infrastructure and reflexively constructs it ‘by every choice the ethnographer makes in selecting, connecting, and bounding the site and via the interactions through which s/he engages with the material artifacts and the people who define the field‘ [@blombergReflections25Years2013, p. 389]. This is one way of defining the boundaries, which puts a lot of responsibilities on the researcher. A different way is following the acts of speech and acts of doing (practices) as brought into the discourse by social actors. As @beaulieuResearchNoteColocation2010 notes, being present together with these actors at the same location just one method for following knowledge production. Following her proposal, other senses of ‘co-presence’ can be achieved through different traces of the mediations and infrastructures for identity matching.

Surprisingly, the matching of identity data in identification has not been closely examined in literature. There is a need to analyse the technical details of security infrastructures, such as the information representations of various systems and the mechanisms of linking identities between systems. The second section of this chapter develops the conceptual framework for analysing data matching in distributed settings. This framework aims to help analyse data matching based on the different perspectives from literature, and visualized in the framework along three axes: the matching of data models, data matching within organizations, and data matching across organizations. The next chapter will detail how this conceptual is operationalized to analyse the matching of identity data in EU and member states‘ systems.

## Research objectives: The conceptual framework as a heuristic for tracing data matching in transnational security data infrastructures

## ‘Infrastructural inversions’ for matching identity data

What methods and frameworks are best suited to trace the developments of matching of identity data in border and migration control? In recent years, there has been an increasing amount of literature on Europe’s technological systems of border, migration control, and security as infrastructures [@dijstelbloemBordersInfrastructureTechnopolitics2021; @potzschEmergenceIBorderBordering2015]. Such large technical systems or infrastructures have been a long-term concern for scholars studying the processes, actors, tensions in the long-term historical developments of technologies [@hansethDevelopingInformationInfrastructure1996; @hughesNetworksPowerElectrification1983; @starStepsEcologyInfrastructure1996]. In this way, existing research on border, migration control, and security as infrastructures recognizes the critical role played by underlying infrastructures in organizing and giving shape to large-scale phenomena. Key to infrastructural perspectives is to bring out how, for instance, data matching technologies would ‘consist of numerous systems, each with unique origins and goals, which are made to interoperate by means of standards, socket layers, social practices, norms, and individual behaviors that smooth out the connections among them’ [@borgmanKnowledgeInfrastructuresIntellectual2013, p. 5]. Conceptualizing the socio-technical systems to manage people’s identities as security infrastructures makes it possible to untangle the networks in these complex ‘ecologies’ [@starStepsEcologyInfrastructure1996].

A well known result of these overlapping between infrastructures and practices is that infrastructures have a tendency to be taken for granted and to ‘remain as invisible backdrops to social action’ [@harveyIntroductionInfrastructuralComplications2016a, p. 3]. Studies on infrastructures have therefore provided different strategies to invert this tendency of infrastructures to disappear, and to make visible the interconnections between technical minutiae and the politics of knowledge production [e.g. @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Methods proposed by these authors to invert the tendency of infrastructure to disappear include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019]. These kinds of ‘infrastructural inversions’ [@bowkerScienceRunInformation1994] makes it possible to acknowledge the links between technical systems for identity matching and the processes of identification in transnational security settings.

The case of the EU interoperability project is one example where tensions in the emergence and growth of infrastructure can make visible infrastructures that otherwise tend to disappear. The communication from the European Commission, for instance, remarked that the ‘EU’s architecture of data management for border control and security is marked by fragmentation [...] caused by the various institutional, legal and policy contexts in which the systems have been developed’ [@europeancommissionCommunicationCommissionEuropean2016, p. 3]. Similarly, the work of proto-infrastructure scholar Thomas P. Hughes [@hughesChapterReverseSalients1983] on the growth of the electrical supply network, was influential to show how the growth of systems is not ‘fore-ordained’ but evolves in a complex social and technical environment. Hughes showed how systems often arise out of the difficulties inside existing technical systems, or what he calls ‘critical problems’. Critical problems get identified when there are so called ‘reverse salients’, problems with specific components that hold back the overall system as it evolves to realize certain goals.[^reverse-salients]. Following the EC communication, the existing ‘architecture of data management’ is seen as a critical problem that holds back further developments for information systems at EU level.

[^reverse-salients]: His metaphorical concept imagines systems moving in a battle line towards a goal with specific components as inverted salients that hold back the overall development.

As @bowkerSortingThingsOut1999 have pointed out, choices made during design processes easily become invisible, as only what is in the systems is usually (and at best) documented. Infrastructural inversions can thus help understand the choices made in the various complex settings for such ‘architecture of data management’. This is crucial, as infrastructures connect different contexts to large-scale networks that bridge these differences to allow for exchanging and making use of information collected at various local situations [@dijstelbloemBordersInfrastructureTechnopolitics2021]. Understanding choices made in the EU information systems would make it possible to uncover tensions in linking people’s data from different systems. As scholars of digital infrastructures have shown, the networks that connect and shape relations between a multitude of actors such as individual, states, companies, technologies are often sources of tensions in the developments toward certain goals[@edwardsUnderstandingInfrastructureDynamics2007; @ribesLongNowInfrastructure2009].

Despite these insights on the invisibility of infrastructures, scholarship on security shows a tendency to primarily refer to the (in)visibility of humans. Less attention has been given to the invisibility of those data infrastructures that allow making human mobility, migration, and borders visible. This is a key aspect, though. The technical mechanisms used to link people’s data within and across are key for tracking people and detecting patterns. People on the move become visible through infrastructures that are not neutral, but enact them in certain ways [@pelizzaProcessingAlterityEnacting2019]. Passenger screening systems, for example, will try to identify passengers against police watchlists and detect suspicious patterns in someone’s travel history. The passenger date models and rules to detect such patterns embedded in the infrastructures can be conceptualized as ‘scripts’ through which security subjects are enacted.

Since the 1990s, the question of how to make visible infrastructures has primarily been addressed by approaching infrastructures as a ‘relational concept’ [@starStepsEcologyInfrastructure1996]: what is infrastructure for one person might be a topic for someone else. Such relational approaches maintain that infrastructures are not objects of study by itself, but show preference for ethnographic methods to study how infrastructures emerge through interactive processes and practices [@starEthnographyInfrastructure1999; @karastiStudyingInfrastructuringEthnographically2018]. Alternatively, an often cited article by @larkinPoliticsPoeticsInfrastructure2013 argues that there are degrees of visibility infrastructure. Large-scale infrastructures can also have a certain ‘aesthetics’, such as a state’s infrastructure project that is made prominently visible. In the case of EU systems, a similar example may be found in the grand project to make various EU systems interoperable to solve EU migration and security challenges. But I contend that this aspect is less important to understand the otherwise less noticed work done by security infrastructures for identity matching. I argue that there is, however, still room for other kinds of methodological contributions to further the analysis the technical minutae of matching identity data in transnational security infrastructures and beyond.

First and foremost, are the informational representations or semantic classifications used to identify people — i.e., the ontologies or data models implemented in the databases. Databases utilized at the border support authorities in collecting and linking different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g., @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. In this way, databases institute certain kinds of visions; yet the embedded politics and design choices can easily become invisible. We might thus ask how we can retrace knowledge representation choices, in order to comparatively understand how diverse systems — run by different authorities — enact people and how these diverse representation are matched.

### First inversion strategy: Informational representations and data matching

Data models and ontologies are entwined in digital infrastructures and practices, and institute certain ways of knowing and working [@bowkerSortingThingsOut1999; @hineDatabasesScientificInstruments2006; @lamplandStandardsTheirStories2009; @timmermansWorldStandardsNot2010]. Data models designed to represent phenomena can be considered as a technology that enact particular kinds of knowledge, organizations and practices [@bloomfieldVisionsOrganizationOrganizations1997]. The politics in the processes of designing these informational representations shape how information is represented and affect what becomes visible and consequently, what turns invisible. Applied to the field of border and migration management, this implies that data models do not only represent people at the border: they also make them knowable in specific ways. For example, family composition fields that can count up to five members assume the Western parental family model. Data models that foresee only female and male gender options assume migrants who are binarily sexualized. Even the choice of identifying people through fingerprints instead of study degree or family tree shrink the possibilities in the way they can be known [@pelizzaIdentificationTranslationArt2021].

The current, as well as the historical antecedents, of registration practices and use of database technology as tools to sort populations for controlling borders and migration has been widely researched, [@lyonSurveillanceSocialSorting2003; @torpeyInventionPassportSurveillance2000; @ansorgeIdentifySortHow2016]. In his book on ’The invention of the passport’, @torpeyInventionPassportSurveillance2000 for example recounts the long historical developments of how states came to establish their imperative authority to control movements of people. He stresses the role of documents such as the passport and related practices to register, identify and track people and their movements. With people moving between different territorial authorities, authorities introduced different forms of documentation and registration to distinguish between persons that are allowed access to and rights within the territory. These mundane artefacts such as passports and population registers do embed powerful data models to sort and control people.

More recently, database technologies have played an important role for states to surveil and control migration through data collection practices, allowing the collecting of different kinds of biographic and biometric data to make people and bodies ‘legible’. At the same time these technologies also have opened possibilities for ideas of sorting populations for different treatments. According to @vanreekumDrawingLinesEnacting2017, infrastructure developed for monitoring and controlling persons and border crossings can be understood as forms of visualization that make these phenomena known or visible. In their view, the schematized and abstract imageries of maps, flows, and border crossings that underpin the technologies and practices do not just make these things visible, but are in fact co-constitutive what borders and crossings are. And to make such phenomena visible van Reekum and Schinkel emphasize the role of documents and categories to render visible and infer borders and border crossings. What is less clear is how multifarious categories and representations collected about people, their movements are matched and linked together.

Database technologies are, as such, one technology in the infrastructures often declared to make people at the border visible, by allowing states to collect — and connect — different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g. @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. However, these examples of research on technologies on the alleged invisibility of people show a tendency to adopt authorities’ perspective. Less attention has been given to the invisibility of those information representation in infrastructures that allow the management of mobility, migration and border control. Therefore, what is less clear is how we can make visible these parts of the infrastructure to retrace what and how specific forms of knowledge are represented.

To retrace what might be lost or made invisible in the development of data models and standards we can compare different standards for similar phenomena. @cornfordRepresentingFamilyHow2013, for instance, have compared digital standards that have been developed as public service initiatives in the United Kingdom for representing of family relationships. Their comparison highlights ‘the kinds of family relationships that are recorded and those that are not recorded or harder to record, any hierarchies, implicit or explicit, for family forms or relationships and the implicit and explicit assumptions that underlie the terms and classifications used’ (p. 8). In our case this means comparing data models for representing persons that are embedded in different authorities’ information systems for migration and border control.

Data models and standards cannot always be compared separately, as often it is only through the interplay between different standards that subjects are enacted as new kinds of entities. @ruppertNotJustAnother2013, for example, has studied information systems in the UK for sharing data on young persons who are at risk of (re-)offending, and for allowing interventions. In her study she shows how the ‘joining up’ of data stored, which includes various biographical information from different agencies, produces a ‘subject multiple’. Drawing on the work of Jane Bennet, Ruppert sees the consequence of connected data as then not just from a sum of the individual parts, but that is a part a becoming of something else. In the case of border and migration control someone may for example only become an irregular migrant through connecting identity data from different databases.

The politics of data model design shapes how information is represented, and affects what becomes visible and, consequently, what becomes invisible [@hansethInscribingBehaviourInformation1997; @bowkerSortingThingsOut1999]. Discussing the invisibility of humans cannot avoid discussing the invisibility of those data infrastructures that define how they become visible. The first infrastructural strategy has therefore suggested that there is a need to examine infrastructures‘ informational representation and recover choice made in these technical standards that easily become invisible, yet are continuously involved in processes of enacting populations, territories, borders.

The rise of Big Data has prompted, among others, the field of Critical Data Studies (CDS) to renew the call to make visible other dimensions of data related to the volume, variety, and uses of data[@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. Such data practices are never entirely local and information technology, including standards, in this sense have become powerful methods to trace and connect sites [@latourReassemblingSocialIntroduction2005]. The next infrastructural inversion strategy will therefore propose to consider another important element to connect sites: the data practices related to data management and linking identities within and across organizations.

### Second inversion strategy: data practices

It is clear that data are never ‘raw’, but always constituted by different choices and constraints [@bowkerSortingThingsOut1999; @gitelmanRawDataOxymoron2013]. In addition, techniques and practices of managing and cleaning data should be analysed as part of the ‘cooking’. In recent years, there has been an increasing amount of literature that have shed light on the process in which data is generated, circulated, deployed [@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. Yet, scant attention has been paid to actual practices and techniques used to accomplish matching and linking data in security settings — such as probabilistic record linkage, fuzzy matching, data deduplication. This is surprising because by bringing different data into relation with each other, the tools recontextualize data and can have organizational consequences such as redistributing of tasks and responsibilities between different actors. There is therefore a clear need to understand how such technologies for matching identity data are put into practice.

Similarly, there is growing interest in more fine-grained analyses of knowledge production mechanisms and practices of digital technologies in border and migration management and how they enact, among other, populations and states [@mcharekTopologiesRaceDoing2014; @jeandesbozSmarteningBorderSecurity2016; @glouftsiosGoverningCirculationTechnology2018]. Contributions from two recent Special Issues [@cakiciPeoplingEuropeData2020; @scheelEnactingMigrationData2019], for instance, took a practice based approach to analyse data practices and pay attention to the mechanisms through which knowledge enacts migration. And through the concept of ‘alterity processing’, @pelizzaProcessingAlterityEnacting2019 has provided a framework to account for the multiplicity of ways in which non-European populations are enacted when reaching Europe, and their simultaneous enactment with institutions through data practices and infrastructures. As such, these different authors question the kind of worlds the data practices help to make — and potentially engage with ontological politics to think how they could be enacted differently [@lawEnactingSocial2004].

Overall, examining the role of data matching practices and technologies is essential to understand much contemporary coordination work within information infrastructures [@leeHumanInfrastructureCyberinfrastructure2006; @ribesSociotechnicalStudiesCyberinfrastructure2010; @monteiroArtefactsInfrastructures2013]. That is to say, in the case of data matching practices, the activities and technologies that make data findable, shareable, connectable are frequently crucial to make possible distributed cooperative work. In the context of scientific work, for example, scholars have shown the — often invisible — efforts needed to make data sets shareable and useable by others [e.g., @edwardsScienceFrictionData2011; @kervinBackstageWorkData2014; @plantinDataCleanersPristine2018]. A data practice approach thus makes it possible to empirically explore how data quality practices and technologies support and shape data practices in collaborative work within and between organizations.

The move to situated data practices using concepts from practice theory still leaves some things unresolved. Rather than technologies, people or other actors, practice theories takes social practices as the central topic of inquiry [@reckwitzTheorySocialPractices2002; @schatzkiIntroductionPracticeTheory2005; @shoveDynamicsSocialPractice2012]. Only recently have the materiality of technological artefacts been incorporated as ‘aspects of practice-arrangement nexuses’ in which the materiality and practices are intertwined and constitute each other [@schatzkiMaterialitySocialLife2010]. Taking data matching practices as an analytical starting point can focus attention on the performativity of practices as they are situated and entangled with technological artefacts [@ruppertDataPracticesMaking2021]. Analysing data practices, such as the processing of an asylum application based on information collected about a person by different organizations and agencies, can then highlight how tools are supporting this work to link data from different sources. At the same time, such data practices of cleaning duplicate data may perform certain identity data as irregular. But while practice theory has been very productive to understand the situatedness of practices, debate continues about how social practices travel, how different practices are related to each other and other ’large phenomena’ [@shoveMattersPractice2016; @nicoliniSmallOnlyBeautiful2016].

Following @schatzkiIntroductionPracticeTheory2005, practice approaches can either elaborate on the various interconnected practices of a subdomain of human activity, or consider practices as ‘the place to study the nature and transformation of their subject matter‘. This latter view is supported by @shoveDynamicsSocialPractice2012 who proposed that ‘the dynamics of social practice’ can be useful to understand social changes by looking at the change and stability of practices. In this way, I propose that examining the dynamics of data practices for matching identity data can be useful to understand developments of identification in security settings. Operationally, these dynamics of data matching practices can be examined by investigating the dynamics of data practices: how they change, stabilize, travel.

Analysing the dynamics of data matching practices can show how technologies are intertwined with these practices to shape, transform, stabilize identification in transnational security infrastructures.  The second infrastructural inversion strategy to make visible the matching identity data in transnational security infrastructures foregrounds usual backstage elements as work practices of managing, linking, cleaning data. Difficulties arise, however, when an attempt is made to understand how the entangled technologies themselves have evolved and moved across contexts and organizations. The third inversion strategy will thus focus on how to make visible the processes of social shaping of technologies.

### Third inversion strategy: lifecycle of technologies

STS scholars have for a long time chronicled the choices and directions of design of technological artefacts and systems through different approaches to examine and understand the nexus between users, technologies, and designers. Social constructivists, in particular, have emphasized the social construction of technological artefacts (rather than human action determined by technologies) [@mackenzieSocialShapingTechnology1985; @pinchSocialConstructionFacts1984]. This scholarship accordingly emphasized how the construction of technologies is tentative and shaped by specific contexts and actors in which the artefact emerges [@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. Concepts such as ‘interpretative flexibility’ [@pinchSocialConstructionFacts1984] and ‘boundary objects’ [@starInstitutionalEcologyTranslations1989] emphasized how different social actors and groups attribute different meanings to artefacts and how technologies are put to use. Likewise, these concepts are crucial to analyse and contrast how the design and evolution of the technologies and how actors perceive, use, configure the tools for searching and matching data.

Scholars have proposed various concepts to compare the development of technologies with how actors use technologies. Concepts such as ‘script’ have been introduced to account for the gaps between designed intended uses (a ‘script’) and actual uses. While notions such as ‘configuration’ emphasize how expectations and imaginaries of designers are inscribed into technological artefacts, and how they users and artefacts are figured together — delimiting and making possible certain forms of agency. However, critics question the ability of some of these approaches to account for the longer-term design and evolution of technologies [@williamsMovingSingleSite2012]. This scholarship argues that technologies cannot studied through a single ‘snapshot’ in time. Rather, they argue that there is a need for a kind of ’biography of artefacts and practices’ [@pollockSoftwareOrganisationsBiography2009].

In summary, and drawing on these literatures, there is a need to investigate data matching technologies and practices in a way which allows taking at same time a situated and a more longitudinal approach to investigate how they are shaping and shaped by transnational security infrastructures. Looking at different moments in the life of the data matching technology and practices should be addressed.

## A conceptual framework for analysing data matching in distributed settings

Following the overview of existing work and the gaps in their methods, I now put forward a conceptual framework for tracing data matching practices and technologies in transnational security data infrastructures. Such a framework is needed since no single method or theory can be adapted to examine the multiple facets of data matching practices and technologies within and across organizations. In this section, I therefore propose a new conceptual framework that combines established methods and theories into one framework that can guide the research in examining those multiple facets. The conceptual framework, in essence, integrates more computer science views on linking and matching data models and records, with social science views from fields associated with Science and Technology Studies (STS).

As previously said, for government agencies, business, and other organizations the tasks of finding records in databases that refer to the same person can be important in many situations. As organizations, states, agencies have been collecting ever more data about people, they may perceive performance benefits of having complete and accurate information about people, or even see it as beneficial to national security. The techniques for data matching — such as the mathematical models for linking records — are therefore well established. However, the creation, development, and consequences of deploying technologies for data matching are less well understood and researched. With the framework that I outline in this section I aim to address this gap in the research. I propose to trace data matching practices and technologies as sociotechnical phenomena, and from four different angles: (1) how the kinds of data that are collected about people can be compared and what this tells us about how organizations can search, match, and use this data, (2) how within organizations search and match for data about people that may have data quality issues, (3) how such identity data is matched between organizations, and (4) how, simultaneously with matching identity data, data matching technologies and practices travel and evolve across organizations.

To discuss these different aspects of analysing the matching of identity data, I propose to visualize the framework through several axes (Figure \@ref(fig:method-axes-visualization)). This visualization roughly follows a relational model to represent data as tables that model relationships between data. The three axes capture different dimensions of data, which are: _data models_, _categories of data_, and _data values_. Next, I will describe how each of these three axes and how different relations between the axes correspond to specific aspects of data matching within and across organizations.

```{r method-axes-visualization, echo=FALSE, fig.cap="A visualization of the different aspects of data matching examined through the framework.", out.width="100%", fig.align="center"}
knitr::include_graphics("figures/method-axes-visualization.pdf")
```

To begin, the axis for _data models_ (axis `z`) represents the abstract models that standardize the kinds of information about people collected by different organizations. An example of such a data model could be the schemas specifying data collected about migrants by a government agency. Each organization generally defines its own data model. However, some data models might even be shared to make interfacing between systems possible. Next, each data model specifies _categories of data_ (axis `x`) which are collected about people. For example, the categories of data of the example asylum agency’s data model may include ‘name’, ‘place of birth’, ‘date of birth’. Finally, there are the actual _data values_ (axis `y`) in systems’ databases for a data model and its categories of data. For example, a value for the category of data ‘place of birth’ may be ‘Brussels’.

That being so, I propose to schematize different facets of data matching as combinations of pairs of these axes. In Figure \@ref(fig:method-axes-visualization), three axes each zoom in on specific facets of data matching within and across organizations.

### Matching data models

The first pair _(data models `z`, categories of data `x`)_ represents the identification of semantic similarities and differences between different data models and their categories of data. Such a process will also be used in the research approach to support discursive analyses of data models. In this way, the analysis intends to compare the expectations and imaginaries of diverse actors concerning data collected about people and the ways data can be searched, matched, used. For example, the data model of a government agency may contain the category of data ‘family name’ while another agency uses ‘surname’ as category of data. Even though the name of the categories of data are distinct, they clearly refer to a similar concept. In this case, a part of someone’s personal name that indicates their family. To connect information about people from these two agencies’ systems should therefore identify similarities between their data models and these two categories of data.

This first aspect of matching people’s identity data that is addressed by the conceptual framework is therefore the relations between different information systems’ data models. That is, how different data models for collecting information about people-on-the-move are situated in relation to one another, and what this tell about the organizations which developed and make use of these models to collect, search, and match identities.

To do this analysis, the method draws, first, on studies of classification and its consequences which have addressed the question of how to retrace the ethical and political work of otherwise mundane devices of representations — i.e., data models. Researchers have previously used and combined various makeshift methods — from narrative interviews [@gazanImposingStructuresNarrative2005] to discursive textual analysis [@caswellUsingClassificationConvict2012], from participant observation [@meershoekConstructionEthnicDifferences2011] to archival and genealogical research [@gassonGenealogicalStudyBoundaryspanning2006] — for the ethnographic and historical studies of information systems [@starStepsEcologyInfrastructure1996] and their classifications. Furthermore, it is altogether necessary to take a more interdisciplinary approach and also pay close attention to the technical details of classification, as to avoid only considering the effects of classification [@kitchinCodeSpaceSoftware2011].

Following Geiger and Ribes [-@geigerTraceEthnographyFollowing2011], data models can be considered ‘thin’ traces: being rather standardized schemas made of categories and values, they are hardly meaningful in themselves. The method to analyse them therefore should consider how to turn them into ‘thick’ data. Nevertheless, given their implementation in diverse infrastructures, data models can be an excellent starting point to understand how geographically distributed sites are connected [@latourReassemblingSocialIntroduction2005]. Burns and Wark [@burnsWhereDatabaseDigital2020], for example, have dubbed such approach ‘database ethnography’ and used traces left behind of a database as a site to analyse how social meanings of phenomena change over time — but, only using their own ad-hoc mix of methods.

On the other hand, matching data models shares concerns with methods developed in computer science fields interested in the use of semantic technologies, such as knowledge engineering, linked data, and natural language processing. For example, the relationship between the different types of categories and their variations is closely linked to the outcome of processes known as schema or ontology matching, which finds correspondences between concepts of schemas or ontologies [@euzenatOntologyMatching2007; @kementsietsidisSchemaMatching2009]. Another linked technique called ontology learning aims to automate processes for creating ontologies. These (semi-)automatic mechanisms then include extracting concepts and relationships between concepts from a domain of discourse by analysing a corpus of documents in that domain. There are thus shared concerns about how to examine the way different knowledge representations interrelate, and about how to recover such representations from a discursive domain. Yet, there are fundamental differences which make the methods hard to effectively support a discursive analysis.

Methods of knowledge engineering are generally more concerned with creating technologies that help machines understand better means for the systems, and with integrating different sources of information work together. By contrast, the method for this research needs to support discursive analysis by taking differences in representations as a starting point to analyse imaginaries of diverse organizations and authorities and to combine the analysis with ethnographic observation. To address this aspect of data matching there is therefore a need for approaches that can support analyses of formalized data models in two ways. First, to support analyses of information systems which define their own data models, even if these systems are not immediately comparable. Second, to systematically and quantitatively support discursive analysis of ‘thin’ data models, also by detecting differences and absences between systems.

### Data matching within organizations

The second pair _(categories of data `x`, data values `y`)_ relates to the categories of data and their actual values in databases, which may not always correspond to the expectations of the data models. For example, when values for the categories ‘first name’ and ‘last name’ are mistakenly switched in the database. Practices and technologies may therefore be employed to remediate such data quality issues — i.e., data matching. Overall, two applications of data matching within organizations and their databases can be distinguished: real-time data matching and deduplication. Real-time data matching mainly addresses the issue of retrieving information about a person through search queries. For example, police officers may need query databases based on the categories of data ‘name’, ‘nationality‘, ‘date of birth’ and to see if approximate matches exist for those personal details. Data matching techniques will therefore be used to find similar matching identities’ data, rather than only exact matches. Deduplication, on the other hand, is the use of data matching technologies for identifying records in a database that refer to the same person and fusing the multiple records.

On one hand, it is crucial to understand the moments when actual data are not aligned with the expected data model. For example, when multiple records were created for a person due to name variations. In this way, the relation between data expected by data models and actual data in databases can draw attention to the data practices of how organizations collect and use knowledge about people-on-the-move. For example, how they choose to take up technologies for searching and matching to help deal with data uncertainties deriving from frictions between the moments of collection and use of data. On the other hand, the design and use of these tools for dealing with such data matching need scrutiny. The design and use of such data matching tools can be compared to understand the relations between users, designers, and technologies. Such comparisons can identify expectations and imaginaries of designers that are inscribed into technological artefacts and how they are all configured together.

Scholars in fields of STS and beyond have devised different approaches to examine and understand the relations between users and technologies [@oudshoornHowUsersMatter2003], and how expectations and imaginaries of designers can be inscribed into technological artefacts. For example, the approach of script analysis was advanced to examine the assumed competences of users and affordances that are embedded in artefacts [@akrichSummaryConvenientVocabulary1992; @latourWhereAreMissing1992]. The script of an artefact then requires users to adopt designers’ envisaged behaviours and actions to interact with an artefact. This approach also allows accounting for situations when those assumptions by artefact designers don’t match the actual uses and practices. @woolgarConfiguringUserCase1990 furthermore noted how system designers might attempt to ‘configure the user’ of the system by incorporating the user into the sociotechnical system. The method to understand such configuration is by treating the computer systems as a text that is read by users and can be interpreted differently. Designers may thus attempt to anticipate and delimit this flexibility. In these views, a sociotechnical system functions well when users and the system are successfully configured together.

Suchman [-@suchmanHumanmachineReconfigurationsPlans2007] furthermore argued that the inscriptions of users and uses are never that coherent, and that a more open-ended and indeterminate approach towards artefacts is required. She proposes configuration as a ‘method assemblage’ that pays particular attention to technologies and ‘the imaginaries and materialities that they join together’ [@suchmanConfiguration2014, p. 48]. Configuration as a methodological device can thus be, in accordance with Suchman’s view, useful to unpack the material and discursive elements of software for dealing with data uncertainties.

This axis in the conceptual framework therefore aims to answer the question of what kinds of imaginaries and materialities are joined together in data matching technologies. Analysing and comparing the technical designs with actual uses, and thus also where they mismatch, can give evidence of the various imaginaries, expectations, assumptions at play. Yet, the approach also needs be both specific to the situatedness of the design, and keep an open-ended and indeterminate view of how such technologies are put to use [@suchmanConfiguration2014].

### Data matching across organizations

Finally, the pair _(data models `z`, data values `y`)_ addresses data matching across organizations and agencies. Identity data records that refer to the same person may exist in systems and databases of different organizations and government agencies. Data matching in this case refers to the data practices related to identifying and potentially linking or merging such identity data spread across various databases. A crucial challenge for data matching techniques is due to the fact that unique identifiers are not always available and, hence, that matching relies on personal information which is not always complete or accurate. Personal information such as names, dates of birth frequently contain typographical errors and variations. For example, a woman may have used her maiden name as surname in one system while information collected in another database uses her husband’s surname after marriage. However, the question for us here is how — by bringing such data in relation with each other — data matching practices and technologies shape the relations between different actors.

Furthermore, these data matching practices and technologies cannot be studied independently of the infrastructures they are embedded in. Data matching technologies can be crucial to make certain collaborative work possible, such as the processing of an asylum application based on information collected about a person by different organizations and agencies. At the same time, the work of cleaning and managing data often can become invisible.

Based on the insights from infrastructural studies, data matching technologies in fields of security can therefore become an inconspicuous element in transnational data infrastructures. This in line with current debates in fields of International Relations (IR) and Security Studies who have started to acknowledge the role technology international politics [@amicelleQuestioningSecurityDevices2015; @hoijtinkTechnologyAgencyInternational2019; @bellanovaAlgorithmicRegulationSecurity2020]. Crucially, these scholars extend agency to the technologies and artefacts used in security practices. Glouftsios has, for example, shown how the often-invisible maintenance for the large-scale information systems for border security in the EU has a crucial role in sustaining the governance of international mobility [@glouftsiosGoverningBorderSecurity2020].

In the case of data matching technologies, the underlying assumption is that such technologies can trigger or even settle controversies about which actors produce more reliable data. For example, when matching identity data from different agencies these tools will bring certain records in relation with each other. If there are uncertainties about the data, they may then play a role in disputes about which agency’s data are correct. The hypothesis is that data matching technologies can thus inscribe certain forms of agency that can reconfigure practices of security and migration in transnational data infrastructures. Infrastructural inversions are therefore key to calling attention to the background or invisible work of mundane data management practices.

### Travelling data matching software

Furthermore, and related to the last pair, is the question of how not only identity data are matched across systems, but also how technologies and practices for data matching travel across organizations. As companies develop tools to support these tasks and deploy them at multiple locations and times, the concomitant knowledge of how to manage typographic errors, name variations, missing data, and so forth may also travel across organizations. This axis is consequently related to how data are not only matched between different organizations and agencies in security contexts. Rather, the hypothesis is that, at the same time, knowledge and technologies for such data practices travel between organizations.

Although the notions of ‘script’ and ‘configuring the user’ have been generative in understand the relations between users and technologies, they tend to overemphasize the role of designers [@oudshoornHowUsersMatter2003]. More recent approaches have therefore suggested to, for example, take a more comprehensive approach to examine important moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. An alternative approach has been developed by scholars called the ‘Biography of Artefacts and Practices’ which can detail the complexity of development of such technologies and related practices.

Following this approach, the emphasis of this aspect in the conceptual framework is to understand the biography of a data matching software packages. This approach is thought to highlight the wide range of actors involved in the development of such security technology and how the software travels and evolves over extended periods of time and between in different locales. This expands the narrower focus of the user-design-technology nexus used in the second axis to take a more comprehensive view of how the technology has been shaped over time, and by a broader number of actors at multiple sites.

The conceptual framework served as a heuristic for tracing data matching in transnational security data infrastructures. A combination of methods was employed to operationalize the framework for the different axes. To research this topic I took advantage of well established methods from the areas of Human–Computer Interaction (HCI), Computer Supported Collaborative Work (CSCW) and sociotechnical research to investigate the relations between the technologies, users, and organizations. Hence, I employed methods such as document analysis and user interviews for data collection and data analysis. Furthermore, these methods informed each other at different points throughout the duration of the research.

<!-- > _To-do_. This section will give an overview of how the conceptual framework will be applied and tested. Essentially, I will introduce how each axis of the framework is operationalized. The methods to do this are then further detailed in the following sections. -->

## Fieldwork site and access

### WCC ELISE — A technology for matching identity data

In approaching the company WCC, I presented my intent to research the use of their software package ELISE as beneficial to both the interests of the Processing Citizenship and PhD research projects, and the interests of WCC and its customers. First, relevant aims of the PhD and PC projects include exploring how information systems for identity management support the production of knowledge about non-European populations and the challenges encountered. The technologies developed by WCC — and deployed at government agencies and (international) organizations concerned with identity and security — were therefore identified as an entry point to learning about such processes of knowledge production. As such, studying the design and real-world use of this software technology was thought to provide important insights to understand technologies for identity management in security settings, and related challenges of data matching within and across organizations. Second, findings of the research were thought to provide better understanding for WCC and their customers into the actual use of their technologies. Such insights can be used for further development of their technologies.

My proposal for research focused specifically on deployments of the ELISE ID platform — previously also known as ELISE Smart Search & Match — at customers in WCC’ security and identity domain. The ELISE software solution serves in fact as a core technology in all solutions developed by WCC, ranging from products for border management to public employment services. In short, the platform provides sophisticated functionalities for data matching such as searching and matching of personal identity information for different data sources. The novelty of the technology comes from using various kinds of fuzzy logic algorithms for data matching that consider that data may be incomplete or inaccurate. Here, fuzzy logic refers to a form of logic in mathematics where truth variables are calculated in probabilities instead of only being ‘true’ and ‘false’. Search results in the ELISE ID platform are consequently returned as values indicating the probability that two sets of identity data can be considered identical — i.e., match.

In short, the overall goal of the fieldwork was thus, first, to understand how organizations do not simply use the ELISE ID platform and its identity matching organizations. Rather, the leading hypothesis was that embedded assumptions in the tools to a certain extent shape the organizations — such as how users think about data, the uncertainty of establishing identity and, hence, how this shapes their work practices. Through my fieldwork I intended to find evidences of such dynamics by comparing the design of the tools with how different users use the biographic matching and multicultural name matching capabilities in their daily tasks when searching for identities. Such a comparison was thought to bring out how users’ expectations about data — and uncertainty surrounding data — are influenced by the software solutions. That is, how these expectations may be the source of tensions in imagined and actual use of the search and match functionalities. In this way this first goal would provide evidences for the second axis of the conceptual framework — the design and use of data matching within organizations. I therefore draw inspiration from STS approaches to study the possibilities, constraints, and resistances of devices embedding prescriptive representations [for an overview see @suchmanHumanmachineReconfigurationsPlans2007].

The second goal of the fieldwork was to understand the processes and practices of developing and deploying such a software package for matching identity data. This corresponds to the fourth aspects of the conceptual framework, i.e., that the knowledge and technologies for matching identity data travel and circulate. The hypothesis was therefore that the software for searching and matching identity data would embed certain imaginaries and assumptions that evolved over time and while circulating and interacting between different actors, such as the company, their customers, competitors, and other organizations. For this second goal I aimed to identify important moments in the development and deployment of the software package. For example, it is crucial to understand the work performed to make such matching of identity data work across different contexts and organizations. That is, we should question exactly the generic nature of the software package and instead focus on the struggles and accomplishments to make it so. This second, as such, draws upon the perspectives developed by ‘biographies of artifacts and practices’ (BOAP) framework urges to understand technological development as a complex and dynamic process happening across time, locales, and involving various actors [@hyysaloMethodMattersSocial2019].

To summarize, the overarching hypothesis leading this fieldwork research was thus that this particular form of searching provided by the solutions — taking for granted uncertainty about data — incorporates certain imaginaries — a ‘smart search and match’ — that shapes and is shaped by users, their work practices, and the organizations in particular ways. Tracing such complex dynamics then requires taking a simultaneously more and less specific perspective [see also @suchmanConfiguration2014]. More specific in the sense of looking at specific moments of use and development of the tools for searching and matching. And less specific in a sense of taking into account the often unpredictable and uncertain actors shaping the software.

### Deployments at the IND and EU-VIS

To test the overarching hypothesis, it was decided to investigate the deployment of the solutions at two customers of WCC’s software solutions in both national and transnational settings. First, the integration of the ELISE ID platform with the systems of the immigration and naturalization service of the Netherlands (IND). Second, the integration of ELISE in the EU Visa Information System (VIS). As mentioned previously, we should however be careful in distinguishing between the national and the transnational. The social methods employed for this inquiry do not only describe realities, but they are a part of the ‘ontological politics’ of enacting certain kinds of realities [@lawEnactingSocial2004]. My aim is therefore not to reproduce fixed distinctions between national, transnational, or international. Following Law and Urry, the question is which realities I want to enact. The choice, in this case, is rather a hotchpotch of actors and technologies without clear boundaries of national or transnational.

Furthermore, for both cases the use of the search and match solution were deemed relevant to understand how the tools support and shape the practices of knowledge production related to the identity of people. In the case of the IND, the agency is responsible for, among others, processing the applications from people who want to stay in The Netherlands or who want to become Dutch nationals. The use of the ELISE relates to these responsibilities. The IND information system leverages the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system. The EU-VIS system, on the other hand, has a narrower scope in its use of biographic matching and multicultural name matching capabilities for its search engine. At the same time, due to its transnational scope, the amount of records in the EU-VIS database is much higher.[^eu-vis-capacity]

[^eu-vis-capacity]: The ‘Report on the technical functioning of the Visa Information System (VIS)’ [@eu-lisaReportTechnicalFunction2020] notes a database capacity of 60 million records.

The methodological choice of tracing the development and deployment of the ELISE software package for these two organizations necessarily leads to certain ‘framing effects’ [@hyysaloMethodMattersSocial2019]. The software solutions of WCC have been deployed at other customers such as other government agencies and in different fields (e.g., public employment services). However, the other customers were not possible to research due to issues such as confidentiality. The other fields were at first identified as not immediately relevant, but came up at several moments during the fieldwork and interviews in discussions about the genericness of the search and match. In future sections I will return to how the genericness of the product to work across domains is accomplished. Finally, there are other actors such as competitors and other potential customers that have a direct and indirect influence on the company and its products. Subsection \@ref(events) and \@ref(other-observations) describe how I included these peripheral actors in the fieldwork observations by, for example, attending industry events and relying on other sources.

## Methods for data collection

### Traces of data models

An important aspect of investigating the matching of identity data across organizations is how data models of different systems correspond to one another. For matching persons’ data within and between organizations some correspondences and similarities in the types of information stored in the respective databases need to be found. For example, two systems may store a combination of persons’ last name, date of birth, nationality and used to match separate identity records; but with distinct data models and variations in attribute labels, types, formats. Such data models and their categories of data, i.e., the labels describing a state that can assume different values, can be described in various kinds of documents: from database schemas to design documents, or even in regulations. Here, I will describe only the processes of collecting such traces of data models that were used for the analysis. In general, such traces of data models can be collected through desk research or fieldwork. In the next chapter I will detail how such diverse descriptions of data models were made comparable through additional steps to code, harmonize, and group all documents, categories, and values.

All those steps contributed to the development of a method and tool to compare data models utilized in national and international information systems that jointly work to support registration and identification practices at border zones in Europe. Regarding the traces of those data models I draw on data which were collected during fieldwork conducted in the context of the Processing Citizenship project at border zones in Europe. Given linguistic constraints and the Project’s task plan organized as a matrix, some documents were collected by other researchers employed as collaborators in the Processing Citizenship project.[^pc-team] Overall, the data collection efforts included desk research of European regulations, technical documents made available by European and German authorities, systems screenshots collected at border zones in the Hellenic Republic, and technical documents collected during fieldwork in The Netherlands.

[^pc-team]: These researchers include: A. Bacchi, E. Frezouli, Y. Lausberg, C. Loschi, L. Olivieri, A. Pelizza, A. Pettrachin, S. Scheel.

For transnational security infrastructures such as the information systems developed by European Commission agencies in the Area of Freedom, Security and Justice (AFSJ), the issue of data matching is becoming increasingly important. Ongoing projects to increase information-sharing and interoperability of information systems in this area necessarily demands various new forms of data matching. Broadly speaking, various international actors are developing a central infrastructure to store links and references to EU information systems. In this new architecture, identity data from previously separate systems will be matched against each other to link them and with the aim of facilitating identity checks and detecting identity fraud. Besides these new types of data matching under development, the existing systems already do real-time data matching such as when searching for personal details of a person by a police officer. To explore the relations between the categories of data recorded in these three EU systems I draw upon the regulations establishing each system.

Data collection therefore includes documents of the regulation of the three international information systems developed by European Commission agencies: Eurodac, the Schengen Information System (SIS) and the Visa Information System (VIS). All three systems have specific aims in supporting policing tasks related to travel, cross-border crime and irregular migration. Eurodac (_European Dactyloscopy_) aims to support the identification of asylum seekers through fingerprints, and to determine the Member State which is responsible for processing their asylum applications in the context of the Dublin System.[^dublin-iii] The database was established in 2003 to store fingerprint and other basic data of asylum seekers. It is currently operational in the Member States of the European Union (plus Norway, Iceland, Switzerland, and Liechtenstein). Since its original inception, the use of the system has been extended to law enforcement authorities and Europol [@ajanaAsylumIdentityManagement2013]. New proposals aim to include more biographic data and additional biometric data, and a facial image.[^recast]

[^dublin-iii]: The Dublin System (Regulation No. 604/2013; also known as the Dublin III Regulation) establishes the criteria and mechanisms for determining which EU Member State is responsible for examining an asylum application.

[^recast]: Procedure 2016/0132/COD, recast of the Eurodac Regulation.

The purpose of the Schengen Information System (SIS II) is to support external border control and law enforcement cooperation in the European Union. It supports this task by storing alerts which contain information on persons and objects, and information/instructions on what to do when such persons or objects are encountered. This information can then be exchanged between law and border enforcement authorities through the SIRENE network. Finally, the Visa Information System (VIS) allows for exchange of visa data (including personal data and biometrics) and in this way it aims to maintain a common EU visa policy. Checks on the VIS data are done in the context of identification procedures at the borders. This kind of access is different from the previous systems, as the data can only be accessed on a case by case basis. Authorities at the external border of the European Union also have access for purposes such as ‘verifying the identity of the person, the authenticity of the visa or whether the person meets the requirements for entering, staying in or residing within the national territories’ [@europeancommissionVisaInformationSystem]. Furthermore, data in VIS can be used by asylum authorities to determine which EU Member State is responsible for examining the asylum application.

Concerning national systems, the analysis includes the systems of the Dutch (DRF), Hellenic (HRF) and German (GRF) Register of Foreigners. The DRF is an information system stores all basic data of people who have a relation with the Dutch government in the context of the ‘Vreemdelingenwet’. The system allows digital exchange of identity data, information about travel and identity documents, biometric characteristics and status data between the partners. Various ministries and organizations in The Netherlands use this system to share and consult information about foreign nationals. As such, it acts as a kind of single, authoritative source of information (_basis registratie_ in Dutch). That is, although other partners may have their database and information about a person, the data needs to be kept aligned and the DRF leads [@InformatievoorzieningVreemdelingenketen2015]. This can also be seen due to the fact that through the BVV all foreigners receive a unique identification number — the v-number. In practice then, processes of establishing identity involves an interplay of multiple systems. For example, IND users searching for person matches always need to consider result from this system as well.

The HRF is the main system used at border zones in Greece to identify and register persons who arrive at the border without the required documents. The system is used to support different tasks during the identification and asylum procedures: from retrieving migrants’ biographic and biometric data, to conducting screening and asylum interviews, to assessing health conditions. Users of the systems therefore include police, administrative personnel, and asylum officers.

The GRF is a German database which contains a large amount of personal information of foreigners in Germany who have or had a residence permit, as well as those who seek or have sought asylum or are recognized asylum seekers [@bvaAuslanderzentralregister]. This central register is accessed by various partner authorities and organisations in fields such as asylum, migration, border control. The data sent to the GRF during the first registration is described in the XAusländer standard, a data exchange format which formalizes and enables data exchange between the immigration authorities in Germany [@bamfStandardXAuslander]. According to the description of the standard’s motivation, it aims to facilitate exchange of such data between authorities in Germany, in order to reduce data re-entry, and to enable reuse of such data by the authorities.

All the information systems described above define their own data models and the kinds of people they aim to represent, in ways that are not immediately comparable. In subsequent sections I will therefore introduce the method developed for extracting, analysing, comparing and visualizing diverse data models.

### Document analysis

My analysis of documents related to the deployment of ELISE is grounded in understandings developed by STS of documents and practices of documenting as objects of study. As @shankarRethinkingDocuments2016 explain, different STS scholars broadened the understanding of documents as artefacts that don’t just document and stand for something in the world. Rather, documents play a role in social contexts — such as accounting for and coordinating of workplace activities — and cannot be separated from the practices through which they are produced. For this document analysis, I therefore looked not only at documents that help explain how the software packages function and how they were configured and deployed. Instead, the documents also ‘refer to the practices, objects, rules, knowledge, and organizational forms that produced them’ (ibid., p. 62). Such a document analysis can thus contribute an understanding of the key biographical moments in the life of this software package.

At a first stage, I began the research by studying documents related to, first, the more generic technical details of the ELISE ID platform. And second, documents related to the specific implementation of the platform at the IND. WCC provided these documents — such as technical design documents and meeting minutes — during several on-site visits at their head office in Utrecht, The Netherlands. Members of the WCC ID team provided me additional context for the documents along with updates on the current status of the implementation at the IND.

In general, I distinguished between two broad aims of the document analysis, corresponding to two different aspects of the conceptual framework. A first aim was to find evidences of frictions between design and use of the tools by comparing how different users at the IND use the biographic matching and multi-cultural name matching capabilities in their daily tasks when searching for identities. Such a comparison was thought to help answer RQ3 by understanding how users’ expectations about data — and uncertainty surrounding data — is influenced by the software solutions. That is, how these expectations may be the source of tensions in imagined and actual use of the search and match functionalities. As I will explain further in Chapter \@ref(ch-dm-within-org), there are indeed differences between the designed probabilistic identity match, and user’s understanding of, for example, the search results.

The second aim of the document analysis was to unearth important moments in the trajectory of the software package. Key insights from scholars in this regard are that instead of looking at single versions of document, we should pay attention to the processes of documenting [@shankarRethinkingDocuments2016]. For example, by looking at different versions of the document — changes within them, added annotations, and so forth. In the case of the deployment of ELISE at the IND, I found different versions of the design document, documents describing updates to the package, and meetings notes discussing changes to the configuration. However, documents generally do not have a clear provenance. In encountered plenty of ambiguity and uncertainty in knowing why certain documents were created. This was especially the case for the documentation of ELISE at the IND. Most of those documents date back a decade ago and people involved with the project are no longer part of the organization.

In order to understand how knowledge and technologies for matching identity data travel and circulate, documents should be understood as assemblages of different authors and sources. As such, documents do not have an easily identifiable origin but are ‘assembled from multiple sources [where] content often flows from application to application and document to document, constantly recycled, reworked, and repackaged’ (ibid., p. 63). Such flows are easy to recognize in the ELISE documentation for the IND. Technical documents incorporate generic information about the package from other company documents with details specifically for the IND implementation. In this way the genealogy of the package becomes evident — how it has moved and evolved when moving across domains and organizations.

Finally, an important — but less visible — aspect of documentation is missing or unavailable documents. During my fieldwork it became evident that documents for the deployment of ELISE in the EU Visa Information System would not be available for me. This is due to specific configuration of actors in the development of the EU-VIS systems. In this configuration WCC acted as the technology supplier and worked together with a technology integrator, Accenture. This meant that relevant documents are in the hand of this integrator. What’s more, the technology integrator acts as sort of gatekeeper which makes it impossible to access such materials. This lack of documents therefore also is revealing of the practices of deploying such packages.

In short, studying the documents allowed me to, on one hand, form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents give insights in the genealogy of the package and the practices of configuring, deploying, designing the software. Table \@ref(tab:documents) gives an overview of some these most important documents. In addition to the more technical documents, public communications and reports helped me gain contributory insights into the specific context of developments of the IND information systems. The outcome of the document analysis served as input for developing the interview protocols.

```{r documents, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "documents.csv")
data = read.csv(path, header = TRUE)

dt = data[]

# pander(dt)

kbl(data)

# kbl(dt, booktabs = T) %>%
  # kable_styling(latex_options = c("striped", "scale_down"))

# block_table(data[,c("Document", "Author", "Year", "Description")], header = TRUE, properties = pt)
# ft <- flextable(data)
# ft <- add_header_row(ft,
#   colwidths = c(4, 2),
#   values = c("Document", "Author", "Year", "Description")
# )
# ft <- autofit(ft)
# ft <- theme_vanilla(ft)
# ft <- set_caption(ft, caption = "Documents")
# ft
```

### Interviews {#interviews}

In order to learn more about the design and use of the search and match tools, I conducted semi-structured interviews with a diverse set of actors. Overall, my interview approach revolved around two main groups of themes and participants. The first group resolved around people at the IND whose tasks involve the searching and matching of identities in their databases — and which thus involves the use of the ELISE ID platform. The second group centred around people at WCC who are, more generally, involved in some way or another in the development, deployment, (pre-)sales of their software package for identity matching in the security and identity market segment.

More specifically, the interview approach focused on individual interviews, which allowed for more time and possibilities to discuss topics in detail. However, it should be noted that compared to other approaches opportunities to observe group dynamics and discussion were not possible. Having said that, individual user interviews were the easiest to set up and provided a high degree of detail of individual experiences. I developed a protocol for the interviews — which included questions and probes to use for follow-up — based on the research questions and insights from the document analysis and my fieldnotes. The Appendix includes a sample of these interview’s questions.

In the case of the IND interviews, participants were not immediately accessible due to my unfamiliarity with the organization and the need to conduct research digitally. Members of the ‘ID Team’ at WCC Group therefore facilitated establishing the first contacts with the IND organization. Next, I emailed the contacts I received and invited them for an online interview. After these first contacts, I attempted to use a kind of respondent-driven sampling for reaching out to more users willing to contribute to the research. On the whole, this approach allowed me to include participants that were not known before the start of the research, but could be included through the use of their networks. However, I had little control over the sampling size of respondents. It should be noted that the WCC organization only had one contact at the IND to begin with. The reliance on their networks furthermore necessarily lead to a bias in who was included, in this case more senior people in the organization.

The need to conduct the research digitally was due to the Covid-19 pandemic related measures, which made it not possible to have any in person interviews. For this reason, I contacted participants by email to schedule an online meeting or a phone call. All things considered, a benefit of these online meetings and phone calls may have been that interviews were in some sense easier to set up — neither me nor participants needed to travel. On the other hand, not all communication may have come across in the same way, and it may have hampered the possibility for networking and scheduling more interviews. The need to resort to phone interviews was due to limitations encountered by participants in IT access. For the online meetings I mainly relied on the secured Microsoft Teams installation of the WCC company. However, not all participants — who themselves were working from home — actually had the rights to install this application on their corporate laptop.

Another hypothesis was that users involved with different tasks at the IND would use the tools for searching and matching identities in different ways. For this reason, I reached out to users of different organizational units at the IND (see Table \@ref(tab:ind-departments) for an overview of these units). Indeed, the interviews clearly showed that there are distinct uses and knowledge of the tools. Unfortunately, it was not possible to conduct interviews with users from all units. Information about the use of the search and match tools in the other units did however emerge during other interviews. In total, I conducted five interviews regarding the use of the search and match tools at the IND, each lasting around one hour. The ‘Yes’ or ‘No’ values in the fourth column of the Table indicate if a user from this unit was included or not.

```{r ind-departments, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "ind-departments.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Org", "Unit", "Incl", "Description")], header = TRUE, properties = pt)
```

In parallel and following the IND interviews, I interviewed people from the company WCC. I structured these interviews rather differently from the IND ones and followed a more open-ended approach. The aim of the interviews was to shed light on important biographical moments of their identity matching software package. I therefore adapted my questions and probes on the person — i.e., their role, projects, and experiences in the company. For example, to people having worked on the IND or EU-VIS project I could ask more direct question about the development and deployment of the systems. To people with other profiles I could ask more generally about their relations with existing and potential customers in the security and identity market. Operationally, I interviewed, on one hand, people I met during my fieldwork visit at their office. On the other hand, I relied on a kind of respondent-driven sampling to receive more contacts. This lead a group of participants from which I distinguish two clusters. First, there are the members of the ‘ID Team’ at WCC, with different role titles such as consultant, pre-sales, solutions manager. Second, I interviewed more technical people, senior software developers and a UX designer. Similarly to the IND situation, all interviews were conducted online due to the restrictions related to the Covid-19 pandemic.

For all interviews, I requested permission to record the sessions using the informed consent form established by the Processing Citizenship project. The form guarantees anonymity and confidentiality for the participants, informs them of the project, and allowed them to specify how collected data may be used in the research. As per the protocol, I recorded only the audio with voice distortion. Furthermore, interviews were transcribed manually to ensure that no confidential data would be leaked via the use of automated transcription platforms. Table \@ref(tab:interviews) gives an overview of all interviews.

```{r interviews, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "interviews.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Number", "Organization", "Date", "Format", "Language", "Description")], header = TRUE, properties = pt)
```

### Events {#events}

Another source of data I collected are various relevant publications and through participating in events which were organized and attended by a variety of representatives of industry, academia, Member States authorities, and EU institutions. Table \@ref(tab:fieldwork-events) gives an overview and description of events I participated in, either in person or digitally. The purpose of attending such meetings was to understand how these various actors define and frame the problems and solutions of data quality of personal data in their systems. On one hand, I hypothesized that the presentations and discussions would include useful examples which could help understand the problems actual practitioners are facing. For example, during the Q&A of a panel on the implementation of interoperability at the eu-LISA conference in 2018 a member of the audience asked a question about how they would manage the large number of false positives which might be triggered when the different databases are connected that contain sometimes old data. The answer given by one of the panellists was that, indeed, a lot of man-hours will be needed to check all those false positives. This example is revealing of the otherwise background work that is needed to integrate distinct systems.

On the other hand, I hypothesized that the solutions proposed by industry representatives would make evident the kinds of imaginaries related to connecting and using data from different sources. For example, a common trope used to highlight the importance of tools for searching and matching data spread across different databases is based on presumed threats of terrorism. In one presentation, a software company used the real-life example of the Boston bombers who was, prior to the attack, added to police watch lists but with variations in his name and other personal information. The premise of these examples is that having such divergent information across databases makes investigations more difficult for officers, and could be potential blind spots.

During these events I made notes to capture such discussions and problems and solutions of data quality of people’s data. In addition, I collected screenshots and other publicly available materials related to these events such as video recordings and handouts. From these notes and pieces of observations and other materials I selected the most relevant to be coded and used in the data analyses.

<!-- ```{r fieldwork-events, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "fieldwork-events.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Date", "Event name", "Location", "Audience", "Description")], header = TRUE, properties = pt)
``` -->

### Other observations {#other-observations}

Other observations I use in the analysis are from news articles, press releases, and other articles which I collected to provide additional context and help me understand important moments in the biography of the WCC search and match tool and its deployment at the IND and EU-VIS. Generally, I collected articles which mention the company WCC and/or the of its software in the INDIGO system of the IND or the EU-VIS system. To do these queries I used the LexisNexis database and tool — which aggregates and makes it possible to query all major English and Dutch-speaking newspapers, online business news, and other related publications. For example, I found several articles detailing the progress of the implementation of the INDIGO system that were published in IT business news outlines.

What is not included in the LexisNexis database are some blog posts and announcements by companies publishing information related to the design, development, deployment, and use of the systems. To access such material, I relied on more conventional search engines. For example, I found that a competitor of WCC publishes frequent blog posts on Natural Language Processing such as one titled ‘Understanding Dari and Pashto Names: A Challenge to Intelligence Gathering in Afghanistan’ [@rosetteUnderstandingDariPashto2012]. From all such material I selected only the most relevant to be further coded and analyses. This means that, for the analysis, I included only those articles which either directly deal with topics of data quality, data matching, or other relevant discussing surrounding the functioning of the technical systems.

Together these different kinds of materials — news articles, blog posts, press releases — can assist in making a more finished picture of the biography of the software artefacts for searching and matching identity data.

## Techniques of data analysis

The techniques for analysing the collected data can be divided in two distinct methods. Once more, this distinction follows different aspects of data matching explained previously: the relations between different data models, and the matching of data within and between organizations. For analysing data models which underpin information systems a new method and a tool was developed to compare the data models defined in different formats. For the latter aspects, I relied on well-established social methods to analyse heterogeneous set of sources (i.e., fieldnotes, interview transcripts, technical documents).

Chapter \@ref(ch-data-model-matching) details the methodology for comparing data models dubbed the ‘Ontology Explorer’. Briefly, the Ontology Explorer (OE) is a semantic method and javascript-based open-source tool thought to compare the data models collected in different formats and from diverse systems. As such, the OE is distinctive in two respects.  First, it supports analyses of information systems which define their own data models, even if these systems are not immediately comparable. Second, it systematically and quantitatively supports discursive analysis of ‘thin’ data models, also by detecting differences and absences between systems. To achieve this, the method extracts, analyses, compares and visualizes heterogeneous data models at once. Applying this structured approach to the data models used by diverse organizations eventually made it possible to observe differences and similarities in the data models of diverse data infrastructures.

In this way, the analysis using the OE addresses a first aspect of matching identity data: the correspondence between data models of different EU and Member State systems. That is, how such different data models for collecting information about people-on-the-move are situated in relation to one another, and what this tells us about the organizations which developed and make use of these models to collect, search, and match identities.  Data models of EU and Member States’ systems were analysed using the methodology and tool of the ontology explorer. The results from this analysis showed that, for example, there are indeed differences and similarities between all data collected in the systems and those data that are used for searching and matching identities. Furthermore, these differences and similarities between authorities’ data models can give evidence of how knowledge of people circulates and shows divisions of responsibilities between actors. A typology of data could thus be developed which includes, on one hand, shared categories of data important for searching and matching identities. Whereas other distinctive categories give evidence of other responsibilities distinctive to those systems and actors.

For analysing the other fieldwork data, I followed a process inspired by the Noticing-Collecting-Thinking (NCT) method described by @frieseQualitativeDataAnalysis2014. After collecting and preparing data from documents and interviews (including transcription), I analysed this data by applying this method using the computer-assisted qualitative data analysis software ATLAS.ti. In short, the three interconnected steps of the Noticing-Collecting-Thinking method can be summarized as follows. In the Noticing step, labels are assigned to segments in the documents — known as codes. This coding step is guided by the research questions, but at the same time open to inductive findings from the data. Next, in the Collecting step, those codes are reviewed and gathered into similar codes. Finally, the Thinking step relates to finding patterns, processes, typologies among the developed codes. Of course, in practice, the whole process proceeds by moving back and forth between the different steps of noticing, collecting and thinking. Figure \@ref(fig:data-analysis) summarizes this recursive process of data collection and applying the different NCT steps.

```{r data-analysis, fig.cap="Data analysis."}
knitr::include_graphics("figures/data-analysis.pdf")
```

For example, the NCT method was used for analysing data collected during fieldwork research on the use of the ELISE data matching technology at the agency for asylum and naturalization of The Netherlands. From these evidences I developed a non-exhaustive typology of possible data frictions for alphanumeric identity data. This typlogy then types included types such as: frictions resulting from human errors during data entry, ambiguities and incommensurabilities in transliterations, differences in identification policies, and so forth. Next, such findings were used to theorize about how practices for population management are shaped by databases containing information that is not always complete, accurate, reliable. And which imaginaries regarding the way data uncertainties should be managed are at play at the same time in the case of ELISE at the IND.

These examples show how, by drawing from different sources, it is possible to trace different aspects of the practices related to data matching and the design and use of data matching technologies. In the next chapter I will explore in more detail the method of the OE and how I used it to analyse the relations between different data models of both EU and Member State systems.
