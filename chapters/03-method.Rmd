# Research design and methods {#ch-method}

## Research objectives: Applying and testing the conceptual framework for tracing data matching in transnational security data infrastructures

## Fieldwork site and access

### WCC ELISE — A technology for matching identity data

In approaching the company WCC, I presented my intent to research the use of their software package ELISE as beneficial to both the interests of the Processing Citizenship and PhD research projects, and the interests of WCC and its customers. First, the PhD and PC projects aim to explore how information systems for identity management support the production of knowledge about non-European populations and the challenges encountered. The technologies developed by WCC — and deployed at government agencies and (international) organizations concerned with identity and security — could therefore serve as an entry point to learning about such processes of knowledge production. As such, studying the design and real-world use of this software technology can provide important insights to understand technologies for identity management in security settings, and related challenges of data matching within and across organizations. Second, findings of the research were thought to provide better understanding for WCC and their customers into the actual use of their technologies. Such insights can be used for further development of their technologies.

My research approach focused specifically on deployments of the ELISE ID platform at customers in WCC' security and identity domain. The ELISE software solution serves in fact as a core technology in all solutions developed by WCC, ranging from products for border management to public employment services. In short, the platform provides sophisticated functionalities for data matching such as searching and matching of personal identity information for different data sources. The novelty of the technology comes from using various kinds of fuzzy logic algorithms for data matching that take into account that data may be incomplete or inaccurate. Here, fuzzy logic refers to a form of logic in mathematics where truth variables are calculated in probabilities instead of only being "true" and "false". Search results in the ELISE ID platform — also previously known as ELISE Smart Search & Match — are consequently returned as values indicating the probability that two sets of identity data can be considered identical — i.e., match.

In short, the overall goal of the fieldwork was, first, to understand how organizations do not simply use the ELISE ID platform and its identity matching organizations. Rather, the leading hypothesis was that embedded assumptions in the tools to a certain extent shape the organizations — such as how users think about data, the uncertainty of establishing identity and, hence, how this shapes their work practices. Through my fieldwork I sought evidences of such dynamics by comparing the design of the tools with how different users use the biographic matching and multicultural name matching capabilities in their daily tasks when searching for identities. Such a comparison was thought to bring out how users' expectations of data — and uncertainty surrounding data — are influenced by the software solutions. That is, how these expectations may be the source of tensions in imagined and actual use of the search and match functionalities. For this first goal I therefore draw inspiration from STS approaches to study the possibilities, constraints, and resistances of devices embedding prescriptive representations [for an overview see @suchmanHumanmachineReconfigurationsPlans2007].

The second goal of the fieldwork was to understand the processes and practices of developing and deploying such a software package for matching identity data. The leading hypothesis in this case was that the knowledge and technologies for matching identity data travel and circulate. The software for searching and matching identity data would therefore embed certain imaginaries and assumptions that evolved over time and while circulating and interacting between different actors, such as the company, their customers, competitors, and other organizations. For this second goal I therefore aimed to identify important moments in the can development and deployment of the software package. For example, it is crucial to understand the work performed to make such matching of identity data work across different contexts and organizations. That is, we should question exactly the generic nature of the software package and instead focus on the struggles and accomplishments to make it so. This second, as such, draws upon the perspectives developed by "biographies of artifacts and practices" (BOAP) framework urges to understand technological development as a complex and dynamic process happening across time, locales, and involving various actors [@hyysaloMethodMattersSocial2019].

To summarize, the overarching hypothesis leading this fieldwork research was thus that this particular form of searching provided by the solutions — taking for granted uncertainty about data — incorporates certain imaginaries — a "smart search and match" — that shapes and is shaped by users, their work practices, and the organizations in particular ways. Tracing such complex dynamics then requires taking a simultaneously more and less specific perspective [see also @suchmanConfiguration2014]. More specific in the sense of looking at specific moments of use and development of the tools for searching and matching. And less specific in a sense of taking into account the often unpredictable and uncertain actors shaping the software.

### Deployments at the IND and EU-VIS

To test the overarching hypothesis it was decided to investigate the deployment of the solutions at two customers of WCC's software solutions in both national and transnational settings. First, the integration of the ELISE ID platform with the systems of the immigration and naturalization service of the Netherlands (IND). Second, the integration of ELISE in the EU Visa Information System. As mentioned previously, we should however be careful in distinguishing between the national and the transnational. The social methods employed for this inquiry do not only describe realities, but they are a part of the "ontological politics" of enacting certain kinds of realities [@lawEnactingSocial2004]. My aim is therefore not to reproduce fixed distinctions between national, transnational, or international. Following Law and Urry, the question is which realities I want to enact. The choice, in this case, is rather a hotchpotch of actors and technologies without clear boundaries of national or transnational.

Furthermore, for both cases the use of the search and match solution were deemed relevant to understand how the tools support and shape the practices of knowledge production related to the identity of people. In the case of the IND, the agency is responsible for, among others, processing the applications from people who want to stay in The Netherlands or who want to become Dutch nationals. The use of the ELISE relates to these responsibilities. The IND information system leverages the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system. The EU-VIS system, on the other hand, has a more narrow scope in its use of biographic matching and multi-cultural name matching capabilities for its search engine. At the same time, due to its transnational scope, the amount of records in the EU-VIS database is much higher.[^eu-vis-capacity]

[^eu-vis-capacity]: The "Report on the technical functioning of the Visa Information System (VIS)" [@[@europeanagencyfortheoperationalmanagementoflargescaleitsystemsintheareaoffreedomsecurityandjustice] notes a database capacity of 60 million records.

The methodological choice of tracing the development and deployment of the ELISE software package for these two organizations necessarily leads to certain "framing effects" [@hyysaloMethodMattersSocial2019]. The software solutions of WCC have been deployed at other customers such as other government agencies and in different fields (public employment services). However, the other customers were not possible to research due to issues such as confidentiality. The other fields were at first identified as not immediately relevant, but came up at several moments during the fieldwork and interviews. In future sections I will describe return to how the genericness of the product to work across domains is accomplished. Finally, there are other actors such as competitors and other potential customers that have a direct and indirect influence on the company and its products. Subsection \@ref(events) and \@ref(other-observations) describe how I included these peripheral actors in the fieldwork observations by, for example, attending industry events and relying on other sources.

## Methods for data collection

### Traces of data models

An important aspect of investigating the matching of identity data across organizations is how data models of different systems correspond to one another. For matching persons' data within and between organizations some correspondences and similarities in the types of information stored in the respective databases need to be found. For example, two systems may store a combination of persons' last name, date of birth, nationality and used to match separate identity records; but with distinct data models and variations in attribute labels, types, formats. Such data models and their categories of data, i.e. the labels describing a state that can assume different values, can be described in various kinds of documents: from database schemas to design documents, or even in regulations. In this section, I will describe the processes of collecting such traces of data models that were used for the analysis. In general, such traces of data models can be collected through desk research or fieldwork. However, to make such diverse descriptions of data models comparable additional steps were needed to code, harmonize, and group all documents, categories, and values.

All those steps contributed to the development of a method and tool to compare data models utilized in national and international information systems that jointly work to support registration and identification practices at border zones in Europe. Regarding the traces of those data models I draw on data which were collected during fieldwork conducted in the context of the Processing Citizenship project at border zones in Europe. Given linguistic constraints and the Project's task plan organized as a matrix, some documents were collected by other researchers employed as collaborators in the Processing Citizenship project.[^pc-team] Overall, the data collection efforts included desk research of European regulations, technical documents made available by European and German authorities, systems screenshots collected at border zones in the Hellenic Republic, and technical documents collected during fieldwork in The Netherlands. My own efforts included the desk research and fieldwork in The Netherlands.

[^pc-team]: These researchers include: A. Bacchi, E. Frezouli, Y. Lausberg, C. Loschi, L. Olivieri, A. Pelizza, A. Pettrachin, S. Scheel.

For transnational security infrastructures such as the information systems developed by European Commission agencies in the Area of Freedom, Security and Justice (AFSJ), the issue of data matching is becoming increasingly important. Ongoing projects to increase information-sharing and interoperability of information systems in this area necessarily demands various new forms of data matching. Broadly speaking, various international actors are developing a central infrastructure to store links and references to EU information systems. In this new architecture, identity data from previously separate systems will be matched against each other to link them and with the aim of facilitating identity checks and detecting identity fraud. Besides these new types of data matching under development, the existing systems already do real-time data matching such as when searching for personal details of a person by a police officer. To understand relations between the categories of data recorded in these three EU systems I draw upon the regulations establishing each system.

Data collection therefore includes documents of the regulation of the three international information systems developed by European Commission agencies: Eurodac, the Schengen Information System (SIS) and the Visa Information System (VIS). All three systems have specific aims in supporting policing tasks related to travel, cross-border crime and irregular migration. Eurodac (_European Dactyloscopy_) aims to support the identification of asylum seekers through fingerprints, and to determine the Member State which is responsible for processing their asylum applications in the context of the Dublin System.[^dublin] The database was established in 2003 to store fingerprint and other basic data of asylum seekers. It is currently operational in the Member States of the European Union (plus Norway, Iceland, Switzerland, and Liechtenstein). Since its original inception, the use of the system has been extended to law enforcement authorities and Europol [@ajanaAsylumIdentityManagement2013]. New proposals aim to include more biographic data and additional biometric data, and a facial image.[^recast]

[^dublin]: The Dublin System (Regulation No. 604/2013; also known as the Dublin III Regulation) establishes the criteria and mechanisms for determining which EU Member State is responsible for examining an asylum application.

[^recast]: Procedure 2016/0132/COD, recast of the Eurodac Regulation.

The purpose of the Schengen Information System (SIS II) is to support external border control and law enforcement cooperation in the European Union. It supports this task by storing alerts which contain information on persons and objects, and information/instructions on what to do when such persons or objects are encountered. This information can then be exchanged between law and border enforcement authorities through the SIRENE network. Finally, the Visa Information System (VIS) allows for exchange of visa data (including personal data and biometrics) and in this way it aims to maintain a common EU visa policy. Checks on the VIS data are done in the context of identification procedures at the borders. This kind of access is different from the previous systems, as the data can only be accessed on a case by case basis. Authorities at the external border of the European Union also have access for purposes such as "verifying the identity of the person, the authenticity of the visa or whether the person meets the requirements for entering, staying in or residing within the national territories" [@europeancommissionVisaInformationSystem]. Furthermore, data in VIS can be used by asylum authorities to determine which EU Member State is responsible for examining the asylum application.

Concerning national systems, the analysis includes the systems of the Dutch (DRF), Hellenic (HRF) and German (GRF) Register of Foreigners (GRF). The DRF is an information system stores all basic data of people who have a relation with the Dutch government in the context of the "Vreemdelingenwet". The system allows digital exchange of identity data, information about travel and identity documents, biometric characteristics and status data between the partners. Various ministries and organizations in The Netherlands use this system to share and consult information about foreign nationals. As such, it acts as a kind of single source of truth. That is, although other partners may have their database and information about a person, the data needs to be kept aligned [@InformatievoorzieningVreemdelingenketen2015]. This can also be seen due to the fact that through the BVV all foreigners receive a unique identification number — the v-number. In practice then, processes of establishing identity involves an interplay of multiple systems. For example, IND users searching for person matches always need to consider result from this system as well.

The HRF is the main system used at border zones in Greece to identify and register persons who arrive at the border without the required documents. The system is used to support different tasks during the identification and asylum procedures: from retrieving migrants' biographic and biometric data, to conducting screening and asylum interviews, to assessing health conditions. Users of the systems therefore include police, administrative personnel, and asylum officers.

The GRF is a German database which contains a large amount of personal information of foreigners in Germany who have or had a residence permit, as well as those who seek or have sought asylum or are recognized asylum seekers [@bundesverwaltungsamtbvaAuslanderzentralregister]. This central register is accessed by various partner authorities and organisations in fields such as asylum, migration, border control. The data sent to the GRF during the first registration is described in the XAusländer standard, a data exchange format which formalizes and enables data exchange between the immigration authorities in Germany [@StandardXAuslaender]. According to the description of the standard's motivation, it aims to facilitate exchange of such data between authorities in Germany, in order to reduce data re-entry, and to enable reuse of such data by the authorities.

All the information systems described above define their own data models and the kinds of people they aim to represent, in ways that are not immediately comparable. In subsequent sections I will therefore introduce the method developed for extracting, analysing, comparing and visualizing diverse data models.

### Document analysis

My analysis of documents related to the deployment of ELISE is grounded in understandings developed by STS of documents and practices of documenting as objects of study. As @shankarRethinkingDocuments2016 explain, different STS scholars broadened the understanding of documents as artefacts that don't just document and stand for something in the world. Rather, documents play a role in social contexts — such as accounting for and coordinating of workplace activities — and cannot be separated from the practices through which they are produced. For this document analysis, I therefore looked not only as documents that help explain how the software packages function and how they were configured and deployed. Instead, the documents also "refer to the practices, objects, rules, knowledge, and organizational forms that produced them" (ibid., p. 62). Such a document analysis can thus contribute an understanding of the key biographical moments in the life of this software package.

At a first stage, I began the research by studying documents related to, first, the more generic technical details of the ELISE ID platform. And second, documents related to the specific implementation of the platform at the IND. WCC provided these documents — such as technical design documents and meeting minutes — during several on-site visits at their head office in Utrecht, The Netherlands. Members of the WCC ID team provided me additional context for the documents along with updates on the current status of the implementation at the IND.

In general, I distinguished between two broad aims of the document analysis, corresponding to two different research questions and hypotheses. A first aim was to find evidences of frictions between design and use of the tools by comparing how different users at the IND use the biographic matching and multi-cultural name matching capabilities in their daily tasks when searching for identities. Such a comparison was thought to help answer RQ3 by understanding how users' expectations of data — and uncertainty surrounding data — is influenced by the software solutions. That is, how these expectations may be the source of tensions in imagined and actual use of the search and match functionalities. As I will explain further in Chapter \@ref(ch-dm-within-org), there are indeed differences between the designed probabilistic identity match, and user's understanding of, for example, the search results.

The second aim of the document analysis was to unearth important moments in the trajectory of the software package. Key insights from scholars in this regard are that instead of looking at single versions of document, we should pay attention to the processes of documenting [@shankarRethinkingDocuments2016]. For example, by looking at different versions of the document — changes within them, added annotations, and so forth. In the case of the deployment of ELISE at the IND, I found different versions of the design document, documents describing updates to the package, and meetings notes discussing changes to the configuration. However, documents generally do not have a clear provenance. In encountered plenty of ambiguity and uncertainty in knowing why certain documents were created. This especially the case for ELISE at the IND. Most of the documents date back a decade ago and people involved with the project are no longer part of the organization.

In order to understand how knowledge and technologies for matching identity data travel and circulate, documents should be understood as assemblages of different authors and sources. As such, documents do not have an easily identifiable origin but are "assembled from multiple sources [where] content often flows from application to application and document to document, constantly recycled, reworked, and repackaged" (ibid., p. 63). Such flows are easy to recognize in the ELISE documentation for the IND. Technical documents incorporate generic information about the package from other company documents with details specifically for the IND implementation. In this way the genealogy of the package becomes evident — how it has moved and evolved when moving across domains and organizations.

Finally, an important — but less visible — as aspect of documentation is missing or unavailable documents. During my fieldwork it became evident that documents for the deployment of ELISE in the EU Visa Information System would not be available for me. This is due to specific configuration of actors in the development of the EU-VIS systems. In this configuration WCC acted as the technology supplier and worked together with a technology integrator, Accenture. This meant that relevant documents are in the hand of this integrator. What's more, the technology integrator acts as sort of gatekeeper which makes it impossible to access such materials. This lack of documents therefore also is revealing of the practices of deploying such packages.

In short, studying the documents allowed me to, on one hand, form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents give insights in the genealogy of the package and the practices of configuring, deploying, designing the software. Table \@ref(tab:documents) gives an overview of some these most important documents. In addition to the more technical documents, public communications and reports helped me gain contributory insights into the specific context of developments of the IND information systems. The outcome of the document analysis served as input for developing the interview protocols.

```{r documents, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "documents.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Document", "Author", "Year", "Description")], header = TRUE, properties = pt)
```

### Interviews {#interviews}

In order to learn more about the design and use of the search and match tools, I conducted semi-structured interviews with a diverse set of actors. Overall, my interview approach revolved around two main groups of themes and participants. The first group resolved around people at the IND whose tasks involve the searching and matching of identities in their databases — and which thus involves the use of the ELISE ID platform. The second group centred around people at WCC who are, more generally, involved in some way or another in the development, deployment, sales of their software package for identity matching in the security and identity market segment.

More specifically, the interview approach focused on individual interviews, which allowed for more time and possibilities to discuss topics in detail. However, it should be noted that compared to other approaches opportunities to observe group dynamics and discussion were not possible. Having said that, individual user interviews were the easiest to set up and provided a high degree of detail of individual experiences. I developed a protocol for the interviews — which included questions and probes to use for follow-up — based on the research questions and insights from the document analysis. The Appendix includes a sample of these interview's questions.

In the case of the IND interviews, participants were not immediately accessible due to my unfamiliarity with the organization and the need to conduct research digitally. Members of the "ID Team" at WCC Group therefore facilitated establishing the first contacts with the IND organization. Next, I sent a mail to the contacts I received and invited them for an online interview. After these first contacts, I attempted to use a kind of respondent-driven sampling for reaching out to more users willing to contribute to the research. On the whole, this approach allowed me to include participants that were not known before the start of the research, but could be included through the use of their networks. However, I had little control over the sampling size of respondents. It should be noted that the WCC organization only had one contact at the IND to begin with. The reliance on their networks furthermore necessarily lead to a bias in who was included, in this case more senior people in the organization.

The need to conduct the research digitally was due to the Covid-19 pandemic related measures, which made it not possible to have any in person interviews. For this reason, I contacted participants by email to schedule an online meeting or a phone call. All things considered, a benefit of these online meetings and phone calls may have been that interviews were in some sense easier to set up — neither me nor participants needed to travel. On the other hand, not all communication may have come across in the same way, and it may have hampered the possibility for networking and scheduling more interviews. The need to resort to phone interviews was due to limitations encountered by participants in IT access. For the online meetings I mainly relied on the secured Microsoft Teams installation of the WCC company. However, not all participants — who themselves were working from home — actually had the rights to install this application on their corporate laptop.

Another hypothesis was that users involved with different tasks at the IND would use the tools for searching and matching identities in different ways. For this reason, I reached out to users of different organizational units at the IND (see Table \@ref(tab:ind-departments) for an overview of these units). Indeed, the interviews clearly showed that there are distinct uses and knowledge of the tools. Unfortunately it was not possible to conduct interviews with users from all units. Information about the use of the search and match tools in the other units did however emerge during other interviews. In total, I conducted five interviews regarding the use of the search and match tools at the IND, each lasting around one hour. The "Yes" or "No" values in the fourth column of the Table indicate if a user from this unit was included or not.

```{r ind-departments, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "ind-departments.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Org", "Unit", "Incl", "Description")], header = TRUE, properties = pt)
```

In parallel and following the IND interviews, I interviewed people from the company WCC. I structured these interviews rather differently and followed a more open-ended approach. The aim of the interviews was to shed light on important biographical moments of their identity matching software package. I therefore adapted my questions and probes on the person — i.e., their role, projects, and experiences in the company. For example, people having worked on the IND or EU-VIS project I could ask more direct question about the development and deployment of the systems. While people other profiles I could ask more generally about their relations with existing and potential customers in the security and identity market. Operationally then, I interviewed, on one hand, people I met during my fieldwork visit at their office. On the other, I relied on a kind of respondent-driven sampling to receive more contacts. This lead a group of participants from which I distinguish two clusters. First, there are the members of the "ID Team" at WCC, with different role titles such as consultant, pre-sales, solutions manager. Second, I interviewed more technical people, senior software developers and a UX designer. Similarly to the IND situation, all interviews were conducted online due to the restrictions related to the Covid-19 pandemic.

For all interviews, I requested permission to record the sessions using the informed consent form established by the Processing Citizenship project. The form guarantees anonymity and confidentiality for the participants, informs them of the project, and allowed them to specify how collected data may be used in the research. As per the protocol, I recorded only the audio with voice distortion. Furthermore, interviews were transcribed manually to ensure that no confidential data would be leaked via the use of automated transcription platforms. Table \@ref(tab:interviews) gives an overview of all interviews.

```{r interviews, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "interviews.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Number", "Organization", "Date", "Format", "Language", "Description")], header = TRUE, properties = pt)
```

### Events {#events}

I collected various relevant publications and participated in events which were organized and attended by a variety of representatives of industry, academia, Member States' authorities, and EU institutions. Table \@ref(tab:fieldwork-events) gives an overview and description of events I participated in, either in person or digitally. The purpose of attending such meetings was to understand how these various actors define and frame the problems and solutions of data quality of personal data in their systems. On one hand, I hypothesized that the presentations and discussions would include useful examples which could help understand the problems actual practitioners are facing. For example, during the Q&A of a panel on the implementation of interoperability at the eu-LISA conference in 2018 a member of the audience asked a question about how they would manage the large amount of false positives which might be triggered when the different databases are connected that contain sometimes old data. The answer given by one of the panellists was that, indeed, a large amount of man-hours will be needed to check all those false positives. This example is revealing of the otherwise background work that is needed to integrate distinct systems.

On the other hand, I hypothesized that the solutions proposed by industry representatives would make evident the kinds of imaginaries related to connecting and using data from different sources. For example, a common trope used to highlight the importance of tools for searching and matching data spread across different databases is based on presumed threats of terrorism. In one presentation, a software company used the real-life example of the Boston bombers who was, prior to the attack, added to police watch lists but with variations in his name and other personal information. The premise of these examples is that having such divergent information across databases makes investigations more difficult for officers, and could be potential blind spots.

During these events I made notes to capture such discussions and problems and solutions of data quality of people's data. In addition, I collected screenshots and other publicly available materials related to these events such as video recordings and handouts. From these notes and pieces of observations and others materials I selected the most relevant to be coded and used in the data analyses.

<!-- ```{r fieldwork-events, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "fieldwork-events.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Date", "Event name", "Location", "Audience", "Description")], header = TRUE, properties = pt)
``` -->

### Other observations {#other-observations}

Other observations I use in the analysis are from news articles, press releases, and other articles which I collected to provide additional context and help me understand important moments in the biography of the WCC search and match tool and its deployment at the IND and EU-VIS. Generally, I collected articles which mention the company WCC and/or the of its software in the INDIGO system of the IND or the EU-VIS system. To do these queries I used the LexisNexis database and tool — which aggregates and makes it possible to query all major English and Dutch-speaking newspapers, online business news, and other related publications. For example, I found several articles detailing the progress of the implementation of the INDIGO system that were published in IT business news outlines.

What is not included in the LexisNexis database are some blog posts and announcements by companies publishing information related to the design, development, deployment, and use of the systems. To access such material, I relied on more conventional search engines. For example, I found that a competitor of WCC publishes frequent blog posts on Natural Language Processing such one titled "Understanding Dari and Pashto Names: A Challenge to Intelligence Gathering in Afghanistan" [@rosetteUnderstandingDariPashto2012]. From all such material I selected only the most relevant to be further coded and analyses. This means that, for the analysis, I included only those articles which either directly deal with topics of data quality, data matching, or other relevant discussing surrounding the functioning of the technical systems.

Together these different kinds of materials — news articles, blog posts, press releases — can assist in making a more finished picture of the biography of the software artefacts for searching and matching identity data.

## Techniques of data analysis
