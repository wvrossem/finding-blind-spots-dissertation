# Research design and methods {#ch-method}

\chaptermark{Research design and methods}

This dissertation examines the complex phenomenon of linking and matching identity data matching in transnational security infrastructures. Still, what methods can we employ to examine these various aspects of data matching technologies and practices in relation to infrastructures? This chapter develops a conceptual framework that unifies methodological approaches and strategies into a visual tool that will direct the analysis of identity data matching in transnational security infrastructures. The chapter is divided into two parts. The first part introduces three methodological strategies for dealing with the well-known issue that as these practices and technologies become part of large-scale infrastructures, they have the propensity to disappear. The three methodological strategies thus draw upon literature that devised ways of making parts of security infrastructures more visible. The three methodological strategies are named after and inspired by @bowkerSortingThingsOut1999 seminal work on "infrastructural inversions". The resulting conceptual model seeks to deepen our knowledge of identification infrastructures by offering a view that strikes a balance between the usual emphasis on local interactions and broad technical overviews.

## Infrastructures as topics and resources of research

A fundamental premise of the dissertation's fundamental is that social research can use digital infrastructures as both a topic and a resource [@marresDigitalSociologyReinvention2017; @pelizzaDevelopingVectorialGlance2016]. In other words, we can either study infrastructures as distinct research topics or use infrastructures as resources for examining other areas of social research. Similarly, this research examines, on the one hand, infrastructures supporting the linking and matching identity data as a standalone subject. On the other hand, examining infrastructures can open up opportunities for study in areas like internationalization, commercialization, and securitization of identification. Nonetheless, both options must address the elusive issue of defining what infrastructure is and where and when it begins and ends.

Typically, researchers get around the problem of a priori delineating infrastructure by studying infrastructure ethnographically [@karastiStudyingInfrastructuringEthnographically2018; @starEthnographyInfrastructure1999]. To this end, the researcher as ethnographer can approach infrastructure in a self-reflexive manner, building it "by every choice the ethnographer makes in selecting, connecting, and bounding the site and via the interactions through which s/he engages with the material artifacts and the people who define the field" [@blombergReflections25Years2013, p. 389]. Ethnographically studying infrastructure is just one possible means of delineating infrastructure, but one imposing considerable responsibility on the researcher. A different approach is to follow the acts of speech and acts of doing (practices) by social actors. Researchers also do not necessarily have to be present with the actors but can rely on various traces of the mediations and infrastructures [@beaulieuResearchNoteColocation2010]. This chapter takes cues from such methods to create a conceptual framework that does not define infrastructures at the outset but instead follows practises and actors by examining the finer technical details of matching and connecting identity data.

Thus, there are at least two reasons for investigating the technical aspects of matching and linking identity data in security infrastructures. One reason is that, as discussed in Chapter 2, research has yet to thoroughly investigate the mechanisms for matching and linking identity information in identification. For instance, linking identities across systems and databases with various informational representations needs to be better understood. Second, as noted by @pelizzaDevelopingVectorialGlance2016, focusing on the "technical minutiae" of such systems' and databases' interoperability can be "strategic sites" that can highlight more significant "institutional shifts". Similarly, looking at how identities are matched and linked could show how these technical mechanisms also change the way actors relate to each other, such as between governments and security companies. In order to invert the embedded and less obvious technical details of matching and linking identity data in data infrastructures, the following section outlines the methodological strategies from the literature that serves as the basis of the conceptual framework. The various "infrastructural inversions" are then combined into a single conceptual framework that examines the matching of identity data in EU and member state systems by using infrastructures as both a topic and a resource.

## "Infrastructural inversions" for matching identity data

What methods and frameworks are best suited to trace the developments of matching of identity data in border and migration control? In recent years, there has been an increasing amount of literature on Europe’s technological systems of border, migration control, and security as infrastructures [@dijstelbloemBordersInfrastructureTechnopolitics2021; @potzschEmergenceIBorderBordering2015]. Such large technical systems or infrastructures have been a long-term concern for scholars studying the processes, actors, tensions in the long-term historical developments of technologies [@hansethDevelopingInformationInfrastructure1996; @hughesNetworksPowerElectrification1983; @starStepsEcologyInfrastructure1996]. In this way, existing research on border, migration control, and security as infrastructures recognizes the critical role played by underlying infrastructures in organizing and giving shape to large-scale phenomena. One of the most important aspects of infrastructural viewpoints is highlighting how something like data matching technologies would "consist of numerous systems, each with unique origins and goals, which are made to interoperate by means of standards, socket layers, social practices, norms, and individual behaviors that smooth out the connections among them" [@edwardsKnowledgeInfrastructuresIntellectual2013, p. 5]. Conceptualizing the socio-technical systems to manage people’s identities as security infrastructures allows us to untangle the networks in these complex "ecologies" [@starStepsEcologyInfrastructure1996].

Yet, a well known result of the overlapping between infrastructures and practices is that infrastructures have a tendency to be taken for granted and to "remain as invisible backdrops to social action" [@harveyIntroductionInfrastructuralComplications2016, p. 3]. Studies on infrastructures have therefore provided different strategies to invert this tendency of infrastructures to disappear, and to make visible the interconnections between technical minutiae and the politics of knowledge production [e.g. @bowkerSortingThingsOut1999; @edwardsIntroductionAgendaInfrastructure2009; @monteiroArtefactsInfrastructures2013]. Methods proposed by these authors to invert the tendency of infrastructure to disappear include looking at, among others, moments of breakdown [@starEthnographyInfrastructure1999], tensions in the emergence and growth of infrastructure [@hansethReflexiveStandardizationSide2006], and material aspects [@ribesMaterialityMethodologyTricks2019]. These kinds of "infrastructural inversions" [@bowkerScienceRunInformation1994] makes it possible to acknowledge the links between technical systems for identity matching and the processes of identification in transnational security settings.

As @bowkerSortingThingsOut1999 have pointed out, technological choices easily become invisible, as only what is in the systems is usually (and at best) documented. This is crucial, as infrastructures connect different contexts to large-scale networks that bridge these differences to allow for exchanging and making use of information collected at various local situations [@dijstelbloemBordersInfrastructureTechnopolitics2021]. Understanding choices made in identification systems would make it possible to uncover tensions in linking people’s data from different systems. As scholars of digital infrastructures have shown, the networks that connect and shape relations between a multitude of actors such as individual, states, companies, technologies are often sources of tensions in the developments toward certain goals[@edwardsUnderstandingInfrastructureDynamics2007; @ribesLongNowInfrastructure2009].

Despite these insights on the invisibility of infrastructures, scholarship on security shows a tendency to primarily refer to the (in)visibility of humans. Less attention has been given to the invisibility of those data infrastructures that allow making human mobility, migration, and borders visible. This is a key aspect, though. The technical mechanisms used to link people’s data within and across are key for tracking people and detecting patterns. People on the move become visible through infrastructures that are not neutral, but enact them in certain ways [@pelizzaProcessingAlterityEnacting2019]. Passenger screening systems, for example, will try to identify passengers against police watchlists and detect suspicious patterns in someone’s travel history. The passenger date models and rules to detect such patterns embedded in the infrastructures can be conceptualized as ‘scripts’ through which security subjects are enacted.

Since the 1990s, the question of how to make visible infrastructures has primarily been addressed by approaching infrastructures as a ‘relational concept’ [@starStepsEcologyInfrastructure1996]: what is infrastructure for one person might be a topic for someone else. Such relational approaches maintain that infrastructures are not objects of study by itself, but show preference for ethnographic methods to study how infrastructures emerge through interactive processes and practices [@starEthnographyInfrastructure1999; @karastiStudyingInfrastructuringEthnographically2018]. Alternatively, an often cited article by @larkinPoliticsPoeticsInfrastructure2013 argues that there are degrees of visibility infrastructure. Large-scale infrastructures can also have a certain ‘aesthetics’, such as a state’s infrastructure project that is made prominently visible. In the case of EU systems, a similar example may be found in the grand project to make various EU systems interoperable to solve EU migration and security challenges. But I contend that this aspect is less important to understand the otherwise less noticed work done by security infrastructures for identity matching. I argue that there is, however, still room for other kinds of methodological contributions to further the analysis the technical minutae of matching identity data in transnational security infrastructures and beyond.

First and foremost, are the informational representations or semantic classifications used to identify people — i.e., the ontologies or data models implemented in the databases. Databases utilized at the border support authorities in collecting and linking different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g., @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. In this way, databases institute certain kinds of visions; yet the embedded politics and design choices can easily become invisible. We might thus ask how we can retrace knowledge representation choices, in order to comparatively understand how diverse systems — run by different authorities — enact people and how these diverse representation are matched.

### First inversion strategy: Informational representations and data matching

Data models and ontologies are entwined in digital infrastructures and practices and institute specific ways of knowing and working [@bowkerSortingThingsOut1999; @hineDatabasesScientificInstruments2006; @lamplandStandardsTheirStories2009; @timmermansWorldStandardsNot2010]. Data models designed to represent phenomena can be considered a technology that enacts particular kinds of knowledge, organizations and practices [@bloomfieldVisionsOrganizationOrganizations1997]. The politics in designing these informational representations shape how information is represented and affect what becomes visible and, consequently, what turns invisible. Applied to border and migration management, this implies that data models represent people at the border and make them knowable in specific ways. For example, family composition fields with up to five members assume the Western parental family model. Data models that foresee only female and male gender options assume migrants are binarily sexualized. Identifying people through fingerprints instead of study degree or family tree shrinks the possibilities of how they can be known [@pelizzaIdentificationTranslationArt2021].

The current, as well as the historical antecedents, of registration practices and use of database technology as tools to sort populations for controlling borders and migration has been widely researched [@lyonSurveillanceSocialSorting2003; @torpeyInventionPassportSurveillance2000; @ansorgeIdentifySortHow2016]. In his book on "The invention of the passport", @torpeyInventionPassportSurveillance2000, for example, recounts the long historical developments of how states came to establish their imperative authority to control the movements of people. He stresses the role of documents such as the passport and related practices in registering, identifying and tracking people and their movements. With people moving between different territorial authorities, authorities introduced different forms of documentation and registration to distinguish between persons allowed access to and rights within the territory. These mundane artefacts, such as passports and population registers, embed powerful data models to sort and control people.

More recently, database technologies have played an indispensable role for states to surveil and control migration through data collection practices, allowing the collection of different kinds of biographic and biometric data to make people and bodies "legible". At the same time, these technologies have also opened possibilities for sorting populations for different treatments. According to @reekumDrawingLinesEnacting2017, infrastructure developed for monitoring and controlling persons and border crossings can be understood as forms of visualization that make these phenomena known or visible. In their view, the schematized and abstract imageries of maps, flows, and crossings that underpin the technologies and practices do not just make these things visible but are, in fact, co-constitutive of what borders and crossings are. Moreover, to make such phenomena visible, van Reekum and Schinkel emphasize the role of documents and categories to render visible and infer borders and border crossings. What needs to be clarified is how various categories and representations collected about people and their movements are matched and linked.

Database technologies are, as such, one technology in the infrastructures often declared to make people at the border visible by allowing states to collect — and connect — different kinds of biographic and biometric data to distinguish between populations for different treatments [e.g. @bestersGreedyInformationTechnology2010; @dijstelbloemBorderSurveillanceMobility2015]. However, these examples of research on technologies on the alleged invisibility of people show a tendency to adopt the authorities’ perspective. Less attention has been given to the invisibility of those informational representations in infrastructures that allow the management of mobility, migration and border control. Therefore, what is less clear is how we can make these parts of the infrastructure visible to retrace what and how specific forms of knowledge are represented.

We can compare different standards for similar phenomena to retrace what might be lost or made invisible in developing data models and standards. @cornfordRepresentingFamilyHow2013, for instance, has compared digital standards that have been developed as public service initiatives in the United Kingdom for representing family relationships. Their comparison highlights ‘the kinds of family relationships that are recorded and those that are not recorded or harder to record, any hierarchies, implicit or explicit, for family forms or relationships and the implicit and explicit assumptions that underlie the terms and classifications used’ (p. 8). In our case, this means comparing data models for representing persons that are embedded in different authorities’ information systems for migration and border control.

Data models and standards cannot always be compared separately, as often it is only through the interplay between different standards that subjects are enacted as new kinds of entities. @ruppertNotJustAnother2013, for example, has studied information systems in the UK to share data on young persons at risk of (re-)offending and to allow interventions. In her study, she shows how the "joining up" of data stored, which includes various biographical information from different agencies, produces a "subject multiple". Drawing on the work of Jane Bennet, Ruppert sees the consequence of corresponding data as then not just from a sum of the individual parts, but that is a part becoming something else. In the case of border and migration control, someone may, for example, only become an irregular migrant by connecting identity data from different databases.

The politics of data model design shapes how information is represented and affects what becomes visible and, consequently, what becomes invisible [@hansethInscribingBehaviourInformation1997; @bowkerSortingThingsOut1999]. Therefore, discussing the invisibility of humans must also include the invisibility of those data infrastructures that define how people become visible and invisible. The first infrastructural strategy has therefore suggested a need to examine infrastructures‘ informational representation and recover choices made in these technical standards that quickly become invisible yet are continuously involved in processes of enacting populations, territories, and borders.

The rise of Big Data has prompted, among others, the field of Critical Data Studies (CDS) to renew the call to make visible other dimensions of data related to the volume, variety, and uses of data[@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. Such data practices are never entirely local, and information technology, including standards, in this sense, have become powerful methods to trace and connect sites [@latourReassemblingSocialIntroduction2005]. Therefore, the next infrastructural inversion strategy will consider another vital element to connect sites: the data practices related to data management and linking identities within and across organizations.

### Second inversion strategy: data practices

Data are never "raw" but always constituted by different choices and constraints [@bowkerSortingThingsOut1999; @gitelmanRawDataOxymoron2013]. In addition, techniques and practices of managing and cleaning data should be analyzed as part of the ‘cooking’. In recent years, an increasing amount of literature has shed light on how data are generated, circulated, and deployed [@iliadisCriticalDataStudies2016; @kitchinCriticalDataStudies2018]. However, scant attention has been paid to practices and techniques used to match and link data in security settings — such as probabilistic record linkage, fuzzy matching, and data deduplication. This is surprising because by bringing different data into relation with each other, the tools recontextualize data and can have organizational consequences such as redistributing tasks and responsibilities between different actors. There is, therefore, a clear need to understand how such technologies for matching identity data are put into practice.

Similarly, there is a growing interest in more fine-grained analyses of knowledge production mechanisms and practices of digital technologies in border and migration management and how they enact, among other populations and states [@mcharekTopologiesRaceDoing2014; @jeandesbozSmarteningBorderSecurity2016; @glouftsiosGoverningCirculationTechnology2018]. Contributions from two recent Special Issues [@cakiciPeoplingEuropeData2020; @scheelEnactingMigrationData2019], for instance, took a practice-based approach to analyse data practices and pay attention to the mechanisms through which knowledge enacts migration. Through the concept of "alterity processing", @pelizzaProcessingAlterityEnacting2019 has provided a framework to account for the multiplicity of ways non-European populations are enacted when reaching Europe and their simultaneous enactment with institutions through data practices and infrastructures. As such, these different authors question the kind of worlds the data practices help to make — and potentially engage with ontological politics to think how they could be enacted differently [@lawEnactingSocial2004].

Overall, examining the role of data matching practices and technologies is essential to understand much contemporary coordination work within information infrastructures [@leeHumanInfrastructureCyberinfrastructure2006; @ribesSociotechnicalStudiesCyberinfrastructure2010; @monteiroArtefactsInfrastructures2013]. In the case of data matching practices, the activities and technologies that make data findable, shareable, and connectable are frequently crucial to make possible distributed cooperative work. In the context of scientific work, for example, scholars have shown the — often invisible — efforts needed to make data sets shareable and useable by others [e.g., @edwardsScienceFrictionData2011; @kervinBackstageWorkData2014; @plantinDataCleanersPristine2019]. A data practice approach thus makes it possible to empirically explore how data quality practices and technologies support and shape data practices in collaborative work within and between organizations.

Some questions still need to be answered as we transition to situated data practices grounded in practice theory concepts. Rather than technologies, people or other actors, practice theories take social practices as the central topic of inquiry [@reckwitzTheorySocialPractices2002; @schatzkiIntroductionPracticeTheory2005; @shoveDynamicsSocialPractice2012]. Only recently have the materiality of technological artefacts been incorporated as "aspects of practice-arrangement nexuses" in which the materiality and practices are intertwined and constitute each other [@schatzkiMaterialitySocialLife2010]. Taking data matching practices as an analytical starting point can focus attention on the performativity of practices as they are situated and entangled with technological artefacts [@ruppertDataPracticesMaking2021]. Analyzing data practices, such as processing an asylum application based on information collected about a person by different organizations and agencies, can then highlight how tools support this work to link data from different sources. At the same time, such data practices of cleaning duplicate data may flag some identity data as irregular. Nevertheless, while practice theory has been very productive in understanding the situatedness of practices, debate continues about how social practices travel, how different practices are related to each other and other "large phenomena" [@shoveMattersPractice2016; @nicoliniSmallOnlyBeautiful2016].

Following @schatzkiIntroductionPracticeTheory2005, practice approaches can either elaborate on the interconnected practices of a subdomain of human activity or consider practices as "the place to study the nature and transformation of their subject matter". This latter view is supported by @shoveDynamicsSocialPractice2012, who proposed that "the dynamics of social practice" can help understand social changes by looking at the change and stability of practices. In this way, examining the dynamics of data practices for matching identity data can help understand developments of identification in security settings. Operationally, these dynamics of data matching practices can be examined by investigating the dynamics of data practices: how they change, stabilize, and travel.

Analyzing the dynamics of data matching practices can show how technologies are intertwined with these practices to shape, transform, and stabilize identification in transnational security infrastructures. The second infrastructural inversion strategy to make visible the matching identity data in transnational security infrastructures foregrounds usual backstage elements as work practices of managing, linking, and cleaning data. However, difficulties arise when attempts are made to understand how the entangled technologies have evolved and moved across contexts and organizations. The third inversion strategy will thus focus on how to make visible the processes of social shaping of technologies.

### Third inversion strategy: lifecycle of technologies

For a long time, STS scholars have chronicled the choices and directions of the design of technological artefacts and systems through different approaches to examine and understand the nexus between users, technologies, and designers. Social constructivists, in particular, have emphasized the social construction of technological artefacts (rather than human action determined by technologies) [@mackenzieSocialShapingTechnology1985; @pinchSocialConstructionFacts1984]. This scholarship accordingly emphasized how the construction of technologies is tentative and shaped by specific contexts and actors in which the artefact emerges [@jacksonSocialConstructionTechnology2002; @pinchTechnologyInstitutionsLiving2008]. In addition, concepts such as "interpretative flexibility" [@pinchSocialConstructionFacts1984] and "boundary objects" [@starInstitutionalEcologyTranslations1989] emphasized how different social actors and groups attribute different meanings to artefacts and how technologies are put to use. As such, these concepts are crucial to analyze and contrast the design and evolution of the technologies and how actors perceive, use, and configure the tools for searching and matching data.

Scholars have proposed various concepts to compare the development of technologies with how actors use technologies. Concepts such as "script" have been introduced to account for the gaps between designed, intended uses (a "script") and actual uses. In contrast, concepts such as "configuration" emphasize how expectations and imaginaries of designers are inscribed into technological artefacts and how the users and artefacts are figured together — delimiting and enabling certain forms of agency. However, critics question the ability of some of these approaches to account for the longer-term design and evolution of technologies [@williamsMovingSingleSite2012]. This scholarship argues that technologies cannot be studied through a single "snapshot" in time. Instead, they argue that there is a need for a "biography of artefacts and practices" [@pollockSoftwareOrganisationsBiography2009].

In summary, drawing on this literature, there is a need to investigate data matching technologies and practices in a way that allows taking, at the same time, a situated and a more longitudinal approach to investigate how they are shaping and shaped by transnational security infrastructures. Different moments in the life of the data matching technology and practices should be addressed.

## A conceptual framework for analysing data matching in distributed settings

The three strategies for infrastructural inversions are now combined into a conceptual framework for analysing data matching practices and technologies in transnational security data infrastructures. Such a framework is needed since no single method or theory can be adapted to examine the multiple facets of data matching practices and technologies within and across organizations. Therefore, this section proposes a new conceptual framework that combines established methods and theories into one framework that can guide the research in examining those multiple facets. The conceptual framework, in essence, integrates more computer science views on linking and matching data models and records with social science views from fields associated with Science and Technology Studies (STS).

As previously said, for government agencies, businesses, and other organizations, finding records in databases that refer to the same person can be important in many situations. In addition, as organizations, states, and agencies have been collecting more data about people, they may perceive the performance benefits of having complete and accurate information about people or even see it as beneficial to national security. The techniques for data matching — such as the mathematical models for linking records — are therefore well established. However, the creation, development, and consequences of deploying technologies for data matching are less well understood and researched. The framework outlined in this section aims to address this gap in the research. It does this by tracing data matching practices and technologies as sociotechnical phenomena from four different angles, corresponding to the four research questions: (1) how the kinds of data that are collected about people can be compared and what this tells us about how organizations can search, match and use this data, (2) how within organizations search and match for data about people that may have data quality issues, (3) how such identity data is matched between organizations, and (4) how, simultaneously with matching identity data, data matching technologies and practices travel and evolve across organizations.

The conceptual framework lends itself to a three-dimensional graphical representation. (Figure \@ref(fig:method-axes-visualization)). This visualization conforms roughly to a relational model representing data as tables that model relationships between data. The three axes capture different data dimensions: _data models_, _categories of data_, and _data values_. Next, we will discuss how these three dimensions, and the relationships between them, are relevant to different facets of data matching within and across organizations.

```{r method-axes-visualization, echo=FALSE, fig.cap="The conceptual framework is represented graphically in three dimensions in this visualization.", out.width="100%", fig.align="center"}
knitr::include_graphics("figures/method-axes-visualization.pdf")
```

The axis for _data models_ (axis `z`) represents the database and other informational models that standardize the kinds of information about people collected by different organizations' IT systems. An example of such a data model could be the schemas specifying data collected about migrants by a government agency. Data models are typically developed within a company and are not available to the public. However, there may be a need for interoperability between organizations, which could be achieved through the use of common data models. Different data models define different _categories of data_ (axis 'x'), which are the characteristics of the people collected. For example, the categories of data of the example asylum agency’s data model may include "name", "place of birth", and "date of birth". Finally, there are the actual _data values_ (axis `y`) in systems’ databases for a data model and its categories of data. For example, a value for the data category "place of birth" may be "Brussels".

To this end, the various combinations of these axes are schematized to represent the relevant aspects of data matching. Three axes in Figure \@ref(fig:method-axes-visualization) each focus on different aspects of data matching both within and between organizations.

### Matching data models

The first pair _(data models `z`, categories of data `x`)_ represents the identification of semantic similarities and differences between different data models and their categories of data. Such a process will also be used in the research approach to support discursive analyses of data models. In this way, the analysis intends to compare the expectations and imaginaries of diverse actors concerning data collected about people and the ways data can be searched, matched, used. For example, the data model of a government agency may contain the category of data ‘family name’ while another agency uses ‘surname’ as a category of data. Even though the names of the data categories are distinct, they refer to a similar concept. In this case, a part of someone’s name indicates their family. To connect information about people from these two agencies’ systems should therefore identify similarities between their data models and these two categories of data.

This first aspect of matching people’s identity data addressed by the conceptual framework is the relations between different information systems’ data models. That is, how different data models for collecting information about people-on-the-move are situated in relation to one another and what this tells about the organizations which developed and use these models to collect, search, and match identities.

The method draws, first, on studies of classification and its consequences which have addressed the question of how to retrace the ethical and political work of otherwise mundane devices of representations — i.e., data models. Researchers have previously used and combined various makeshift methods — from narrative interviews [@gazanImposingStructuresNarrative2005] to discursive textual analysis [@caswellUsingClassificationConvict2012], from participant observation [@meershoekConstructionEthnicDifferences2011] to archival and genealogical research [@gassonGenealogicalStudyBoundaryspanning2006] — for the ethnographic and historical studies of information systems [@starStepsEcologyInfrastructure1996] and their classifications. Furthermore, it is necessary to take a more interdisciplinary approach and pay close attention to the technical details of classification to avoid only considering the effects of classification [@kitchinCodeSpaceSoftware2011].

Following Geiger and Ribes [-@geigerTraceEthnographyFollowing2011], data models can be considered "thin" traces: being relatively standardized schemas made of categories and values, they are hardly meaningful in themselves. Therefore, the method to analyze them should consider how to turn them into "thick" data. Nevertheless, given their implementation in diverse infrastructures, data models can be an excellent starting point to understand how geographically distributed sites are connected [@latourReassemblingSocialIntroduction2005]. Burns and Wark [@burnsWhereDatabaseDigital2020], for example, have dubbed such an approach "database ethnography" and used traces left behind of a database as a site to analyze how social meanings of phenomena change over time — but, only using their ad-hoc mix of methods.

On the other hand, matching data models share concerns with methods developed in computer science fields interested in the use of semantic technologies, such as knowledge engineering, linked data, and natural language processing. For example, the relationship between the different types of categories and their variations is closely linked to the outcome of schema or ontology matching processes, which finds correspondences between concepts of schemas or ontologies [@euzenatOntologyMatching2007; @kementsietsidisSchemaMatching2009]. Another linked technique called ontology learning aims to automate processes for creating ontologies. These (semi-)automatic mechanisms then include extracting concepts and relationships between concepts from a domain of discourse by analysing a corpus of documents in that domain. There are thus shared concerns about how to examine how different knowledge representations interrelate and how to recover such representations from a discursive domain. However, fundamental differences make it challenging to support a discursive analysis.

Knowledge engineering methods are generally more concerned with creating technologies that help machines understand better means for the systems and integrating different sources of information work. By contrast, the method for this research needs to support discursive analysis by taking differences in representations as a starting point to analyze imaginaries of diverse organizations and authorities and to combine the analysis with ethnographic observation. Therefore, there is a need for approaches that can support analyses of formalized data models in two ways. First, to support analyses of information systems which define their data models, even if these systems are only sometimes comparable. Second, to systematically and quantitatively support discursive analysis of "thin" data models by detecting differences and absences between systems.

### Data matching within organizations

The second pair _(categories of data `x`, data values `y`)_ relates to the categories of data and their actual values in databases, which may not always correspond to the expectations of the data models. For example, when values for the categories "first name" and "last name" are mistakenly switched in the database. Practices and technologies may therefore be employed to remediate such data quality issues — i.e., data matching. Overall, two applications of data matching within organizations and their databases can be distinguished: real-time data matching and deduplication. Real-time data matching mainly addresses the issue of retrieving information about a person through search queries. For example, police officers may need to query databases based on the categories of data "name", "nationality", and "date of birth" to see if approximate matches exist for those personal details. Data matching techniques will be used to find similar matching identities’ data rather than only exact matches. On the other hand, deduplication is the use of data matching technologies for identifying records in a database that refer to the same person and fusing multiple records.

On the one hand, it is crucial to understand the moments when actual data are not aligned with the expected data model. For example, when multiple records were created for a person due to name variations. In this way, the relationship between data expected by data models and actual data in databases can draw attention to the data practices of how organizations collect and use knowledge about people-on-the-move. For example, they use technologies for searching and matching to help deal with data uncertainties deriving from frictions between the moments of collection and use of data. On the other hand, the design and use of these tools for dealing with such data matching need scrutiny. The design and use of such data matching tools can be compared to understand the relations between users, designers, and technologies. Such comparisons can identify expectations and imaginaries of designers that are inscribed into technological artefacts and how they are all configured together.

Scholars in STS and beyond have devised different approaches to examine and understand the relations between users and technologies [@oudshoornHowUsersMatter2003] and how expectations and imaginaries of designers can be inscribed into technological artefacts. For example, the approach of script analysis was advanced to examine users' assumed competencies and affordances embedded in artefacts [@akrichSummaryConvenientVocabulary1992; @latourWhereAreMissing1992]. The script of an artefact then requires users to adopt designers’ envisaged behaviours and actions to interact with an artefact. This approach also allows accounting for situations when those assumptions by artefact designers do not match the actual uses and practices. @woolgarConfiguringUserCase1990 furthermore noted how system designers might attempt to ‘configure the user’ of the system by incorporating the user into the sociotechnical system. The method to understand such configuration is by treating the computer systems as a text that is read by users and can be interpreted differently. Designers may thus attempt to anticipate and delimit this flexibility. In these views, a sociotechnical system functions well when users and the system are successfully configured.

Suchman [-@suchmanHumanmachineReconfigurationsPlans2007] furthermore argued that the inscriptions of users and uses are never that coherent and that a more open-ended and indeterminate approach towards artefacts is required. She proposes configuration as a "method assemblage" [see also @lawMethodMessSocial2004] that pays particular attention to technologies and "the imaginaries and materialities that they join together" [@suchmanConfiguration2014, p. 48]. Configuration as a methodological device can thus be, per Suchman’s view, practical to unpack software's material and discursive elements for dealing with data uncertainties.

Therefore, this axis in the conceptual framework aims to answer what kinds of imaginaries and materialities are joined together in data matching technologies. Analysing and comparing the technical designs with actual uses, and thus also where they mismatch, can give evidence of the various imaginaries, expectations, and assumptions at play. However, the approach also needs to be both specific to the situatedness of the design and keep an open-ended and indeterminate view of how such technologies are used [@suchmanConfiguration2014].

### Data matching across organizations

Lastly, the pair _(data models `z`, data values `y`)_ addresses data matching across organizations and agencies. Identity data records that refer to the same person may exist in systems and databases of different organizations and government agencies. Data matching, in this case, refers to the data practices related to identifying and potentially linking or merging such identity data spread across various databases. A crucial challenge for data matching techniques is that unique identifiers are only sometimes available; hence, matching relies on personal information, which is only sometimes complete or accurate. In addition, personal information such as names and dates of birth frequently contain typographical errors and variations. For example, a woman may have used her maiden name as a surname in one system while information collected in another database uses her husband’s surname after marriage. However, the question for us here is how — by bringing such data in relation to each other — data matching practices and technologies shape the relations between different actors.

Furthermore, these data matching practices and technologies cannot be studied independently of the infrastructures in which they are embedded. Data matching technologies can be crucial to make particular collaborative work possible, such as processing an asylum application based on information collected about a person by different organizations and agencies. At the same time, cleaning and managing data can often become invisible work.

Based on the insights from infrastructural studies, data matching technologies in security fields can become an inconspicuous element in transnational data infrastructures. This insight is in line with current debates in International Relations and Security Studies, which have started to acknowledge the role of technology in international politics [@amicelleQuestioningSecurityDevices2015;  @bellanovaAlgorithmicRegulationSecurity2022; @hoijtinkTechnologyAgencyInternational2019]. Crucially, these scholars extend agency to the technologies and artefacts used in security practices. For example, @glouftsiosGoverningBorderSecurity2021 demonstrated how the often-invisible maintenance of the large-scale information systems for border security in the EU has a crucial role in sustaining the governance of international mobility.

A central premise of the dissertation is that data matching technologies can instigate or resolve debates over which actors produce the most reliable data. For example, when matching identity data from different agencies, these tools will bring certain records in relationship with each other. If there are uncertainties about the data, they may play a role in disputes about which agency’s data are correct. The hypothesis is that data matching technologies can thus inscribe certain forms of agency that can reconfigure practices of security and migration in transnational data infrastructures. Infrastructural inversions are crucial in calling attention to everyday data management practices.

### Travelling data matching software

Furthermore, and related to the last pair, is the question of how identity data are matched across systems and how technologies and practices for data matching travel across organizations. The underlying premise is that as companies develop tools to support these tasks and deploy them at multiple locations and times, the concomitant knowledge of managing typographic errors, name variations, and missing data may also travel across organizations. This axis is consequently related to how data are not only matched between different organizations and agencies in security contexts. Instead, the hypothesis is that, at the same time, knowledge and technologies for such data practices travel between organizations.

Although the notions of "script" and "configuring the user" have been generative in understanding the relations between users and technologies, they tend to overemphasize the role of designers [@oudshoornHowUsersMatter2003]. More recent approaches have therefore suggested, for example, examining moments and sites of interactions in the life and entanglement of artefacts between users, designers, and other actors [@hyysaloNewProductionUsers2016]. In addition, an alternative approach has been developed by scholars called the "Biography of Artefacts and Practices" (BOAP), which can detail the complexity of the development of such technologies and related practices.

Following the BOAP approach, the emphasis of this aspect in the conceptual framework is to understand the biography of data matching software packages. This approach can highlight the wide range of actors involved in developing such security technology and how the software travels and evolves over extended periods and between different sites. BOAP expands the narrower focus of the user-design-technology nexus used in the second axis to take a more comprehensive view of how the technology has been shaped over time and by a broader number of actors at multiple sites.

The conceptual framework serves as heuristics for guiding the analysis of data matching in transnational security data infrastructures. Therefore, a combination of methods was employed to operationalize the framework for the different axes. The research thus took advantage of well-established methods from the areas of Human–Computer Interaction (HCI), Computer Supported Collaborative Work (CSCW) and sociotechnical research to investigate the relations between the technologies, users, and organizations. Therefore, the study relied on methods such as document analysis and user interviews to gather and analyze data. These methods informed each other at different points throughout the research.

<!-- > _To-do_. This section will give an overview of how the conceptual framework will be applied and tested. Essentially, I will introduce how each axis of the framework is operationalized. The methods to do this are then further detailed in the following sections. -->

## Fieldwork site and access

### WCC ELISE — A technology for matching identity data

Looking into WCC's ELISE software would meet the needs of the overarching research questions of both research projects (Processing Citizenship and this PhD dissertation), as well as WCC and its clients. First, recall that the PC and PhD projects have as one of their main goals to examine how identity management information systems support the creation of knowledge about non-European populations, and the difficulties face. As a result, the technologies developed by WCC and deployed at government agencies and (international) organizations concerned with identity and security were selected as a starting point for learning about such knowledge production processes. In light of this, the underlying hypothesis was that researching the development and implementation of this software technology would shed light on the problems of cross-organizational data matching and identity management in secure environments. Second, the study's findings could shed light on how WCC's clients use the company's technologies. Such insights are valuable to WCC to (critically) advance its technologies.

The research proposal presented to WCC focused specifically on deployments of the ELISE ID platform — previously known as ELISE Smart Search & Match — at customers in WCC’s security and identity domain. The ELISE software solution serves, in fact, as a core technology in all solutions developed by WCC, ranging from products for border management to public employment services. In short, the platform provides sophisticated functionalities for data matching, such as searching and matching personal identity information for different data sources. The novelty of the technology comes from using various kinds of fuzzy logic algorithms for data matching that consider that data may be incomplete or inaccurate. The term "fuzzy logic" in this context refers to a type of mathematical logic that computes truth variables using probabilities rather than boolean "true" or "false" values. For this reason, the ELISE ID platform returns search results as numbers showing how likely two sets of identity information are identical or "match".

In short, the goal of the fieldwork was first to understand how the ELISE identity-matching software influences organizations' identification practices. The leading hypothesis was that embedded assumptions in the tools, to a certain extent, influence how organizations identify people — such as how users think about data, the uncertainty of establishing identity and how this shapes their work practices. During the fieldwork, evidence of these dynamics was discovered by contrasting the tool designs with how various users employ the biographic matching and multicultural name-matching features in their everyday tasks when looking for identities. The hypothesis was that contrasting software with actual use would show how software influences users' perceptions of data quality and the associated levels of uncertainty. These expectations may be the source of tensions in the imagined and actual use of the search and match functionalities. In this way, this first goal would provide evidence for the second axis of the conceptual framework — the design and use of data matching within organizations. The research, therefore, draws inspiration from STS approaches to study the possibilities, constraints, and resistances of devices embedding prescriptive representations [for an overview, see @suchmanHumanmachineReconfigurationsPlans2007].

The second goal of the fieldwork was to understand the processes and practices of developing and deploying such a software package for matching identity data. This goal corresponds to the fourth aspect of the conceptual framework, i.e., the knowledge and technologies for matching identity data travel and circulate. Therefore, the hypothesis was that the software for searching and matching identity data would embed particular imaginaries and assumptions that evolved while circulating and interacting between different actors, such as the company, their customers, competitors, and other organizations. For this second goal, the research identified moments in the development and deployment of the software package. For example, it is crucial to understand the work performed to match identity data across different contexts and organizations. That is, we should question the generic nature of the software package and instead focus on the struggles and accomplishments to make it so. This second, as such, draws upon the perspectives developed by the "biographies of artifacts and practices" (BOAP) framework urges to understand technological development as a complex and dynamic process happening across time, locales, and involving various actors [@hyysaloMethodMattersSocial2019].

To sum up, the central hypothesis guiding this fieldwork research was that users, their work practices, and organizations all shape and are shaped by this specific type of searching provided by the solutions, which assumes uncertainty about data and incorporates specific imaginaries (a "smart search and match"). Hence, one needs to adopt both broad and specific perspectives to trace such intricate dynamics [see also @suchmanConfiguration2014]. A more specific perspective can look at specific times of use and building tools for searching and matching. In addition, a broader perspective considers the varying and often elusive factors that influence software development.

### Deployments at the IND and EU-VIS

The overarching hypothesis was tested by investigating the deployments of the solutions at two customers of WCC's software solutions in national and transnational settings. First, the integration of the ELISE ID platform with the systems of the immigration and naturalization service of the Netherlands (IND). Second is ELISE's integration into the EU Visa Information System (VIS). As mentioned previously, we should be careful in distinguishing between the national and the transnational. The social methods employed for this inquiry not only describe realities but are a part of the "ontological politics" of enacting certain kinds of realities [@lawEnactingSocial2004]. The aim is, therefore, to avoid reproducing fixed distinctions between national, transnational, or international. Instead, following Law and Urry, the question is to which realities we want to contribute. The choice, in this case, is instead a hotchpotch of actors and technologies without clear boundaries of national or transnational.

The search and match solution's use was deemed relevant to understand how the tools support and shape the practices of knowledge production related to the identity of people. In the case of the IND, the agency is responsible for, among others, processing the applications from people who want to stay in The Netherlands or who want to become Dutch nationals. The use of the ELISE relates to these responsibilities. The IND information system leverages the ELISE ID platform to facilitate searching for applicant data against the biographic information in their back-office system. On the other hand, the EU-VIS system has a narrower scope in its use of biographic matching and multicultural name-matching capabilities for its search engine. At the same time, due to its transnational scope, the amount of records in the EU-VIS database is much higher.[^eu-vis-capacity]

[^eu-vis-capacity]: The "Report on the technical functioning of the Visa Information System (VIS)" [@eu-lisaReportTechnicalFunction2020] notes a database capacity of 60 million records.

The methodological choice of tracing the development and deployment of the ELISE software package for these two organizations necessarily has some "framing effects" [@hyysaloMethodMattersSocial2019]. Although WCC serves a variety of clients, including both public and private organizations like employment agencies, the research specifically focused on the "Identity and security" market segment. Furthermore, not all identity and security customers were possible to research due to issues such as confidentiality. At first, it seemed like WCC's work in other sectors beyond identity and security was unimportant for the research. However, links with customers from other sectors repeatedly emerged during the fieldwork and interviews in discussions about the genericness of the search and match. This finding will be important in Chapter 6, which investigates the origins of the search and match software's ability to be applied in various contexts. Chapter 6 will also show how other actors beyond the research's initial framing, such as research centres, competitors and potential customers, directly and indirectly influence the company and its products. The research gradually introduced these ancillary actors through methods like attending industry events and relying on other sources, as detailed in the subsections \@ref(events) and \@ref(other-observations).

## Methods for data collection

### Traces of data models

An essential aspect of investigating the matching of identity data across organizations is how data models of different systems correspond. The first step to match people's data within and between organizations includes finding correspondences and similarities in the types of information stored in the respective databases. For example, two systems may store a combination of a person’s last name, date of birth, and nationality. In order to compare these individual identifiers, it is necessary to deal with the different data models that each have their labelling, types, and formats of attributes. Generally, such data models and their categories of data, i.e., the labels describing a state that can assume different values, are specified in various documents, from database schemas to design documents or even regulations. This section will review the procedures for gathering the data model traces needed for the analysis. In general, desk research or fieldwork enabled the collection of such data model traces. The next chapter details a digital method to compare diverse descriptions of data models through additional steps to code, harmonize, and group all documents, categories, and values.

All those steps contributed to developing a method and tool to compare data models utilized in national and international information systems that jointly work to support registration and identification practices at European border zones. Regarding the traces of those models, I draw on data collected during fieldwork conducted in the context of the Processing Citizenship project at border zones in Europe. Given linguistic constraints and the Project’s task plan organized as a matrix, some documents were collected by other researchers employed as collaborators in the Processing Citizenship project.[^pc-team] Overall, the data collection efforts included desk research of European regulations, technical documents made available by European and German authorities, systems screenshots collected at border zones in the Hellenic Republic, and technical documents collected during fieldwork in The Netherlands.

[^pc-team]: These researchers include: A. Bacchi, E. Frezouli, Y. Lausberg, C. Loschi, L. Olivieri, A. Pelizza, A. Pettrachin, S. Scheel.

For transnational security infrastructures such as the information systems developed by European Commission agencies in the Area of Freedom, Security and Justice (AFSJ), the issue of data matching is becoming increasingly important. Ongoing projects to increase information-sharing and interoperability of information systems in this area require various new forms of data matching. As such, international actors are developing a central infrastructure to store links and references to EU information systems. In this new architecture, identity data from previously separate systems will be matched against each other to link them, facilitating identity checks and detecting identity fraud. Besides these new types of data matching under development, the existing systems already do real-time data matching, such as when searching for personal details of a person by a police officer. The research draws upon the regulations establishing each system to explore the relations between the categories of data recorded in these three EU systems.

Data collection, therefore, includes documents on the regulation of the three international information systems developed by European Commission agencies: Eurodac, the Schengen Information System (SIS) and the Visa Information System (VIS). All three systems have specific aims in supporting policing tasks related to travel, cross-border crime and irregular migration. Eurodac (_European Dactyloscopy_) aims to support the identification of asylum seekers through fingerprints and to determine the Member State responsible for processing their asylum applications in the context of the Dublin System.[^dublin-iii] The database was established in 2003 to store asylum seekers' fingerprints and other basic data. It is currently operational in the Member States of the European Union (plus Norway, Iceland, Switzerland, and Liechtenstein). Law enforcement agencies and Europol now also use the system, expanding its original scope significantly since its inception [@ajanaAsylumIdentityManagement2013]. New proposals aim to include more biographic and biometric data, including a facial image.[^recast]

[^dublin-iii]: The Dublin System (Regulation No. 604/2013; also known as the Dublin III Regulation) establishes the criteria and mechanisms for determining which EU Member State is responsible for examining an asylum application.

[^recast]: Procedure 2016/0132/COD, recast of the Eurodac Regulation.

The purpose of the Schengen Information System (SIS II) is to support external border control and law enforcement cooperation in the European Union. It supports this task by storing alerts which contain information on persons and objects, and instructions on what to do when officers encounter such persons or objects. In addition, the SIRENE network can exchange this information between law and border enforcement authorities. Finally, the Visa Information System (VIS) allows for the exchange of visa data (including personal data and biometrics). In this way, it aims to maintain a common EU visa policy. Authorities use VIS data in the context of border identification procedures. Access is thus different from the Eurodac and SISII systems, as authorities can only access VIS data on a case-by-case basis. Authorities at the external border of the European Union also have access for purposes such as "verifying the identity of the person, the authenticity of the visa or whether the person meets the requirements for entering, staying in or residing within the national territories" [@europeancommissionVisaInformationSystem]. Furthermore, asylum authorities use VIS data to determine which EU Member State is responsible for examining the asylum application.

Concerning national systems, the analysis includes the systems of the Dutch (DRF), Hellenic (HRF) and German (GRF) Register of Foreigners. The DRF is an information system that stores data of people who relate to the Dutch government in the context of the Aliens Act ("Vreemdelingenwet"). The system allows the digital exchange of identity data, information about travel and identity documents, biometric characteristics and status data between the partners. Various ministries and organizations in The Netherlands use this system to share and consult the information about foreign nationals. As such, it acts as a single, authoritative source of information (_basis registratie_ in Dutch). When it comes to a person's data, the DRF is always the authoritative source, even if other partners also have their database [@ictuInformatievoorzieningVreemdelingenketen2015]. The DRF also issues "v-numbers", a universally recognized method of identifying foreign nationals in the Netherlands. In practice, establishing and verifying identities involves an interplay of multiple systems. For example, IND users searching for person matches need to consider results from this system.

The HRF is the primary system used at border zones in Greece to identify and register persons who arrive at the border without the required documents. Furthermore, officials use the system to support various tasks during the identification and asylum procedures, such as retrieving migrants' biographic and biometric data, conducting screening and asylum interviews, and assessing health conditions. Therefore, the systems' users include police, administrative personnel, and asylum officers.

The GRF is a database which contains a large amount of personal information of foreigners in Germany who have or had a residence permit, as well as those who seek or have sought asylum or are recognized asylum seekers [@bvaAuslanderzentralregister]. Various partner authorities and organizations in asylum, migration, and border control access this central register. The XAusländer standard, a data exchange format which formalizes and enables data exchange between the immigration authorities in Germany, describes the data sent to the GRF during the first registration[@bamfStandardXAuslaender]. According to the description of the standard’s motivation, it aims to facilitate the exchange of such data between authorities in Germany to reduce data re-entry and enable the reuse of such data by the authorities.

All the information systems described above define their data models and the kinds of people they aim to represent in ways that are not immediately comparable. The next chapter will introduce a digital method called "The Ontology Explorer" used to extract, analyze, compare and visualize diverse data models. The chapter will also include findings from the method's analysis of the data models from the systems mentioned above.

### Document analysis

Understandings developed by STS of documents and practices of documenting as objects of research ground the document analysis related to the deployment of ELISE. Various STS scholars have expanded our conception of documents as objects that do more than just record information, according to @shankarRethinkingDocuments2017's analysis of the relevant literature. Instead, scholars remarked on how documents play an active role in social contexts, such as accounting for and coordinating workplace activities. Documents, in this sense, are inseparable from the processes that generate them. Therefore, the dissertation's document analysis looked not only at documents to help explain how the software package functions. Rather, documents help understand the organizations as their records "refer to the practices, objects, rules, knowledge, and organizational forms that produced them" (ibid., p. 62). In this way, the findings from the document analysis contributed to an understanding of the biographical moments in the life of this software package.

Several on-site visits to WCC's headquarters in Utrecht, the Netherlands, made it possible to consult ELISE-related documents such as technical design documents, product brochures, and meeting minutes. On the one hand, documentation related to the more general technical particulars of the ELISE ID platform made it possible to become acquainted with the software suite and how WCC designed it. On the other hand, documents related to the platform's specific implementations at the IND made it possible to understand how WCC practically configures the software package for customers. In addition, WCC's "ID team" provided additional context for the documents and updates on the status of ELISE integration in the EU-VIS and INDiGO systems.

In general, the document analysis had two broad aims, corresponding to two different aspects of the conceptual framework. The first aim was to find evidence of friction between design and use of the tools by comparing how different users at the IND use the biographic matching and multi-cultural name matching capabilities in their daily tasks when searching for identities. We can investigate question three by contrasting the software package's anticipated and actual uses. In other words, we can understand how each affects software and users' expectations about identification and the uncertainty surrounding identity data. These expectations may be the source of tensions in the imagined and actual use of the search and match functionalities. Chapter 5 shows the differences between the designed probabilistic identity match and the user’s understanding of, for example, the search results.

The second aim of the document analysis was to unearth moments in the trajectory of the software package. Fundamental insights from scholars in this regard are that instead of looking at single document versions, we should pay attention to the processes of documenting [@shankarRethinkingDocuments2017]. For example, we can retrace documents' histories by looking at different document versions, such as changes within documents or added annotations. The group of documents related to the deployment of ELISE at the IND included different versions of the design document, documents that described updates to the package, and meeting notes about changes to the configuration. However, most documents lack a clear provenance. As a result, determining the motivations or source of a document can be challenging. For instance, the IND project documentation that WCC kept dates back more than ten years, and the staff members who worked on it were no longer with the company. As a result, recovering the context of some of these documents and their origins and motivations could be complex, even for current WCC employees.

Rather than seeing individual authors, we should consider documents as assemblages of different authors and sources. That is why documents help understand how knowledge and technologies for matching identity data travel and circulate.  Documents often lack a clear origin and are instead "assembled from multiple sources [where] content often flows from application to application and document to document, constantly recycled, reworked, and repackaged" (ibid., p. 63). In the ELISE documentation for the IND, such flows were readily identifiable. For example, technical documents include IND-specific implementation details alongside package-specific information from other company documents. Thus, documents and documenting practices can help answer research question four. The software's history, including its distribution, modification, and adoption by different groups, is documented in various forms throughout the project's lifecycle.

The absence or inaccessibility of documents is a notable, albeit less visible, aspect of documentation. For example, it became clear during fieldwork that documents for the deployment of ELISE in the EU Visa Information System would be inaccessible. The specific arrangement of actors involved in developing the EU-VIS systems resulted in this lack of access. In this arrangement, WCC was the technology supplier and collaborated with Accenture, the technology integrator. This arrangement meant that the relevant documents were in the hands of the technology integrator. Moreover, even for WCC, the technology integrator acts as a gatekeeper, deterring access to such materials. Nevertheless, the absence or inaccessibility of documentation can thus provide insights into the work of actors building these systems.

In short, the document analysis made it possible to form a first picture of the technical functioning of the search and match solution and the specific context of its deployment at the IND. On the other hand, the documents give insights into the package's genealogy and the practices of configuring, deploying, and designing the software. Table \@ref(tab:documents) gives an overview of the documents consulted. In addition to the more technical documents, public communications and reports provided valuable insights into the specific context of the IND information system developments. The outcome of the document analysis served as input for developing the interview protocols.

```{r documents, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "documents.csv")
data = read.csv(path, header = TRUE)

dt = data[]

# pander(dt)

kbl(data)

# kbl(dt, booktabs = T) %>%
  # kable_styling(latex_options = c("striped", "scale_down"))

# block_table(data[,c("Document", "Author", "Year", "Description")], header = TRUE, properties = pt)
# ft <- flextable(data)
# ft <- add_header_row(ft,
#   colwidths = c(4, 2),
#   values = c("Document", "Author", "Year", "Description")
# )
# ft <- autofit(ft)
# ft <- theme_vanilla(ft)
# ft <- set_caption(ft, caption = "Documents")
# ft
```

### Interviews {#interviews}

Semi-structured interviews with a diverse range of actors provided additional insight into the design and implementation of the search and match tools. The interview strategy centred on two main themes and participant groups. The first group centred on IND employees whose duties included looking up and matching identities in their databases, which necessitates using the ELISE search and match engine. The second set of interviewees were WCC employees with responsibilities related to the company's identity matching software package, including design, implementation, and (pre-)sales.

More specifically, the interview approach focused on individual interviews, allowing more time and possibilities to discuss topics in detail. However, opportunities to observe group dynamics and discussion were missing compared to other approaches. Individual user interviews were the easiest to set up and provided a high degree of detail of individual experiences. The interview protocol included questions and probes for follow-up based on the research questions and insights from the document analysis. The Appendix includes a sample of these interview questions.

Due to unfamiliarity with the IND and the necessity of conducting research digitally, interviewees were only available after considerable time and effort. Members of the WCC's "ID Team" helped set up the first contact with the IND organization. Using respondent-driven sampling, the search for IND employees willing to participate in the study grew after the initial contact. The collected contacts were invited via email to participate in an online interview. The small-scale snowball method of WCC and IND staff networks did allow for the inclusion of participants who were unknown prior to the start of the research. However, there was little control over the sample size of people who answered. It is also worth noting that WCC had only one point of contact within the IND. Dependence on these personal networks led to a bias in who was included, in this case, more senior members of the IND organization.

The Covid-19 pandemic made it tough to have in-person interviews, so much of the research was conducted online. Measures at the time restricted people's options to go to their place of work, so most IND and WCC staff were working from home, which restricted the interview format to video and phone calls. These telephone and online conferences had the advantage that scheduling the interviews was more straightforward because meetings did not involve travel. On the other hand, not all communication may have come across the same way, which may have made it harder to network and schedule more interviews. The interviews used the WCC company's secure Microsoft Teams installation for meeting online. However, not all IND employees who worked remotely had the authority to install the software on their company's laptops. Participants' IT access limitations, therefore, necessitated the use of phone interviews in some cases.

The study hypothesized that users performing different tasks at the IND would use the tools for searching and matching identities differently. For this reason, the participants included members of the IND's various organizational units (for a list of these units, see Table \@ref(tab:ind-departments)). As we will see in Chapter 5, the interviews revealed vast differences in tool use and familiarity. It was, unfortunately, impossible to interview users from each unit. Fortunately, during the interviews, details about utilizing the search and match tools in the other units did surface. Participants described their use of the search and match tools at the IND in five interviews, each lasting approximately an hour. Table \@ref(tab:ind-departments) summarizes the various IND units and whether or not they include members from the respective units.

```{r ind-departments, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "ind-departments.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Org", "Unit", "Incl", "Description")], header = TRUE, properties = pt)
```

The research also included interviews with people from the company WCC, which took place concurrently with and after the IND interviews. These interviews followed a looser format than the ones conducted with IND employees. The interviews aimed to illuminate events in the history of their identity-matching software system. The participant's role, projects, and company experiences thus influenced the interview questions and probes. For example, the interview included questions about system development and deployment for people who worked on the IND or EU-VIS projects. This structure also made it possible to ask people with different profiles about their connections with current and potential customers in the security and identity market. In addition, a few WCC employees who were present during the fieldwork were invited to participate in an online interview. On the other hand, respondent-driven sampling expanded the number of participants. Based on their profiles, we can divide these participants into two clusters. The first cluster consists of WCC's "ID Team" members who hold positions such as consultant, pre-sales, and solutions manager. The second group consisted of the more technically minded; among them were a senior software developer and a user experience designer. The Covid-19 pandemic also made online interviews necessary in this case. Participants described their knowledge of developing the sofware of in seven (TODO) interviews, each lasting approximately an hour.

Participants consented to record the sessions using Processing Citizenship's informed consent form. The form allowed participants to specify how the research would use the data they provided while guaranteeing their anonymity and confidentiality. For added anonymity, the recording only included audio with a distorted voice, per protocol. Furthermore, manual transcription of interviews ensured additional confidentiality by preventing confidential information from being leaked via automated transcription platforms. Table \@ref(tab:interviews) provides an overview of the interviews.

```{r interviews, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "interviews.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Number", "Organization", "Date", "Format", "Language", "Description")], header = TRUE, properties = pt)
```

### Events {#events}

The analysis also used relevant publications from the grey literature and fieldnotes from events where industry representatives, academics, Member State authorities, and EU institutions were all present. The events I attended, in person or online, are summarized and described in the table \@ref(tab:fieldwork-events). The goal of participating in these gatherings was to gain insight into how different stakeholders in the industry define and frame the issues surrounding the quality of personal data used in their systems, as well as potential solutions for linking and matching such data. The study's premise was, on the one hand, that presenters and attendees would use real-world examples to depict the challenges they face in their work. At the 2018 eu-LISA conference, for instance, an audience member asked a panellist how they planned to handle the potentially high volume of false positives generated by integrating disparate databases, some of which may contain outdated information. One of the panellists responded that checking all of those false positives would take time. This example demonstrates the value of attending such events, in this case, by providing insights into the often-invisible effort required to integrate various systems successfully.

Another assumption behind the study was that recommendations from security and identification experts would show people's and organizations' ideas about connecting and using data from different sources. Although not central to the dissertation, this aspect is related to the STS literature on "sociotechnical imaginaries", which developed this concept to investigate how societal expectations of the future, sociotechnological projects, and social order are shaped by one another [@jasanoffSociotechnicalImaginariesNational2013]. For example, in Chapter 5, we come across a scenario that security companies use to illustrate why it is vital to have methods for searching and matching data stored in various databases by invoking the spectre of terrorism. In one presentation, a software company used the real-life example of authorities adding one of the "Boston bombers" to police watch list databases prior to the attack, but with different and invalid transliteration variations of his name. According to those security professionals, disparate information across databases makes it harder for officers to conduct investigations and may result in blind spots. Correspondingly, @trauttmansdorffInfrastructuralExperimentationCollective2021 gave credence to the idea that such sociotechnical imaginaries enable or disable the development of specific forms of large-scale border security infrastructure.

<!-- During these events, I made notes to capture such discussions and problems and solutions to the data quality of people’s data. In addition, I collected screenshots and other publicly available materials related to these events, such as video recordings and handouts. I selected the most relevant from these notes, pieces of observations, and other materials to be coded and used in the data analyses. -->

<!-- ```{r fieldwork-events, echo=FALSE, ft.align="center"}
path = file.path(TABLES_PATH, "fieldwork-events.csv")
data = read.csv(path, header = TRUE)

data = data[]

block_table(data[,c("Date", "Event name", "Location", "Audience", "Description")], header = TRUE, properties = pt)
``` -->

### Other observations {#other-observations}

Other observations used in the analysis came from news articles, press releases, and other publications. These publications were collected to give more context and help understand moments in the history of the WCC search and match tool and its use at the IND and EU-VIS. Thus, collecting data involved finding articles that mentioned the company WCC and its software in the INDIGO system of the IND or the EU-VIS system using the LexisNexis database and tool. LexisNexis allows users to search for articles in newspapers, online business publications, and other sources written in English and Dutch. For example, several articles published in Dutch IT business news websites provided valuable information on IND's INDiGO system. This database also uncovered press releases concerning the WCC's participation in the MITRE multicultural name matching challenge (discussed in Chapter 6).

The document corpus also includes blog posts and press releases from businesses that disseminate information about the systems' creation, development, deployment, and utilization. These additional publications are discoverable via conventional search engines. For example, blog posts on WCC's and competitors' websites showed how experts in the field of name-matching devise and disseminate their knowledge. Take, for instance, the articles "Understanding Dari and Pashto names: a challenge to intelligence gathering in Afghanistan" [@scheersBiographicMatchingUMF2021] and "Biographic matching & UMF standards for EU interoperability" [@btcUnderstandingDariPashto2012]. Such publications give essential insights for Chapter 6's SCOT analysis on how companies,  as relevant social groups, define problems and solutions of identification. A selection of these publications dealing with data quality, data matching, or other relevant discussions surrounding the operation of the ELISE, EU-VIS, and INDiGO systems was further coded and analyzed.

These various other publications (news articles, blog posts, and press releases) contribute to a more complete picture of the history of this identity data searching and matching software.

## Techniques of data analysis

Two distinct methods distinguish the data analysis of the various collected data. This distinction stems from two aspects of the conceptual framework discussed above: the relationships between different data models and data matching within and across organizations. First, for Chapter 4, analyzing information systems' data models necessitated the development of a new method and a tool. Second, for Chapters 5 and 6, analyzing the other field notes, interview transcripts, and technical documents used well-known methods for coding and analyzing qualitative data.

Chapter \@ref(ch-data-model-matching) details the methodology for comparing data models named "The Ontology Explorer". The Ontology Explorer (OE) is a semantic method and JavaScript-based open-source tool to compare the data models collected in different formats and from diverse systems. As such, the OE is distinctive in two respects. First, it supports analyses of information systems which define their data models, even if these systems are only sometimes comparable. Second, it systematically and quantitatively supports discursive analysis of "thin" data models by detecting differences and absences between systems. The method extracts, analyses, compares and visualizes heterogeneous data models to achieve this. Applying this structured approach to the data models used by diverse organizations eventually made it possible to observe differences and similarities in the data models of diverse data infrastructures.

In this way, the OE analysis addresses the first aspect of matching identity data: the correspondence between data models of different EU and Member State systems. That is, how different data models for collecting information about people-on-the-move are situated in relation to one another and what this tells us about the organizations which developed and use these models to collect, search, and match identities. Data models of EU and Member States’ systems were analyzed using the methodology and tool of the ontology explorer. The results from this analysis showed that, for example, there are differences and similarities between all data collected in the systems and those used for searching and matching identities. Furthermore, these differences and similarities between authorities’ data models can show how knowledge of people circulates and shows divisions of responsibilities between actors. A typology of data developed using the OE includes shared categories of data necessary for searching and matching identities. In contrast, other distinctive categories give evidence of other responsibilities distinctive to those systems and actors.

Other qualitative fieldwork data analysis followed standard computer-assisted qualitative data analysis methods. As such, the data analysis relied on the software _ATLAS.ti_. Cleaned-up field notes and transcribed interviews were imported into the software. The data coding and analysis drew inspiration from the three interconnected steps of the "Noticing-Collecting-Thinking" (NCT) method of @frieseQualitativeDataAnalysis2014, tailored for the _Atlas.ti_ software package. First,  labels were assigned to segments in the documents, i.e., codes, in the Noticing step. The research questions guided this coding step, but the coding was simultaneously open to inductive findings from the data. Second, those codes were reviewed and gathered into similar codes in the Collecting step. Third, among the developed codes, patterns, processes, and typologies were found in the Thinking step. Of course, in practice, the whole process proceeds by moving back and forth between the different steps of noticing, collecting and thinking. Figure \@ref(fig:data-analysis) summarizes this recursive process of data collection and applying the different NCT steps.

```{r data-analysis, fig.cap="Data analysis."}
knitr::include_graphics("figures/data-analysis.pdf")
```

Take, for example, the typologies developed based on analyzing data collected during fieldwork and from interviews with practitioners using the ELISE software to search and match data at the Netherlands' asylum and naturalization agency. Chapter 5 employs these typologies to develop the concept of "re-identification". For instance, a non-exhaustive typology of potential data frictions in alphanumeric identity data included ambiguity and incommensurability in transliterations, variations in identification policies, and frictions brought on by human errors during data entry. As a result, it was possible to theorize how algorithms for dealing with incomplete, inaccurate, and unreliable identity data deal with the problem of re-identifying people throughout the agency's bureaucratic procedures. Analyzing the findings of ELISE's implementation at the agency thus revealed how fictions about handling data uncertainties influence daily (re-)identification practices.

These examples demonstrate how, by drawing from various sources, it is possible to trace various aspects of data matching practices and the design and use of data matching technologies. The following chapter describes the OE method and its application in analyzing the relationships between different data models in the information systems of both EU and Member State authorities.
