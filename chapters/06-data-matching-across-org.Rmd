# Uncovering the long-term development of identification infrastructures: A multi-temporal perspective {#ch-dm-across-org}

<!-- TODO

* [ ] Add something about Latour's quasi-standards (2005, p.229)
 -->

\chaptermark{Data matching across organizations}

__Abstract__

\noindent

Systems and infrastructures for identifying and registering mobile populations have many facets and long development histories. On the contrary, researchers’ partial perspectives shape their understanding of the technologies and practices involved. To overcome methodological partiality, researchers frequently study infrastructures by conducting their studies at multiple sites or including human and non-human actors that shape identification encounters. As a further option, this chapter suggests that researchers can use multi-temporal sampling methods to understand the long-term development of identification systems and infrastructures. The chapter proposes two heuristics for selecting crucial moments in the lifecycle of identification technologies. The first heuristic employs the Social Construction of Technology’s concept of “interpretative flexibility” to pick out significant moments when social groups challenge, change or close down the meanings of identification practices and technologies. The second heuristic employs infrastructure studies’ concept of “gateway technologies” to pick out moments when heterogeneous identification software systems and infrastructures intersect. These two heuristics tested through the analysis of data gathered at an IT vendor of software for matching people’s identity data. This chapter makes two contributions to the research agenda of long-term perspectives on identification in border security and migration management The first contribution demonstrates how the flexible interpretation of software packages corresponds to diverse problematizations of identification, such as those related to the securitization of identification. The second contribution demonstrates how “gateway moments” make it possible to see the compromises necessary when building identification infrastructures and adapting globally honed technologies to new settings. For instance, deploying the data matching software in an EU system highlighted compromises required for backward compatibility with MS systems. Together, these findings shed light on the activities of under-the-radar actors, such as software vendors, whose distribution and reuse of software packages have long-term implications on identification practices and infrastructures, not only in the security field.


---

\newpage

<!-- \vspace*{\fill} -->
<!-- \noindent -->
<!-- _Possibly insert citation here._ -->

Contribution to research objectives
: _To investigate the long-term development of identification systems and building of transnational data infrastructures by identifying crucial moments in their lifecycle to explore how data matching expertise travels and circulates._
: This chapter addresses the research objective by investigating the long-term development of data matching software package and the building of transnational security infrastructures. By identifying pivotal moments in the lifecycle of this software package, the chapter delves deeper into the processes and dynamics that shape the evolution of identification systems and infrastructures over time. By tracing the trajectory of these software technologies and practices, we gain insights into the factors that contribute to their development, adaptation, and dissemination. In particular, the analysis offers valuable insights into how data matching expertise travels and circulates. Understanding the long-term development of identification technology is, hence, shown to be crucial for comprehending their complexities and the implications they have in various contexts. The chapter's analysis provides a nuanced perspective on the interplay between technological advancements, and their long-lasting effects on transnational data infrastructures.

Infrastructural inversion strategy
: _Third inversion strategy — sociotechnical change_
: The chapter employs the third infrastructural inversion strategy: to analyze the evolution of technology and design over time. This strategy thus adopts a longitudinal perspective, enabling us to examine the various processes that shape and are shaped by identification systems and infrastructures. The analysis makes use of two heuristical devices to bring out important moments. The first heuristci focuses on the software package's changing interpretive flexibility, which allows us to see actors' varying problematizations of identification. The second heuristic uses "gateway moments" when systems and infrastructures intersect, which make it possible to see the compromises necessary when integrating identification systems into broader infrastructures. By focusing on the multifaceted interactions between technology suppliers and organizations, we can gain valuable insights into the complex dynamics of identification systems and infrastructures and their implications in diverse settings.

Contribution to Research Questions
: _RQ3: How does knowledge and technology for matching identity data circulate and travel across organizations?_
: The chapter addresses this question by examining the networks, actors, and organizations involved in the development and dissemination of technologies used for matching identity data. By exploring the circulation and travel of knowledge and technology related to data matching, the chapter sheds light on the processes through which data matching expertise is shared and disseminated among different actors and organizations. This analysis provides a deeper understanding of how knowledge about these technologies is transferred and utilized, facilitating a comprehensive perspective on the broader social and political contexts that shape identification practices. By following the pathways through which knowledge and technology circulate, we gain valuable insights into the underlying mechanisms that contribute to the development and application of identification technologies, leading to a broader understanding of their long-term implications.

Contribution to Main Research Question
: Through its analysis of crucial moments in the lifecycle of identification technologies and the examination of actors involved in their development, the chapter sheds light on the intricate relationship between identification practices, technologies, and transnational commercialized security infrastructures. By uncovering the compromises and adaptations required when building identification infrastructures, the chapter provides insights into how globally honed technologies are adjusted to new contexts. Moreover, it highlights the activities of under-the-radar actors, such as software vendors, whose distribution and reuse of software packages have long-term implications for identification practices and infrastructures. Overall, the chapter deepens our understanding of the complex interplay between identification systems, infrastructures, and the broader security landscape, offering valuable insights into the ways in which practices and technologies are shaped and influenced by transnational commercialized security infrastructures.

\newpage

## Introduction

What we know about technologies for identifying people is inextricably linked to what we know about the devices and practices involved [@garfinkelStudiesRoutineGrounds1964; @latourVisualisationCognitionDrawing1986; @marresDigitalSociologyReinvention2017; @pollockSoftwareOrganisationsBiography2009]. Every year, for instance, authorities identify millions of people crossing the EU's external border, including migrants, tourists and asylum seekers. In order to identify people, authorities frequently cross-reference and link personal information from different databases. Examples include checking visa applicants' identities, preventing identity theft by linking identities across worldwide law enforcement databases, and screening passengers for potential security risks by comparing their personal information to government watch lists. By facilitating or hindering particular forms of mobility and excluding others, these processes contribute to the fragmentation of mobilities [@olivieriTemporalitiesMigrationTime2023; @sparkeNeoliberalNexusEconomy2006]. However, what do we know about the histories of identification systems, and how are the technologies known to us?

One significant problem in researching technologies lies in the inherent limitations of researchers' partial perspectives. To address this challenge, researchers often employ multi-site sampling methods, to study technologies in diverse contexts. While these methods offer valuable insights into the varied practices and actors shaping identification technologies, they have limitations in tracking how these practices and technologies evolve over time. Recognizing the significance of understanding temporal changes, it becomes crucial to incorporate a multi-temporal perspective. Tracking changes over time allows researchers to unravel the dynamic nature of identification practices and technologies, unveiling the influences, adaptations, and challenges that occur throughout their lifecycle. By adopting a multi-temporal approach, researchers could uncover the intricate interplay between social, technological, and contextual factors that shape identification systems and infrastructures.

By adopting a multi-temporal perspective, researchers can bring to light a variety of moments in the lifecycle of identification technologies, providing distinctive perspectives on the technologies and their associated practices. Of course, chronicling a software package's entire lifecycle would be neither feasible nor desirable, so researchers must make informed decisions based on their familiarity with the subject and research goals [@pollockEInfrastructuresHowWe2010]. Selecting one or more moments in the lifecycle of a sociotechnology of identification (e.g., planning, analysis, design, procurement, implementation, operation, and maintenance) and ignoring others will inevitably result in partial descriptions [@pollockEInfrastructuresHowWe2010].

The focus could shift from earlier stages in developing sociotechnical identification systems to present-day system operation and maintenance. Sometimes, researchers try to learn more about how identification systems emerge by following actors and examining the documents they leave behind; for example, by examing the processes of designing, building, and maintaining systems and policies such as the EU's "smart borders" initiative [@bigoJusticeHomeAffairs2012; @jeandesbozSmarteningBorderSecurity2016] or the EU's Visa Information System [@glouftsiosDesigningDigitalBorders2019]. For example, at airports, we may see the coordination needed between border agents and equipment to identify travellers using automated border systems and their biometric passports [example: @lisleManyLivesBorder2019]. A shift in emphasis to the back-end infrastructure supporting border management could highlight the mundane tasks of maintaining and operating large information systems, as well as the challenges of dealing with incomplete or inaccurate person identification [see, for example, @bellanovaControllingSchengenInformation2022].

<!-- Rather than being isolated examples, those moments can reveal connected and contingent aspects of the lifecycle of sociotechnologies of identification. For example, EU Member State fingerprinting system is designed to meet US standards [@pelizzaProcessingAlterityEnacting2019]. However, the way that those EU and US actors mediate the identification encounter may be missed by a research design that concentrates on using biometric identification practices to make people legible. In contrast, studies that begin with standards and technical design guidelines risk missing the subtleties of tailoring technologies to particular sites. It is impossible to account for all possible moments and actors -->

By concentrating on how sociotechnologies of identification move and circulate across organizations, influencing identification problems and solutions, this chapter hopes to contribute to the latter type of analysis. The chapter address the disseration's goal to investigate the long-term development of identification systems and building of transnational data infrastructures by identifying crucial moments in their lifecycle to explore how data matching expertise travels and circulates. It does so by proposing two heuristics to identify key moments to study the evolution of identification-related sociotechnologies by drawing on concepts and theories from long-term and genealogical analyses of data infrastructures and social constructivist accounts of technology. Consequently, the chapter aims to answer the dissertation's research question 3:

> How does knowledge and technology for matching identity data circulate and travel across organizations?

To address this research question, the chapter first considers methods suitable for tracking the spread of identification practices and technologies, and then provides heuristics for identifying key moments. The next section begins by classifying the various research strands that have investigated sociotechnologies of identification according to the types of sampling methods used. First, much ethnographic research has been influenced by debates about expanding the number of research sites and connecting observations to gain insight into more encompassing phenomena, such as the construction of a border identification regime that criminalizes migration. Second, scholars have begun considering the complex web of human and non-human actors that make up border identification encounters. However, neither of these approaches can show how practices and technologies have changed over time. Thus, there is a need to provide detailed accounts that include the multiplicity of sites and actors and the multiplicity of moments in time [@hyysaloMethodMattersSocial2019]. However, what is missing is clear analytical criteria for identifying key moments in the many interactions of various actors in the co-construction of sociotechnical problems and the identification of solutions.

This chapter, therefore, proposes two heuristics to study the evolution of identification-related sociotechnologies. Long-term and genealogical studies of information systems and infrastructures have convincingly demonstrated that temporal approaches make it possible to avoid a teleological view of the design of technologies [@edwardsIntroductionAgendaInfrastructure2009; @karastiInfrastructureTimeLongterm2010; @ribesLongNowInfrastructure2009; @williamsMovingSingleSite2012]. Instead, temporal approaches allow the inclusion of often overlooked actors and moments in the development of identification sociotechnologies, such as the numerous interactions between government actors and technology consultants as they work together to develop the problems and solutions for identification. The emergence of border and migration identification infrastructure can be better understood through what will be referred to as "multi-temporal sampling". Before going over the two heuristics for multi-temporal sampling, we need to comprehend how researchers typically study identification, which occurs across multiple sites and involves both human and non-human actors.

## Sampling methods for dealing with the scale of sociotechnologies of identification

Ongoing scholarly debates in Science and Technology Studies (STS) about the intertwining of methods and outcomes of investigations may provide inspiration to border identification technology research. In general, the discussion has drawn attention to a mismatch between research methods and the understanding that technologies are shaped over time, in various contexts, and by various actors [@hyysaloMethodMattersSocial2019; @silvastTheorymethodsPackagesScience2023]. These authors argue that many investigations fail to account for technologies' multiple and contingent lives due to inadequate research designs. Their assessment is significant, especially when combined with the realization that methods do not merely describe the world but also play a role in prompting specific realities [@garfinkelStudiesRoutineGrounds1964; @latourVisualisationCognitionDrawing1986; @lawEnactingSocial2004].

Researchers’ choices can indeed have actual consequences because it is widely accepted that research methods play an active role in bringing about the phenomena they set out to describe and discover [@latourVisualisationCognitionDrawing1986; @lawEnactingSocial2004]. Methods, in this view, can be thought of as devices that bring bits and pieces of the world together to enact certain realities. As a result, researchers must answer the question of what kind of "ontological politics" (sic.) they participate in and what kinds of realities they contribute to. There are compelling reasons, for example, to use migrants' practices and experiences as a starting point for understanding the forms of discrimination and unpredictability embedded in border technologies. However, this focus can cause one to miss other phenomena, such as the rise to power of a small oligopoly of technology companies and IT consultancy firms in the development of technologies for identifying people [@lemberg-pedersenPoliticalEconomyEntry2020; @jonesFundsFortressEurope2022].

What we know about sociotechnologies of identification is primarily based on empirical studies that investigate encounters between people and technologies or based on desk research and document analysis. This section demonstrates how methodological choices lead sociotechnologies for identifying people to produce specific research findings. Particular attention will be paid to the growing body of literature exploring the information systems that regulate the entry of people into EU territory. There are two main reasons why these systems were selected. First, there is much research on these EU systems, which are among the largest identification systems in the world. Second, the software I looked into, and will expand on later, is directly connected to one of the systems (the Visa Information System). In reviewing the literature on sociotechnologies of identification, it is possible to highlight some of the thematic and methodological similarities and differences that affect the reviewed research findings.

This section first examines how research has dealt with the large-scale nature of identification systems. In other words, how research has dealt with the many people, places, and things involved in making, deploying and using these systems. As we will see, it is possible to discern three different sampling methods to address these scale issues. First, researchers can multiply the number of research sites to account for the dispersed nature of the studied phenomenon. Second, researchers can increase the number of actors in their analysis. Third, and most importantly for this chapter, different moments in a technology's lifecycle can be compared and analyzed. Much of the current literature on identification sociotechnologies focuses on the first and second approaches for dealing with scale issues. Section 3 of this chapter will cover the theoretical foundation and empirical value of using a multi-temporal sampling approach to broaden the analytical reach of IT for border and migration control.

### Transverse sampling, or situating and tracing connections across sites

It is common to conceptualize information systems and database technologies that store data about mobile populations as part of larger structures, regimes, systems, infrastructures, and assemblages that bring together border and migration-related practices, rules, and meanings. Researchers are then confronted with the methodological conundrum of localizing and investigating these more comprehensive phenomena. One approach is to multiply the research sites to provide multiple accounts of the connections between sites and unravel distributed phenomena. Such approaches are inspired by multi-sited ethnography [@marcusEthnographyWorldSystem1995]. This theoretical framework improved upon the limitations of earlier ethnographic methods, which relied on researchers physically "being there," or observing and interacting with a specific group of people or community in a bounded field site for an extended period. While these accounts may be rich in empirical data, focusing on particular fieldwork locations was deemed insufficient to comprehend globally overlapping phenomena. When ethnographers trace links between sites, they do more than just put one site in a broader global context. In contrast, an ethnographer can decide to focus, for example, on consumption processes within a capitalist political economy by following connections between sites. If we imagine this sampling strategy as a line that crosses and extends across several research sites, we can refer to it as _transverse sampling_.

For instance, the "Mig@Net-Transnational Digital Networks, Migration and Gender" project [@tsianosTransnationalMigrationEmergence2010] used a multi-sited ethnographic approach to investigate European border policies and practices by bringing together data from sites and contexts across Europe. Triangulating observations from various actors (migrants, policy experts) and locations (Greece, Germany, and Italy) allowed the team of researchers to show how, for instance, classifications of asylum seekers do not only follow a coherent system of governance. In principle, the Eurodac system categorizes asylum seekers into one of three categories based on whether they (1) regularly apply for international protection, (2) are discovered illegally crossing the border, or (3) are discovered illegally present within a Member State. However, based on interviews with officials conducted in 2011, the researchers discovered that there are differences in how these categories are applied and understood on a national and institutional level. A German contact for the Mig@Net project, for example, asked the researcher why Greece primarily employed the second rather than the third category. The German practitioner's apparent ignorance runs counter to the Eurodac system's purported role in the development of a uniform EU asylum policy. Additionally, by emphasizing the inconsistency and unpredictability of border and migration control, this case highlights how approaches that sample and aggregate observations from multiple sites can provide counter-evidence to the view of coherent migration and border control practices and policies.

However, researchers should avoid a priori dichotomizing into groupings such as local sites and macro phenomena [e.g., @latourReassemblingSocialIntroduction2005]. A sense of global phenomena can instead emerge from tracing paths between heterogeneous actors. The problem may arise when researchers begin with a theoretical construct — e.g., the existence of an EU border regime that criminalizes migration — and investigate it by triangulating results from different sites. Actor-network theory, in particular has cast doubt on these methods' capacity to demonstrate how scale is accomplished in practice, and argued, instead, that "scale is the actor’s own achievement" [@latourReassemblingSocialIntroduction2005, p.185] and that, hence, theoretical divisions between micro and macro should be dropped [see also, @latourRecallingANT1999; @lawTraductionTrahisonNotes2006]. In this way, one valuable methodological insight is to "localiz[e] the global" [@latourReassemblingSocialIntroduction2005, p.173] by following the "connections leading from one local interaction to the other places, times, and agencies through which a local site is made to do something" (173). In other words, researchers can allow any notion of providing actors and locations structure to come from following connections rather than assuming specific orderings.[^tarde]

[^tarde]: Ontological stances similar to these have their roots in earlier forms of social theory, such as Gabriel Tarde's "monadology" [see, for example, @latourGabrielTardeEnd2002].

Following the multiple circulating standards and categories is one effective way of tracking down these links [@latourReassemblingSocialIntroduction2005]. For example, @pelizzaIdentificationTranslationArt2021 has documented how regulations for adopting FBI biometric identification standards in equipment used at the Greek border "create trans-national associations with the EU Commission, corporate contractors and the US security regime" (p. 16). Another example is provided by @donkoMigrationControlLocal2022, who have demonstrated how European migration management technology for identification stretches beyond the external EU borders. The authors describe how border checkpoints between Burkina Faso and Niger are linked to EU agencies such as the European Border and Coast Guard Agency (Frontex) via IOM border management information systems that record people's biographic and biometric data. Furthermore, the EU-funded West Africa Police Information System connects these border checkpoints to all INTERPOL member nations via the global police communication system. In order to see the formation of such novel and distinct "flat 'networky' topographies" [@latourReassemblingSocialIntroduction2005, p. 242] of interactions between players, it is crucial to avoid dichotomies of scale.

So, if we imagine this sampling strategy as a line that crosses and extends across several analytically valuable research sites, we can refer to it as transverse sampling for studying large-scale data infrastructures. Far-reaching relationships might appear by exploring the lines between local encounters and other places. Tracing these connections highlights the different interconnected actors involved in large-scale data infrastructures.

### Perpendicular sampling, or incorporating ecologies of interlinked actors

The need for solutions to scale up the number of research locations to examine large-scale infrastructures runs parallel to the challenge of who should be included in research designs. Scholars have debated the field's proclivity to investigate socially, politically, and geographically marginalized individuals. Contrary to the abundance of research on marginalized individuals, less research has been conducted to "study up" on the wealthy and powerful [@naderAnthropologistPerspectivesGained1972; @gustersonStudyingRevisited1997]. Similarly, many published studies on sociotechnologies of identification focus on the experiences of marginalized persons at the border and the related power imbalances. For instance, a growing body of literature [e.g., @kloppenburgSecuringIdentitiesBiometric2020; @kusterHowLiquefyBody2016; @olwigBiometricBorderWorld2019] has investigated the contentious processes of collecting migrants' biometric data in border zones. Following the studying up/down analogy, researchers interested in ethnography have tended to concentrate on the perspectives of people who are identified and controlled down at the border and less on those responsible for creating and maintaining these large-scale identification infrastructures.

Alternatively, scholars can arrive at more balanced portrayals of the numerous human and non-human actors involved by concentrating on moments and places where diverse actors interlink and impact one another. Examples in the literature show that researchers can thus include a more diverse set of actors: from interviews with local administrators and officers of international organizations of migration centres [@pollozekInfrastructuringEuropeanMigration2019] to interviews with officials and experts from European, national, and international institutions [@glouftsiosGoverningCirculationTechnology2018; @pelizzaTellingMoreComplex2023;  @trauttmansdorffInfrastructuralExperimentationCollective2021], and professionals in the security domain at industrial fairs [@bairdKnowledgePracticeMultisited2017]. Studies influenced by STS and ANT highlight the significance of non-human actors alongside these human ones [e.g., @pelizzaIdentificationTranslationArt2021; @pollozekInfrastructuringEuropeanMigration2019]. Suppose we visualize these approaches as highlighting the multitude of actors whose paths cross at sites. In that case, we can refer to this strategy of increasing the number of actors in studies as _perpendicular sampling_. [^vertical-slice]

[^vertical-slice]: @naderVerticalSliceHierarchies1980 proposed the concept of "vertical slice" to, for instance, map the various actors, government agencies, policies, corporations, and associations to understand how power and governance of problems are organized [see also @shorePolicyWorldsAnthropology2011]. In using the term perpendicular sampling, I aimed to avoid implying that some vertical hierarchical organization exists.

The decision of whom to include in the study reflects the questions and politics of the researcher. The "Autonomy of Migration" (AoM) approach, for example, begins with migrants' practices of subverting and appropriating mobility regimes and contrasts them with the Fortress Europe discourse [@scheelAutonomyMigrationAppropriating2019]. It has been argued that depicting migration and border controls as fortifications fosters a narrative that ignores the diversity of experiences on borders and migration [@mezzadraBorderMethodMultiplication2013], thus instilling a paternalistic view of migrants as helpless victims in need of protection. Authors in this body of work seek to destabilize such tropes by centring on the experience of migrants and illustrating how migrants can circumvent and subvert restrictive migration and border control mechanisms. One may wonder, however, if the emphasis on migrants' practices and tactics to demonstrate that migration politics are "a site of struggle" [@strangeIrregularMigrationStruggles2017, p. 243] does not also contribute to a negative image of migrants as subversive of a rules-based international order. In addition, restrictive definitions of migrants run the risk of excluding other "privileged migrants" [@bensonMigrationSearchBetter2009], like professionals living abroad, recipients of golden visas, and retirees who migrate. Other approaches, provide methodologies that even begin from migrants' perspectives, such as focusing on their acts and claims of citizenship [e.g., @isinClaimingEuropeanCitizenship2013], or where research adopt a more interventionist stance [e.g., @olivieriTemporalitiesMigrationTime2023].

As Laura Nader stated in her 1972 article on studying up, "we aren’t dealing with an either/or proposition; we need simply to realize when it is useful or crucial in terms of the problem to extend the domain of study up, down, or sideways" [@naderAnthropologistPerspectivesGained1972, p. 8]. Since then, the sites and domains of ethnographically inspired research have expanded in many directions. Ethnographers have expanded their work to include tracing the work of scientists in laboratories [for example, @latourLaboratoryLifeConstruction1986; @gustersonNuclearRitesWeapons1996] and policymakers in governance processes [@shorePolicyWorldsAnthropology2011]. Furthermore, these developments coincided with researchers' recognition of other forms of non-human agency. For example, @glouftsiosGoverningBorderSecurity2021 theorized that the agency of EU information systems, as well as the labour of IT and security professionals to maintain those systems, shapes mobility governance throughout the Schengen area. More researchers are now looking into how devices used in security practices affect political agency due to the realization that technological properties shape practices and are intertwined with effects that shape the world [for example, @amicelleQuestioningSecurityDevices2015].

Despite these achievements, the diversity of places and actors involved and the specialized and closed nature of border and security work can create barriers for researchers. A report by the "Advancing Alternative Migration Governance" project, for example, describes how the development of EU information systems "has been engineered in specialized and closed forums, such as expert workshops, task forces, technical studies, pilots, or advisory groups and technological platforms steering not just policies, but also the formulation of research and development priorities of funding programmes" [@jeandesbozFinalReportEntry2020, p.10]. Moreover, according to the report's authors, the influence of less visible actors, such as global actors who build, fund, and thus profit from border infrastructure construction, needs to be studied more. To support this effort, in what follows I propose two heuristics that can serve as valuable analytical tools. The first heuristic, based on "interpretative flexibility," allows us to identify significant moments when social groups challenge or transform identification technologies and practices. The second heuristic, employing "gateway technologies," helps us understand the compromises and adaptations involved in building identification infrastructures and deploying technologies in diverse contexts. Together, these heuristics provide a robust framework to analyze the development and impact of identification systems, considering the role of under-the-radar actors and their long-term implications on identification practices and infrastructures.

## Multi-temporal sampling, or tracing the genealogies of data infrastructures

The reach of sociotechnologies of identification lies not only in their ability to operate across numerous sites and bring together diverse actors but also in how the technologies develop over time and integrate into infrastructures to have long-lasting effects [see also @ribesLongNowInfrastructure2009; @karastiInfrastructureTimeLongterm2010]. While researchers tend to see rational processes of designing and implementing systems by system builders with well-defined goals, they tend to overlook the contigency in those processes over time. A closer examination of, for example, the well-known Second Generation Schengen Information System (SISII) reveals how the system's creation was nearly derailed by "delays, an escalating budget, political crises, and criticisms of the new system's potential impact on fundamental rights" [@parkinDifficultRoadSchengen2011, p. 1, see also Figure \@ref(fig:sisii)]. In building the SISII, the "instability of system requirements", which includes the European Commission's ignorance of how Member States' end-users use the system, was frequently cited as a cause for delays, according to a report by the European Court of Auditors [@ecaLessonsEuropeanCommission2014, p. 13]. Consequently, research on the sociotechnologies of identification should correct the misconception that designing and building systems are purely rational processes. Instead, researchers must recognize that such systems result from negotiations, adaptations over multiple timescales, and interactions between actors from different organizations [@gassonGenealogicalStudyBoundaryspanning2006; @pollockSoftwareOrganisationsBiography2009].

```{r sisii, echo=FALSE, fig.cap="Chronology of the SISII (ECA 2014).", out.width="80%", fig.asp=.75, fig.align="center"}
knitr::include_graphics("figures/sisii.pdf")
```

Tracing how knowledge and technologies for matching identities emerged and circulated thus necessitates uncovering choices and contingencies in the design of information systems over time. Social constructivists, for one, have long emphasized that the growth and spread of new technologies do not adhere to any simple linear models [e.g., @pinchSocialConstructionFacts1984; @hughesNetworksPowerElectrification1983]. Instead, various factors influence the trajectory, resulting in distinct sociotechnologies for identification. There are likely many forking paths that could have produced different technological outcomes. Unfortunately, only some methodological criteria are found in the literature to help detect such seminal moments in a software's lifecycle. For instance, @hyysaloMethodMattersSocial2019 suggest identifying "moments and sites in which the various focal actors in the ecology interlink and affect each other and the evolving technology." In general, STS has historically used technological controversies and breakdowns as points in time for understanding how technology functions and the meanings ascribed by actors who are present or who claim to speak for others [@callonElementsSociologyTranslation1984; @marresIssuesDeserveMore2007].

How can we meaningfully – if not systematically – collect data from various stages in the sociotechnical development of identification software? This chapter proposes two heuristics for detecting pivotal moments in time. First, tracing the moments when the meanings of technologies are challenged, changed, or closed down can help to explain the emergence and establishment of standardized software packages. Second, tracing the moments when technology connects to other systems can help us understand the unfolding of large-scale identification infrastructures.

### Interpretative flexibility and the making of a standard software

Social constructivist accounts of technological innovation provide the theoretical foundation for the first analytical heuristic for identifying pivotal moments in developing standardized identification software packages. Constructivist approaches such as the Social Construction of Technology (SCOT) and Social Shaping of Technology (SST) have criticized linear and deterministic models of innovation and technological development. Instead, these scholarships have shown how technological development is a long-term and open-ended process in which change can be disorderly and protracted.

A fundamental premise of constructivist approaches is that technologies aim to solve difficult-to-solve problems with multiple solutions due to competing demands and requirements. Thus, the adaptability of technological designs shapes and is shaped by the interpretation and interest of specific (groups of) actors. To demonstrate this "interpretative flexibility" of artefacts, the basic template for conducting a SCOT analysis calls for first identifying "relevant social groups" [@pinchSocialConstructionFacts1984]. Since a SCOT analysis is grounded in methodological relativism, relevant social groups are not pre-determined. Instead, social groups are empirically introduced when a group of actors assigns a particular interpretation or meaning to a technological artefact. As such, a SCOT analysis is interested in how different interpretations of social groups give different problems to be solved by technology. The second step is to analyze how interpretative flexibility decreases through the process of "closure", in which the number of alternative solutions narrows and "stabilizes" into one or more artefacts (sic.). It is important to note that the original SCOT approach has some issues due to its ambivalence to structural contexts and that power imbalances can render some actors invisible [@kleinSocialConstructionTechnology2002]. However, analyzing changes and closures in the interpretative flexibility of artefacts can be a valuable heuristic for identifying pivotal moments of change in the lifecycle of identification sociotechnologies.

Information systems are typically (re)designed to be "generic" and "travel" across organizational contexts [@pollockSoftwareOrganisationsBiography2009]. Such genericity can be seen as corresponding to SCOT’s interpretive flexibility. Multiple meanings can be attributed to software, diverse uses are contemplated, and heterogenous problems can be solved with its mediation. Over time, however, interpretive flexibility leaves space to stabilization. Similarly, using the metaphor of a biography, @pollockSoftwareOrganisationsBiography2009 demonstrated how software suppliers must balance requirements as the software matures and accumulates functionalities through its history. For instance, they found that software may be more adjusted to particular user requirements early in the development process. Later, when vendors want to transfer their software to new customers, they must identify overlaps between the sites' needs and the procedures; this moment of closure is labelled as "process alignment work" by the authors (p. 174). However, power disparities between the supplier and large/small customers may skew the requirements in particular directions, eventually giving shape to best practices and standards. 

<!-- One weakness of this argument is that it may emphasize interactions between suppliers and customers. Instead, other actors like industry analysts [@pollockHowIndustryAnalysts2016] and government agencies [@trauttmansdorffInfrastructuralExperimentationCollective2021] shaping current and future technological developments should be included in an analysis of these moments where new meanings may emerge. -->

Scholars have also noted that systems storing personal identity data can easily be used for new and derived purposes aside from their original objectives [@monahanEmergingPoliticsDHS2009]. For example, this type of "function creep" has been referred to for the Eurodac biometric identification system [@ajanaAsylumIdentityManagement2013]. The system's original purpose was to assist the Dublin system in preventing people from requesting asylum in multiple Member States. However, and as an attestation of the connection between migration and crime control (also known as "crimmigration", @stumpfCrimmigrationCrisisImmigrants2006), this scope has gradually expanded by allowing police authorities to query the database [@broedersEuropeanBorderSurveillance2011; @amelungCrimmigrationControlBorders2021]. As a result, it is essential to consider how diverse organizations and their systems are interconnected and how this results in the emergence of new and contentious meanings.

The shifts in meaning enable us to discover the "biography" of commercialized security software packages and their standardization by considering tensions between "local" and "global" aspects of software packages, as well as the links between technical and organizational changes. [@pollockSoftwareOrganisationsBiography2009]. For example, one of the recommendations related to the SISII system problems mentioned above was that the Commission "ensure that there is effective global coordination when a project requires the development of different but dependent systems by different stakeholders" [@ecaLessonsEuropeanCommission2014, p. 07]. [^gpmb] Therefore, the report's recommendation for how to deal with the issue of end-user's differing perspectives was to establish a new organizational structure that could align the various actors, from Member States to international contracting firms.

Another excellent example can be found in the work of @soysurenEuropeanInstrumentsDeportation2022, who used a comparative approach to compare the application of the Dublin III regulation (for the Eurodac system) between a founding EU member (France) and an associated country (Switzerland). The researchers found that France took a more sceptical and decentralized approach to use the Dublin system for deportation. On the other hand, Switzerland eagerly adopted the Dublin system and implemented it in a highly centralized manner. In this way, their comparison demonstrates how even a single European instrument can have different meanings and be used differently. Furthermore, the spatial and temporal reach of sociotechnologies of identification does not imply that these technologies have necessarily stabilized; instead, the systems may still be implemented in various ways depending on the context.

[^gpmb]: The report further notes that for the development of SISII a "Global Project Management Board" was established at a late stage in the project to "to draw more fully on the experience of end-users in member countries" [@ecaLessonsEuropeanCommission2014, p. 37].

Utilizing the SCOT and BOAP approaches to analyze identification software provides several methodological advantages. Firstly, these approaches enable the examination of how the design of identification technology can reach points of closure, wherein customers perceive their problems to be resolved. Secondly, these approaches shed light on the local and global tensions inherent in the development of identification software. They highlight how software systems are designed to be generic and adaptable, capable of traveling across various domains, including security. By studying these tensions, researchers gain insights into the compromises and negotiations involved in adapting identification technologies to different contexts while maintaining their functionality and interoperability. Overall, the SCOT and BOAP approaches offer valuable methodological tools for comprehensively analyzing identification software, uncovering its design processes, and understanding its broader implications in diverse settings.

### Gateways to infrastructures of identification

Infrastructure scholars have argued that (data) infrastructures can only "grow" and build up from pre-existing systems, practices, and communities rather than being purposefully constructed [for example, @edwardsUnderstandingInfrastructureDynamics2007; @karastiKnowledgeInfrastructuresPart2016; @monteiroArtefactsInfrastructures2013; @starStepsEcologyInfrastructure1996]. Hence, studies like those on creating the infrastructure to connect scientific communities [for example, @starStepsEcologyInfrastructure1996 and @edwardsUnderstandingInfrastructureDynamics2007] demonstrated how challenging it is to build large-scale data infrastructure deliberately. In connecting various IT systems, data infrastructures assemble "a combination of standard and custom technology components from different suppliers, selected and adapted to the user’s context and purposes" [@pollockSoftwareOrganisationsBiography2009, p. 286]. At the same time, it is debatable when the assemblage of various elements qualifies as infrastructure. Hence, in light of this argument, @starStepsEcologyInfrastructure1996 posed the question, "When is an Infrastructure?" According to their argument, the concept of infrastructure is fundamentally relational, as it only becomes infrastructure through its relationship with organized practices.

<!-- Similarly, the work of the historian of technology Thomas P. Hughes [@hughesChapterReverseSalients1983] on the growth of the electrical supply network, was influential to show how the growth of systems is not "fore-ordained" but evolves in a complex social and technical environment. He noted how investments of people, business, and other system builders caused an inclination for technology to advance along specific paths and have "technological momentum" which sometimes even "extended beyond national borders". However, Hughes showed how systems often arise out of the difficulties inside existing technical systems, or what he calls "critical problems". Critical problems get identified when there are so called "reverse salients", problems with specific components that hold back the overall system as it evolves to realize certain goals.[^reverse-salients]. In the end, the issue is whether these reverse salient can be resolved within the context of already-existing systems, or if new and competing systems develop as a result. -->

<!-- [^reverse-salients]: His metaphorical concept imagines systems moving in a battle line towards a goal with specific components as inverted salients that hold back the overall development. -->

With several detached systems, it is frequently uncertain which one will succeed or whether other technological and social compromises are necessary to allow systems to work together. Moments were several systems compete are pivotal for infrastructure development, as previously incompatible systems may be able to work and communicate with one another. @edwardsUnderstandingInfrastructureDynamics2007 have referred to the phenomenon of making contending systems compatible as a "gateway problem" (p. 34). We can find a paradigmatic example of a gateway technology to solve such problems in the historical development of electricity infrastructure: the innovation of the rotary converter, which made it possible to convert electric power [@davidEconomicsGatewayTechnologies1988; @edwardsUnderstandingInfrastructureDynamics2007]. The converter qualifies as a gateway technology because it enables compatibility between competing delivery systems, such as alternating current (AC) and direct current (DC). Modern-day equivalents of such a gateway technology are the international travel plug adapters which enable us to charge our electronic devices in different parts of the world without worrying about the various voltages and plug types used.

More generally, @edwardsIntroductionAgendaInfrastructure2009 define a "gateway phase" as a period “in which technical, political, legal, and/or social innovations link previously separate, heterogeneous systems to form more powerful and far-reaching networks” [@edwardsIntroductionAgendaInfrastructure2009, p. 369]. According to a definition provided by @davidEconomicsGatewayTechnologies1988, what gateway technologies, in effect, do is to "make it technically feasible to utilize two or more components/subsystems as compatible complements or compatible substitutes in an integrated system of production." Such sociotechnical arrangements, they say, would "permit multiple systems to be used as if they were a single integrated system" (p. 367). Information and communication technologies, for instance, heavily rely on gateway technologies, such as protocol converters that link telecommunications networks with various network protocols. Similarly, @hansethGatewaysJustImportant2001 uses the term gateway to refer to "elements linking together different networks which are running different protocols" (p. 72). Additionally, Hanseth argues that gateways can be just as critical to the success of large-scale network and infrastructure projects as better-known data standards.

In contrast to standards, gateways have unfortunately gained less scholarly attention. Moreover, gateways are sometimes considered "as a consequence of a failed design effort as they are imperfect translators between the networks they are linking" [@hansethGatewaysJustImportant2001, p. 71]. However, as Hanseth rightly notes, gateways can be crucial building blocks for connecting heterogeneous networks into larger-scale infrastructures. He gives the example of health care data which may be standardized within countries. However, standardizing such data for cross-border data exchange has proven unattainable. A different strategy would be to develop "gateways to enable the (limited, but slowly increasing) transnational information exchange" [@hansethGatewaysJustImportant2001, p. 88]. Building gateways to facilitate communication between heterogeneous systems is often more manageable than settling on a single standard.

The case of the _EU Digital COVID Certificate Gateway_ 

: As an example, look at how the European Union (EU) responded to the Covid-19 pandemic by creating the "EU Digital COVID Certificate Gateway" to authenticate digital COVID certificate signatures across EU member states [@europeancommissionEUDigitalCOVID2021; @dgsanteEHealthCOVID19]. The European Commission established this gateway in 2021 as a means "through which all certificate signatures can be verified across the EU" [@europeancommissionQuestionsAnswersEU2021]. The EU's member states would have had difficulty agreeing on establishing a central health certificate database during the urgency of a pandemic. Since no personal data would be exchanged via the EU gateway, the system did "not require the setting up and maintenance of a database of health certificates at EU level" [@europeancommissionQuestionsAnswersEU2021]. This choice was also significant for Member States because it allowed them to "retain flexibility in how they link the issuing and verification of their certificates to their national systems so long as they meet [the] common standards" (sic.). In those moments of urgency, "most Member States [had] decided to launch a contact tracing" [@europeancommissionCoronavirusEUInteroperability2020]. Nevertheless, through "decentralised systems", those 20 or so apps could be made "interoperable through the gateway service" (sic.). As a result, a sophisticated contact-tracing infrastructure quickly developed as EU member states (and others) were able to link their national applications while maintaining their national backends and data standards. At the same time, this EU contact tracing system revealed variations in how the Member States applied the rules in their domestic context, such as how much time to consider a vaccine's viability before expiration [@calderEUBringsVaccine2022]. The EU gateway exemplifies gateway technologies' critical but underappreciated role in establishing and maintaining networks within larger-scale infrastructures. It also shows how gateways can be either short-lived or long-lived because the EU Gateway is already offline at the time of writing.

According to @egyediInfrastructureFlexibilityCreated2001, there are different kinds of gateways with varying degrees of standardization and, thus, varying degrees of flexibility. Gateways, as per her typology, can be dedicated, generic, or meta-generic. In her view, a dedicated gateway is designed to link only predetermined subsystems and is not or only minimally standardized. She regards the AC/DC rotary converter, for instance, as a specific gateway for converting those two types of currents. On the other hand, generic gateways are standardized and thus can connect an undetermined number of subsystems. We can think of the EU Digital COVID Certificate Gateway (see box) as an example of a generic gateway because it established a common standard that any EU or non-EU country could adopt [@eeasNonEUCountriesWelcome2021]. For example, South Korea, a non-EU country, established a connection with the EU gateway in July 2022 to allow "certificates of vaccination issued in South Korea to be valid in EU countries, and vice versa" [@kimSouthKoreaJoins2022]. Lastly, the best way to understand meta-generic gateways is through examples such as the OSI reference model, which specifies a foundation for computing system communications. These reference models serve as frameworks for developing specific generic standards rather than defining them. This typology of gateway technologies will help us understand how standardized and adaptable the gateways are in linking heterogeneous identification systems into more extensive networks.

This chapter proposes operationalizing the gateway technology concept as a second heuristic for identifying moments where software packages and infrastructures intersect. The term "gateway moment" will be used broadly to refer to instances in which different systems and communities of practice are linked together into larger infrastructures using gateway-like technologies. Such gateway moments are thought to reveal structural constraints that must be reconciled to connect new components in the emergence of identification infrastructures.[^failure]

[^failure]: Of course, these junctions between new components and existing infrastructures are also prone to failure [@edwardsIntroductionAgendaInfrastructure2009]. Such failures have, for instance, been well documented in e-government and information systems literature more broadly, where failures have been a long-standing concern due to their high stakes and use of public money [@pelizzaBirthFailureConsequences2018].

The following section uses these theoretical concepts as heuristics to identify points in the lifecycle of a software package for matching people's identity data that can provide insight into the evolution of practices and technologies for identifying people in the context of migration, borders, and security.

## Tracing fields of identification through the evolution of software for matching data

### Methodology

This section draws on the fieldwork data collected at a software package vendor for matching people's identity data in the context of border security and migration management. This section thus builds on Chapter 5's findings on the specific deployment of the software package at The Netherlands' government immigration agency. In addition, the focus on other software deployments in EU and Member State identification systems sheds light on different stages in developing and using the ELISE software. As a result, the study illustrates the diverse set of actors involved in practices of identifying and circulating data about people on the move at the European border [@pelizzaProcessingAlterityEnacting2019]. As detailed in Chapter 3, I joined the company "WCC Group" (WCC) to investigate the design, use, and evolution of a software product dealing with data matching and deduplication. Since I was a temporary member of the ID team, I could visit the company's headquarters in Utrecht (The Netherlands), review all necessary paperwork, conduct one-on-one interviews with relevant company and personnel, and sit in on some of the team's group meetings.

The research included interviews with people from the company WCC. The interviews aimed to illuminate events in the history of their identity-matching software system. The participant's role, projects, and company experiences thus influenced the interview questions and probes. For example, the interview included questions about system development and deployment for people who worked on the IND or EU-VIS projects. This structure also allowed asking people with different profiles about their connections with current and potential customers in the security and identity market. Based on their profiles, we can divide these participants into two clusters. The first cluster comprises WCC's “ID Team” members who hold consultant, pre-sales, and solutions manager positions. The second group consisted of the more technically minded; among them were a senior software developer and a user experience designer. Participants described their knowledge of building and deploying the company's software in seven interviews, each lasting approximately an hour.

Building on Chapter 5's introduction to WCC's "ELISE ID platform" (ELISE), this chapter analyzes the software's history of deployments in systems like the Netherlands' Immigration and Naturalization Service system (INDiGO) and the EU Visa Information System's central system (VIS). In Chapter 5, we already saw how the software enhances standard database searches and comparisons with sophisticated matching functionalities. The technology does this through probabilistic algorithms that calculate the degree of identity data similarity. Consequently, the program incorporates complex features for matching identity data, such as dealing with incomplete data or different languages, alphabets, and cultural contexts. Hence, this chapter delves into the history and development of WCC's software and its various features.

We can gain insight into identification in national and international settings by contrasting the EU and MS systems' adoption of the ELISE software. As we saw in Chapter 5, the Netherlands' migration and asylum government agency uses the ELISE software to search for applicants' data in their case management system, INDiGO, and to deal with data quality issues such as duplicate records. In addition, the ELISE software is used by the EU Visa Information System to facilitate alphabetical searches from Member State systems into the central EU database, which is essential for exchanging visa data between EU and Member State systems. For instance, border guards may confirm that a person has a visa, or local authorities may check a visa applicant's data in the context of a Schengen visa application. The analysis of the evolution of the ELISE package in these systems is based on (1) field notes, document analysis on the package for data matching, (2) interviews with vendors' developers, sales, and consultants, and (3) participant observations at industry events.

### Interpretative flexibility 1: Searching and matching data in the dot-com era

The history of the ELISE software package, from its inception to its deployment in the VIS and INDiGO systems, provides a fascinating insight into the development of software and practices used in identifying individuals. The company's origins also bring to light the difficulties in translating and "generifying" the software package so that it can be used in different contexts and by different organizations, paving the way for it to play a notable role in international identification systems. The history of this data matching software developed by a relatively small IT company provides an interesting window into the complex web of heterogeneous actors linked to international and commercial identification systems.

#### A generic data matching system

The company's early days reveal a surprising amount of interpretative flexibility regarding what the software should accomplish and for whom it should be helpful. When the company was first conceived in 1996, its founders saw the software primarily as a generic database technology for matching various "things". One of the founders is said to have conceived the initial idea after having unsatisfactory results while looking for a home. In addition, he was aware of other friends who had encountered comparable difficulties in their online job searches. The core issue was that their search results plummeted whenever they used exceedingly specific criteria. Thus, the founders realized there needed to be more advanced search engines that would match users' expectations. There is palpable excitement for the Internet and cutting-edge IT technologies in the company's early stages. There were high expectations at the time for how a technology-driven boom could reorganize various industries and consumer behaviour, such as how people would search for new homes and jobs or plan vacations [@benjaminElectronicMarketsVirtual1995]. As a result, new technologies such as cutting-edge search engines were thought to change or replace the functions of once-essential intermediaries like travel agents and real estate agencies [@wigandWhateverHappenedDisintermediation2020].

The idea and execution for what would become the company's fuzzy searching and matching software were twofold. First, the creators opined that a different search and matching engine was required, which would always produce a result, even when the search criteria were imprecise. To achieve this, the traditional methods of expressing data matching problems, which sought to satisfy Boolean expressions, had to be reworked. For example, a typical search query might read something like this: "SURNAME='Smith' AND NATIONALITY='Canadian'". Running this query would return a list of records that meet these criteria. What if you also want to include alternate spellings like "Smit" and "Smithson" in the final result? A boolean expression can only distinguish between true and false, so the company's software would instead use a different probabilistic data matching method. Hence, search results would be ranked based on the likelihood that two data objects will match. For example, the software would examine the sequence of characters in those name variations of "Smith" and conclude that these strings of characters are X% similar and, thus, possibly the same. In addition, as explained on a page from the company's website in 2007, different search criteria may be given more or less weight when determining this similarity score:

> "In traditional searching, all given criteria must be met exactly in order to be part of the results set. In matching, a search may take into consideration multiple criteria, each with a different level of importance. Some criteria may require an absolute match, where a “no” answer rules out a search result altogether.  Other criteria may be less important, so that 'close enough' results combined with exact matches to the most important criteria bring back a list of extremely strong matches."[^search]

[^search]: https://web.archive.org/web/20070301063433/http://www.wcc-group.com/page.aspx?page=pagecontent&id=4171069

Second, the founders believed advanced search would solve a general issue that various real-world industries and businesses were experiencing. Data matching, in this view, offers a _generic_ solution that a wide range of organizations in various settings and industries can adopt with an improved search that would always return results. In line with this, the following snippet from the company's 2009 website promoted the software as a solution that could address a wide range of search issues:

> "[N]o matter how flawed or incomplete the search criteria, ELISE is always able to return a match. Looking for an email that was sent in October about a team meeting? ELISE will find it, even if you got it wrong and the email was sent in September. Searching for a candidate in a 40 km/mile range? ELISE will find the candidate that best fits the criteria, even if they happen to be 48 km/miles away. Even if you have limited knowledge of what you are actually looking for, just a few simple steps with ELISE will provide all of the information you need."[^results]

[^results]: <https://web.archive.org/web/20090704023143/http://www.wcc-group.com/page.aspx?menu=solutions&page=solutions>

When examining WCC's founders as the initial social group, it becomes apparent that their interpretation of the challenges faced by organizations at that time was as follows. In order for goods and services to be effectively discovered by customers within the context of an emerging Internet-based economy, it was crucial for organizations to implement advanced search mechanisms. From a technical standpoint, this problem definition consequently led to the development of search and match software as a generic solution that could be universally applied across various domains. By conceptualizing the problem in this way, the founders aimed to address the overarching need for efficient discovery and enable organizations to adapt to the evolving digital landscape. This approach sought to provide a versatile and adaptable solution that could facilitate the connection between supply and demand across diverse sectors of the economy.

#### From data matching in employment services to identification

Although customers were quite diverse at the time, we could consider them another social group. As such, it would seem that customers accepted WCC's problem definition and the technical solution. For example, based on interview data and the customers mentioned in old WCC marketing materials, we can understand that WCC had customers in diverse domains. At the time, WCC customers used the software solution to match house seekers with suitable houses, job seekers with relevant jobs, wine lovers with wines that suit their tastes, and tourists with their ideal holiday booking. However, despite the variety of industries and sectors served by WCC, not all amounted to a sizable market. Therefore, as the following interviewee recalls, WCC would gradually reduce the software solution's interpretative flexibility and redefine the problem by focusing on fewer, more commercially successful domains, such as public employment:

> "So, yes, we [WCC Group] were very broad. The first customer, a major customer, was the Dutch employment agency UVV. And that made us think. Because all those other customers were small amounts. And the UVV was a significant customer, and that convinced management at the time that it was a great match. And the main reason for that was—and we are still uniquely ourselves in that regard even compared to the open source competition you see now—the bidirectional matching. So ELISE can not only include your own search criteria, but also what the other party wants. [...] So what the job needs and what the employee is looking for does matter. And that is then matched with each other and that is what ELISE can do very well. So, that’s the reason we entered the labour market. And that has now been completely expanded into much more than just matching wishes with supply, and we are now also solving all kinds of preconditions." (Interview 2021-05-31)

Consequently, WCC started to build domain-specific solutions based on the foundation of the original ELISE data matching software. For instance, WCC presently services public and private employment customers through its "Employment Platform". However, as the previous interview excerpt suggests, not all customers, as relevant social groups, shared the same problem definitions for searching and matching. Public and private employment services are the authorities and organizations that connect job seekers with employers to find the right job for a person or the right person for the job. Hence, these customers must match job seekers' skills, competencies, experiences, and preferences with requirements established for an open position, and vice-versa. This problem definition in the employment services domain was solved using "bidirectional matching". Technically, bidirectional matching uses the same fuzzy matching algorithms to determine compatibility between two sets of data records, in this case, job descriptions and job applicants' preferences.

In contrast, in border security and migration management, searching and matching identity data typically only occurs in one direction. A person's identity record typically has no requirements to match, unlike a job announcement in a database that specifies who should apply. Still, the company could solve both problems using the same data matching engine, despite different problem definitions between the employment services domain and the identity and security domain. Contrary to what one might expect from a SCOT analysis, the company could translate and reconcile such data matching across contexts even with these different problem definitions of the customers as relevant social groups. Because unidirectional search is just a streamlined version of bidirectional matching, there is no technical incompatibility between the two designs. As the software functions without the user being aware of this difference, the software design has reached a point of closure as customers perceive their problems to be resolved. The company now creates dedicated, context-specific platforms that build on the search and match engine to address the unique needs of the two contexts. At the same time, the number of contexts that the searching and matching tool can be used has decreased.

The exploration of interpretative flexibility in this section highlights a key moment in the evolution of the search and match software. Initially conceived as a generic solution applicable across various domains, its interpretative flexibility came into play when the problem was redefined in the context of border security and migration management. It became apparent that the traditional application of searching and matching identity data in two directions was not necessary for these specific contexts. As a result, the company has responded by developing dedicated and context-specific platforms that leverage the search and match engine. These tailored platforms address the unique requirements and challenges of border security and migration management, effectively adapting the technology to suit the distinct needs of these contexts. This shift signifies an important realization of the limitations of generic solutions and the recognition of the value of context-specific platforms in addressing the complexities of identification processes in these specific domains.

The following section delves deeper into the links between the company's customers in employment services and deployments of the software for identification in security contexts. In particular, examining the history of how WCC joined the consortium that created the EU Visa Information System can shed light on how identity matching has evolved over time.

### Interpretative flexibility 2: Counterterrorism and the transnational professional networks standardizing sociotechnologies of identification

The company's involvement in the security and identity markets has some interesting connections with WCC's data matching for employment services. In the early 2000s, for instance, WCC worked with the international IT services and consulting firm Accenture to roll out a data matching system for a new platform for the German public employment service [@betlemUtrechtseDatatechnologieMoet2011]. The WCC's then-CEO, Peter Went, discussed this project's success in an interview published in "Database Magazine" in 2006. In the following passage from that interview, Mr Went describes how the two businesses would work together to use WCC's search and match software, such as for a US Customs and Border Protection management system:

> "WCC entered that world [identity matching] through a successful trajectory with Accenture at the Employment Service in Germany. The response was so positive that Accenture decided to hire WCC for a huge project that the consultancy won in 2004 with US Visit, the US border security company. 'They search there, as every traveller to America knows, by face and fingerprint. Ideally suited for our ranking technology, because there are no perfect Boolean-true matches with biometric data.' [Peter Went]" [@rippenSterkePositieHR2006, p. 37, translated from Dutch]

Following the success of the data matching solution for the German employment service, Accenture invited WCC to participate in the contract given to the consulting firm for the United States Visitor and Immigrant Status Indicator Technology (US-VISIT) system. For a long time, the United States has employed this US-VISIT system to track the entry and exit of foreign nationals. While WCC may have played only a supporting role in providing the data matching technology, its participation of WCC in such comprehensive border control information systems exemplifies the growing importance of private actors in developing government biographic and biometric identification systems for controlling mobility [@amooreBiometricBordersGoverning2006]. Overall, WCC executives were keen to participate in large-scale US government systems for identity management and were cognizant of the new opportunities for businesses in the field of security and biometrics in the wake of the September 11 terrorist attacks [@betlemUtrechtseDatatechnologieMoet2011].

The evolution of WCC and its data matching software thus sheds light on the changes to identification systems after the September 11 terrorist attacks. Such changes can essentially be summarized as increased use of new and latent surveillance technologies, such as biometrics and risk profiling, for controlling borders and governing mobility to prevent terrorism marks this post-9-11 period  [@amooreBiometricBordersGoverning2006; @lyonSurveillanceSeptember112003; @lyonSurveillanceSeptember112003]. Post-9/11 measures can be seen in the US-VISIT system, which checks the fingerprints of visa applicants and temporary visitors against national security databases and terrorist watchlists. Tellingly, one news report even mentioned WCC's transition to biometrics and the fact that "it was Accenture that set WCC on the trail of biometric analysis." We see here a new social group interpreting problems of identification and  mphasizing the use of latent biometric data technologies: border enforcement authorities and industry conceived of identification as a matter of security.

#### Homeland Security Presidential Directive 24 and multimodal fusion

We can see how industry and government actors united to redefine identification as a combination of biographic and biometric data. In 2009, WCC published a position paper titled "Homeland Security Presidential Directive 24 (HSPD-24)" that exemplifies this kind of mutual problem-solving. This report describes how the ELISE software could be used to comply with HSPD-24, a framework for interagency cooperation and interoperability of biographic and biometric data as part of US counterterrorism efforts and screening processes against terrorism watchlists. The following excerpt from a press release on the position paper demonstrates how parties like WCC try to comprehend the US government's problem definitions and position WCC's software as a possible solution:

> "The recently [2009] issued presidential directive mandates a new level of interagency cooperation and interoperability to enhance the terrorist screening process and, at the same time, specifies the use of a layered approach to identification utilizing biometric, biographic, and related contextual data. WCC‘s new white paper explores the ramifications of HSPD-24 and explores its implications for the matching software that supports these processes, with a close look at how WCC‘s ELISE ID supports the layered approach." [@wccHSPD24WhitePaper2009]

This position paper is just one example of exchanges between private companies and public agencies. Executives from WCC also attended events like the annual meetings of the National Defense Industrial Association and international biometric conferences. Using the words of @bairdKnowledgePracticeMultisited2017 and @stockmarrSecurityFairs2015, such "security fairs" are gathering places for government and industry actors to exchange information about security best practices and technologies. According to Baird, talks given at such gatherings can (re)align the issues of current and future policy problems with the solutions offered by the industry. These examples illustrate how governments and international industries co-construct and standardize problems and solutions of identification through transnational networks.

> "HSPD-24 recognizes that technological progress and real-world implementations have substantially advanced in recent years, but also that **a lack of biometrics standardization and the existence of conflicting mission security rules limit data-sharing among federal agencies**. It further acknowledges that **biometrics is only one of several layers of identifying data, and that a layered approach instead of a single mechanism** — is needed to improve the executive branch’s ability to identify and screen for persons who may pose a national security threat." [@wccMeetingChallengesHSDP242009, p.1, emphasis added]

Notable in both the position paper "Meeting the Challenges of HSDP-24: A Layered Approach to Accurate Real-Time Identification" [@wccMeetingChallengesHSDP242009] and the directive "NSPD-59 and HSPD-24 on Biometrics for Identification and Screening to Enhance National Security" [@bushNSPD59HSPD24Biometrics2008] is the emphasis lies on the use of multiple biometric identifiers. As a consequence of this problematization, WCC’s proposed solution primarily addresses the organizational challenges associated with establishing a common standard for biometric data.

Overall, these problem definitions and solutions show how identification and screening were problematized as security issues at that time, and of how design solutions followed. First, the documents identify the need to standardize and share agencies’ biographic and biometric data to screen individuals against “known and suspected terrorists” watchlists. Second, they conclude that matching based solely on either biographic or biometric data will be insufficient and that a "layered approach" [@bushNSPD59HSPD24Biometrics2008] is required, which makes use of multiple methods and data to identify and screen individuals effectively.

> "While HSPD-24 does not provide any definition of a layered approach, it is understood that it refers to successively applying any or all available biographic, biometric, and contextual identifying data in order to arrive at an informed and accurate decision about a person’s identity. This process is commonly known as identity matching, but until now identity matching solutions were primarily single-layered or siloed approaches that used a single biometric modality or a single factor such as a name to perform the identification." [@wccMeetingChallengesHSDP242009, p. 3]

<!-- TODO: Add more SCOT analysis -->

The WCC's proposed solution builds on the "multi-modal fusion" concept they developed in previous years. Contemporary identification trends based on biographic and biometric data led WCC to develop these new features in the ELISE software solution. For example, the company would start to offer a new "vendor neutral and future proof" software architecture that allows new biometric standards to be plugged in "as soon as they are ratified and deployed" [@wccMeetingChallengesHSDP242009, p. 7]. The possibility of new social groups forming and reinstating interpretive flexibility means the closure and reduction of design flexibility are temporary. Accordingly, the issues and solutions of identification were once again open to interpretive flexibility in the post-9/11 era. The United States government, as a social group, re-problematized policy problems of identifying people in the context of security as a problem to be solved with new technical solutions. Hence, biometrics' growing significance and data interoperability between various agencies as technological solutions to identify potential threats. However, as these examples of exchanges between government and business actors have demonstrated, this is not a unidirectional process. Instead, the interactions between public and private actors demonstrate the joint production of these identification issues and their solutions.

The analysis of interpretative flexibility and its relevance to the biography of the identification artefact reveals a significant key moment in its trajectory. At the time when identification and screening became problematized as security issues, design solutions aligned with this emerging focus. This resulted in the rise of biometrics and multimodal approaches as technical solutions, reflecting the evolving needs of identification practices. Recognizing the contemporary trends in identification that relied on both biographic and biometric data, the company, WCC, proactively developed new features within the ELISE software solution. This adaptive response demonstrates how interpretative flexibility enables the identification artefact to evolve and remain relevant in the face of shifting priorities and emerging technologies.

#### The MITRE challenge for multicultural name matching

WCC's participation in a curious event called "the MITRE challenge" exemplifies the growing popularity and widespread adoption of data matching and the efforts made to stabilize the challenges and potential solutions of data matching based on biographic/alphanumeric data. In 2011, the MITRE Corporation, a non-profit research organization for the US federal government, held an international competition for individuals, companies, and researchers to come up with the best solution for "multicultural name matching" [@millerInternationalMulticulturalName2012]. The idea was sparked by the now well-known "Netflix prize", a competition that the then-rising streaming service held in which participants were invited to submit technical solutions that could outperform Netflix's movie recommendation system based on a set of provided data. Similarly, the MITRE group supplied two data sets with names (given name, surname) which included name variations from various cultures and languages that would realistically be found in population registers.

The basic idea was to match names from a smaller "query list" with names from a more extensive "index list" (both in Latin script). Therefore, to compete in the MITRE challenge, participants used these data sets to develop technical solutions to find name matches between the query and index lists. Participants then uploaded their results to a special online platform where they could see their position on a public leaderboard. Therefore, the competition could be seen as a closure mechanism to reduce design flexibility and set a benchmark against which data matching solutions can be evaluated.

The creation of these multi-cultural naming data sets uncovers otherwise unseen ideas and concepts that developers use when creating and testing identification technologies, such as how data matching systems conceptualize the relationships between various naming conventions. For instance, the MITRE challenge's data drew on prior work from MITRE researchers [@millerInfrastructureToolsMethodology2008] to develop a taxonomy and data set of person name variants, including "transliteration variation[s], database fielding errors, segmentation differences, incomplete names, titles, initials, abbreviations, nicknames, typos, OCR errors, and truncated data" [@arehartGroundTruthDataset2008, p. 1136]. In order to represent a group of "culturally diverse Romanized person names" the 2008 set of about 70,000 names drew on both public and commercial data sources and tools. The MITRE researchers used publicly available sources like the Death Master File[^dmf], which includes the names of people who had Social Security numbers in the United States, and the "Mémoire des hommes"[^mhd], which includes the names of deceased French soldiers. All data are situated, so it is crucial to inquire about the circumstances surrounding data collection [@dignazioDataFeminism2020]. However, the resulting MITRE data sets obscure these intricate processes that went into collecting and transforming data for testing and developing data matching technologies.

[^dmf]: <https://www.ssab.gov/research/social-security-and-the-death-master-file/>

[^mhd]: <https://www.memoiredeshommes.sga.defense.gouv.fr/fr/arkotheque/navigation_facette/index.php?f=opendata>

The 2008 MITRE data sets were carefully curated to be "large, multicultural, and realistic" [@arehartGroundTruthDataset2008, p. 1136]. Surprisingly, however, none of the papers' authors raises ethical concerns about the data practices of using the names of deceased persons or other problematic choices. For example, the papers devote much attention to examples and peculiarities of Arabic names. Thus, we can learn how Arabic names are technically handled in the research: "With the exception of Arabic names, we used one set of adjudication guidelines that represents a middle-of-the-road view of what should match" [@arehartGroundTruthDataset2008, p.1138]. The adjudication process described here involves human annotators looking through data sets and indicating how they believe the names should match (see Figure \@ref(fig:adjudication)).[^adjudication] But a different process was used for Arabic names "due to the relatively larger range of name variations and possible judgments [of abjudicators]" (p. 1138).

<!-- TODO: Add analysis in SCOT terms -->

<!-- "An ongoing, open contest to encourage innovation in technologies of interest to the federal government, the Challenge invites the best ideas and novel approaches to solve critical issues facing MITRE's sponsors." -->

The MITRE challenge announcement web page mentions that the MITRE is "helping the Department of Homeland Security and other key national security sponsors with their identity matching needs." As such, they are a social group representing these agencies who share a problematization of how "multicultural name matching affects everything from airport security to Social Security" [@themitrecorporationMITREInvitesTechnology2011]. In this context, the meaning of name matching changed to an issue of security. As critical security scholars have pointed out, meanings attributed to borders and border control shape the technologies developed and employed (see Chapter 2). Hence, these changes contributed to redefining the technical specifications of name matching to focus on identifying supposed security threats through multicultural name matching and biometrics. As a result, a person with an Arabic name may be profiled as suspicious due to what Ruha Benjamin [-@benjaminRaceTechnologyAbolitionist2019] has called "coded inequity". In this case, the prevalence of scientific practices to deal with the technicalities of Arabic names simultaneously reifies preexisting profiling categories that unevenly target specific people.[^profiling] By participating in the challenge and finding solutions to the name-matching puzzles, actors can adopt the same logic. As we will see later, this may explain why one of ELISE's new features was matching based on Arabic and Asian name transcriptions and transliterations.

[^adjudication]: Unfortunately, not much is known about who these adjudicators might have been.

[^profiling]: See also @moghulUnapologeticRacialProfiling2016, quoted in @dignazioDataFeminism2020.

```{r adjudication, echo=FALSE, fig.cap="An example of the adjudication process from the paper."}
knitr::include_graphics("figures/adjudication.pdf")
```

In the end, WCC participated in the MITRE challenge because they knew that if they won the challenge, they would be invited to present their software solution ELISE to "the three letter agencies in America" (Interview 2021-05-31). These agencies include the Federal Bureau of Investigation (FBI), the Drug Enforcement Agency (DEA) and the Central Intelligence Agency (CIA). WCC did exceptionally well on the test, earning a spot as one of only three "Top Tier Vendors" [@WCCWinsTop2011] and securing the meeting. While this success may not have directly led to contracts with US government systems, it probably strengthened the Accenture partnership. In 2012, for example, the European Commission chose a consortium of companies, including Accenture, Morpho, and HP, to maintain the EU Visa Information and Biometric Matching Systems, with WCC serving as a subcontractor to provide the search engine for alphanumeric data (see also Figure \@ref(fig:floorplan)). Furthermore, WCC remained a subcontractor of Accenture's, continuing to supply the solution for searching and matching biographical and biometric data in the UNHCR's Identity Management System.

```{r floorplan, echo=FALSE, fig.cap="This picture of a biometrics conference's floorplan from 2011 shows how close the VIS consortium members were to each other at the conference."}
knitr::include_graphics("figures/floorplan/floorplan-biometrics-2011-highlight.png")
```

Problem definitions and design solutions for identifying people have evolved, as evidenced by these two subsections on the interpretive flexibility of the ELISE software. WCC initially intended the software to be a domain-agnostic tool for matching data records. Later, the company shifted to focus on serving two distinct data matching markets: the first, which serves public and private employment service providers, and the second, which serves the identity and security industry. By retracing these changes, we gained an understanding of the moments and places where governmental and private actors interact to co-produce and define the issues and solutions related to matching identity data. By tracking changes in interpretative flexibility and mechanisms of closure of design flexibility, it was also possible to understand how actors may end up caught up in the "logic of security" [@balzacqContestingSecurityStrategies2015], such as the use of biometrics and the coded inequalities in name matching. The standardization of identity data matching methods and technologies, such as the plug-in architecture for biometric standards and the MITRE challenge for best practices for name matching, are examples of how actors react to technological closure.

The exploration of interpretative flexibility and its relevance for the biography of the identification artefact reveals a key moment exemplified by WCC's participation in the "the MITRE challenge." This event serves as a notable example of the increasing popularity and widespread adoption of data matching techniques, as well as the concerted efforts to address the challenges and potential solutions related to matching biographic and alphanumeric data. The creation of data sets specifically for this challenge sheds light on the process of knowledge creation and expertise in data matching. Furthermore, the introduction of new name matching functions highlights the evolving focus on identifying potential security threats through multicultural name matching and biometrics. Additionally, the involvement of various actors in the competition underscores the formation of networks aimed at securing contracts for the development of identification systems for different states. These insights highlight the dynamic nature of the identification artefact's biography, shaped by evolving technologies, problematics, and the intricate relationships between actors within the identification ecosystem.

### Gateway moment 1: connecting central EU and Member States systems

Let us now discuss the second heuristic key moment of my methodology. This case concerns the Visa Information System (VIS), the European system used to grant Visa at European embassies and consulates worldwide. According to the "Report on the technical function of the Visa Information System (VIS)" [@eu-lisaReportTechnicalFunction2020], the central VIS system received 17 million visa applications in 2019 alone, which thus resulted in the registration of dozens of non-EU citizens' personal information. The report notes 25 million alphanumeric searches on this data, making them an essential form of identification for the VIS. However, national systems input these alphanumeric data into the central EU system. It is well known that the resulting system data is often inaccurate, incorrect, or not up to date [@fraWatchfulEyesBiometrics2018]. Hence, end-users may need to search on sometimes ambiguous alphanumeric data, such as name, date of birth, nationality, or passport number. As we saw in Chapter 5, researchers tend to focus on biometric identification, but alphanumeric data are also essential for identification. The VIS carries out these alphanumeric search queries using the ELISE search and match program. Aside from the need for flexible searching on alphanumeric data, the report explains that users are concerned about the system's availability and response times. It is possible, therefore, that these issues reflect the identification infrastructure's large and expanding scale.[^response-time]

[^response-time]: Indicative of the search volume is the emphasis placed on the median response time for queries (less than 0.8 seconds). The report adds that this response time excludes retrieval of a visa application by its number or visa-sticker number, which has a marginally quicker response time. Additionally, the amount of data stored keeps growing, and by the middle of 2018, the ELISE engine's license had been upgraded to accommodate these increases [@eu-lisaReportTechnicalFunction2020].

#### The VIS evolutions project and the problems of backwards compatibility

While the Visa Information System (VIS) is now widely available, its initial iterations were rolled out to different regions over time. The increased system usage necessitated launching a project to expand the system's capabilities called "VIS Evolutions". The then newly established European Agency for the Operational Management of Large Scale IT Systems in the Area of Freedom, Security, and Justice (eu-LISA) was overseeing the development of a new VIS system through a consortium of companies. The result would be a "completely new VIS system in terms of infrastructure, software versions, and search engine" [@eu-lisaVISReportPursuant2016, p. 8]. Interestingly, the project's objectives included changing "the search engine to improve its performance." [@eu-lisaReportTechnicalFunctioning2013, p. 8]. This way, WCC was part of the consortium as a "subcontractor for a maintenance contract" (Fieldnotes 24-07-2020) for building this upgraded IT infrastructure. The software WCC supplied would provide an improved technical component to search and match based on alphanumeric data. It is common to gloss over these aspects of the VIS's history. However, understanding these evolutions of the VIS reflects how the identification infrastructure expands and grows from its existing technological basis. Following this growth and deployments in countries worldwide can highlight how system builders grapple with technical difficulties, develop solutions, and reach compromises.

The fact that the VIS Evolutions project improved upon a pre-existing system is significant. The system upgrade would entail a process where everything would need to operate normally for Member States already connecting to the central VIS system. Consequently, system technical specifications for searching as defined for the original VIS system had to be adhered to in order for the upgrade to be compatible with the existing integrations between the central EU VIS system and the VIS systems of the Member States (i.e., sometimes known as "backwards compatibility"). As a result, there was a restriction on the number of ELISE matching functionalities that could be implemented or used due to system legacies left over from the previous iteration of VIS. For instance, it would not be possible to add fuzzy name matching based on how a name is pronounced if such a feature was not available previously. Another example is that ELISE would typically "follow best practices" by matching place names (for instance, "Moscow" — "Moscva") using databases that contain known variations of these place names. However, in the EU-VIS, the ELISE software would have to adhere to the earlier strategy based on a metric that assesses the variation in place names. Nevertheless, the system improved the processing power and performance of the VIS, revealing substantial increases in usage, such as by Member States' consular posts [@eu-lisaVISReportPursuant2016]. As such, path dependencies in developing the new and improved VIS prevented the addition of complex identity matching.

#### ELISE as a gateway technology

Can the ELISE system be considered as a gateway connecting the central EU-VIS and the MS-VIS systems of the individual member states? According to our definition of a gateway, the findings indicate that this may be the case. Remember how we defined gateways: as various methods for disparate systems and communities of practice to establish the sociotechnical connections needed to collaborate as part of more extensive infrastructures. The ELISE system, which serves as the matching engine for EU-VIS, forms an essential bridge between gaps in Member States' data practices. This bridging aspect is present because ELISE's search and match engine functions link various communities of practice and eliminate gaps between disparate systems and data practices. For example, the following quote from an EU Fundamental Rights Agency report illustrates how discrepancies in data practices among MS could manifest themselves in VIS:

> "[I]nsufficient verification procedures before entering data into VIS were perceived as problematic by some persons who FRA interviewed. (…) To avoid mistakes, in Poland, migrants are asked to fill in a special questionnaire, which is the basis for transliteration. The International Civil Aviation Organization (ICAO) has developed standardised rules in relation to Latin, Cyrillic and Arabic characters. In Germany, when the police finds evident spelling errors the persons are presented with these errors to clarify them. In some EU Member States, the person has to read and verify the information being inserted. In Finland, in accordance with the Finnish ‘asylum guidelines’ the applicant signs the U3A form, which includes the personal data." [@fraWatchfulEyesBiometrics2018, p. 85]

The issues raised in the excerpt are symptomatic of a broader issue in identification: there is no universally accepted method for transliterating names into the Latin alphabet (the required format for EU-VIS). As a result, various member states may have distinct transliteration procedures, systems, and methods. These differences may introduce inconsistencies. In the examples from the excerpt above, member states attempt to improve the data quality by involving relevant parties in cross-checking their data. So, because of these potential differences in transliterations of names, we can assume that there is a clear benefit in having a fuzzy match that considers nuanced name variations. Similar difficulties may arise when attempting to convert, for instance, dates between calendars of different religions and cultures.[^calendars] Therefore, ELISE's integration into VIS may support the hypothesis that the software acts as a gateway by mediating between incompatible data practices and a lack of common standards. Even if some information is incomplete or inaccurate, one MS can still identify Schengen visa applicants who have applied to other EU States. This finding lends credence to the idea that ELISE serves as a gateway by facilitating interaction between heterogeneous national information systems.

[^calendars]: The recording of dates of birth can present challenges, as highlighted in @fraWatchfulEyesBiometrics2018 which draws on insights from a legal assistance provider for asylum seekers. The report recounts how many Afghans expressed their age or birth year based on the Persian Afghan calendar. However, converting these dates to the Georgian calendar, the standard in information systems, can be complex due to variations in year lengths and starting points between the two calendar systems. Accurate conversion tools are necessary to ensure alignment between the calendar systems, as inaccuracies may result in discrepancies in the registered date of birth in the Gregorian calendar format. The report by FRA points out that inconsistencies in registered dates of birth can create mistrust and suspicion towards asylum seekers, particularly when contrasting information is found in other EU member states' systems.

Another factor supporting ELISE’s conceptualization as a gateway is how the software employs data replication to act as a layer on top of databases. In other words, ELISE copies data from the EU-VIS database to its in-memory database to match identity data quickly and accurately. Because of its technical design, ELISE can integrate and replicate data from multiple sources without disrupting pre-existing infrastructure. As such, the ELISE system's flexible configuration for replicating data to the system's in-memory database enables the linking of disparate systems with varying data formats. WCC refers to this software feature as "data agnosticism", and some of their other customers use it even more frequently. For instance, the speaker in the following interviewee excerpt describes how WCC used application programming interfaces (APIs) to incorporate additional data sources into a different customer's border control system. This use of APIs makes it possible to connect data sources where direct access to a database is not allowed, such as Interpol's stolen lost travel document database. However, ELISE operates most effectively when all data are "ingested" and made accessible for all matching features:

> "Right, so I think this is a really good architecture diagram. [On one side] you’ve got [the WCC system]. And, you’ve got specific interfaces to, on the left, you’ve got the systems that are providing us with passenger data. You see that? And the different standards: PNR gov, four other operators that are maybe non-standard. But different formats that we can still cope with. At the bottom you’ve got watch lists, which we have the permission to ingest into [the WCC system]. So replicate the data, yeah? (…) There’s an example [of one of WCC's customers] where we want to interface to the Interpol stolen lost travel document database. But they will not let us ingest it into [the WCC system]. (…) [So in this case] you have an API connection to an external database. You probably work out quite quickly that if we have to send the passenger data that way and then get a hit or no hit, or match or no match back. They have their own matching systems. (…) But ideally we want to be ingesting the data so that we can use multicultural name matching, fuzzy matching, etcetera. But this is an example which is quite common I believe. That we can’t get a copy of the database. And we [then] have to have an interface [to] an external database like this." (Interview 2021-07-01)

In these scenarios, ELISE exemplifies another essential aspect of gateways: their ability to add a layer of compatibility to facilitate communication between different subsystems that use different data standards and protocols. In the case of EU-VIS, ELISE is used as a search and match engine by replicating data into a fast in-memory database system. In other cases, the software is used to combine data elements from various sources into its internal data model to search and matched on different data sources (for example, an organization's data and external watch list) using a single query operation. From the results presented here, ELISE's biographic matching and multi-cultural name matching features help to sidestep problems with identity data collection. Hence, it can conceivably be hypothesized that the ELISE system acts as a gateway by bridging member states' heterogeneous data practices. However, ELISE merely duplicates these sources into a system-specific data format to search and match. As a result, according to @egyediInfrastructureFlexibilityCreated2001's gateway typology, the system can be considered a _dedicated gateway_ connecting only a limited number of subsystems. Nevertheless, the gateway moments discussed in the next section will show how some standardized components allow the system to be viewed as a more generic gateway.

Articulating ELISE as a gateway unveils valuable insights that might otherwise remain undetectable. It allows us to recognize and appreciate the role of ELISE in facilitating communication between the central EU-VIS system and the EU-VIS Member State systems, which may operate on slightly different data standards and protocols. Acting as a compatibility layer, ELISE bridges the gap and enables seamless interoperability between these systems. This layer of compatibility is essential for the efficient exchange of data, ensuring that identification processes can function smoothly across diverse systems. The gateway framing sheds light on the intricate technical and logistical challenges involved in integrating different identification systems, highlighting the significance of ELISE as a crucial component in harmonizing and facilitating communication within the broader identification infrastructure.

### Gateway moment 2: Standardization of gateways

The deployment of the ELISE software in the IND systems demonstrates how identification knowledge spreads and conceivably standardizes across organizations. Compared to EU-VIS, the INDiGO had a much more fine-grained configuration, and sophisticated matching functionalities were possible for the IND's context. For instance, old meeting minutes described the deliberations between WCC and IND in setting the appropriate weights for various search criteria, such as configuring the importance of matching on last name in calculating match scores. Nevertheless, what is remarkable about this gateway moment is that it reveals how identity matching capabilities move between organizations through the ELISE software's integration into the IND systems and subsequent software updates.

#### INDiGO and travelling data matching knowledge

In this section, we examine a compelling example that demonstrates how data matching knowledge travels and transfers between organizations. A presentation given in 2013 entitled "ELISE: New Features" provides substantial evidence in favour of this hypothesis. WCC showed IND's employees the upgraded capabilities of ELISE in its upcoming version, which the IND could use for the agency's search operations. The presentation also contained recent company updates, such as the EU-VIS as a "recent project" and a slide showing their rank in the MITRE multi-cultural name challenge. Several slides titled "new features" described new name matching functionalities. Interesting new features listed included "transcription and transliteration of Arabic and Asian names", both on "name variations in original script" (with examples in Arabic) and "name variations in Roman script". One slide mentions  "EU-VIS specific feature", which they name "partial" and gives an example: "Wij should match Wijfels". A table comparing features in matching features of different ELISE versions includes a feature to match on also-known-as information provided by third parties and gives an example: "'Ahmed the Tall' should match with 'Sheikh Ahmed Salim Swedan' and vice versa).

The ELISE software upgrades in the IND's systems, including new name matching capabilities developed in the context of the MITRE challenge, show how name- and identity-matching expertise can move between organizations and contexts. For instance, a remark on the slides about how "name databases" have been upgraded are actually quite revealing of the way data matching expertise is compiled and distributed. These name databases include CJKI's "Database of Arabic Names (DAN)", which is described on their website as follows: "DAN covers over 6.5 million entries and consists of Arabic personal names — both given names and surnames — and name variants mapped to the original Arabic script with a large variety of supplementary information" According to the website, the DAN database "plays an important role in helping software developers, especially of security applications related to anti-money laundering and terror watchlists, as well as natural language processing tools, enhance their technology by enabling named entity recognition and extraction, machine translation, variant normalization, and information retrieval of Arabic names." [^cjki] As a result, businesses like the "CJKI Dictionary Institute" compile databases of names for languages, including Arabic and other East Asian names. These databases are then licensed and appear in a variety of software programs. Therefore, in 2013, following the MITRE challenge and the EU-VIS project, the slides demonstrated how newly developed matching features for those other contexts could be transported to the IND's system to improve the agency's identity matching.

[^cjki]: <https://www.cjk.org/data/arabic/proper/database-arabic-names/>

<!-- TODO: add more data -->

<!-- TODO: add more analysis -->

The presentation of ELISE software updates also gives insight into the relations between the generification of the identification software package and the development of new customer-specific data matching functionalities [compare with @pollockGenerificationStrategyHow2016]. Over time, the ELISE "core", as it is known at WCC, has accumulated a wide variety of matching features developed for different contexts. As such, the metaphor of a core and a layer depicts how domain-specific solutions, such as those for security or public employment services, are "wrapped around" the ELISE core, which serves as a central and generic data matching system component. In this way, we can conceivably conceptualize ELISE as a reusable system that "transports" identification expertise across organizations through its identity- and name-matching features. In addition, when WCC releases a new ELISE version, customers can choose to upgrade their current software version and gain access to new features. It is also possible that some matching aspects will not work as well outside their intended settings. Many matching features, for instance, were not utilized in the EU-VIS due to backwards compatibility concerns. The software nevertheless builds up data matching knowledge that can be used in new settings.

#### Gateway standards and the problem of defining duplicates

Software vendors face difficulties when customized features for individual customers are difficult or impossible to port. During fieldwork, such a case surfaced while discussing with WCC staff the development of a new version of a component to query and resolve duplicates (also known as deduplication) in the IND databases (Fieldnotes 2020-07-06). The company was weighing the benefits of a custom-made solution for the IND deduplication tool versus a standardized solution for deduplication that other customers could use in the future. One of the project's employees outlined the differences between the two alternatives. On the one hand, a solution could be made that would be "extremely customizable" but would take work to tailor to customers' context and deduplication needs.

On the other hand, what he called a "ready-made solution" would be easier to set up but harder to customize for specific customers' deduplication needs. Indeed, it soon became apparent that defining what counts as a duplicate identity is difficult, especially when matching using biographic data. For example, if two sets of biometric data, such as fingerprints, match, the data likely belong to the same person because fingerprints are unique.  In contrast, many individuals share the same name and birthdate. Therefore, according to one interviewee, biographic data deduplication necessitates a more contextual approach:

> "You really have to start working differently with biographical data. In that case, you have to make a model based on the data that the customer has. What we always look at is: how old is the data? So if you have two records and they match. Which one is much more recent, for example. Of course, if it is obvious that all fields match, then it is very easy [to identify a duplicate]. But that is often not the case. You often have differences. Sometimes an identity document number does not match. That [document] can of course have been replaced. So then you look at from when the field [was created]. Sometimes surnames do not match. What frequently has happened is that people marry and then, incorrectly use the last name of the man. Especially women. Nowadays, you always have to use your own maiden name in the Netherlands, your own last name. Anyway, so those are things we're looking at." (Interview 2021-05-31)

As the interviewee points out, dealing with biographic data presents challenges due to the circumstances of producing data. For example, naming practices can vary significantly worldwide and change over time. As a result, a common theme in the grey literature on data matching is that biometrics can unambiguously identify duplicates. However, biometrics adds different complexities to data matching and deduplication. As we know from literature, there is no such thing as raw biometric data that can be utilized for matching purposes [e.g, @vanderploegIllegalBodyEurodac1999]. Often, biometric data cannot be reused because the algorithmic systems that produce biometric data are closed and proprietary. The following remark illustrates how biometric data, in the form of templates, cannot be used without a licence to the corresponding algorithms:

> "You make a template from images using an algorithm. That template will be used for matching. If there are proprietary templates generated by an algorithm that we don't have [access to], there's nothing we can do with those templates. And that is something that we've had. We had a customer who came to us with only templates. So, without the images. They asked: Can you use these? We asked them: Do you have the algorithm? They replied: No, they no longer have that as they no longer had the right to that algorithm in terms of the licence. So we replied that there was nothing we could do about that." (Interview 2021-05-31)

We can infer from the quote from the interview that these data materialities influence how data can be used, enabling and restricting specific types of data matching and deduplication. The biometric templates can only be used with a specific sociotechnical configuration because the stored data formats are incompatible with other biometric systems. However, here ELISE can play a crucial role as a gateway technology through the system's "Vendor-agnostic Biometric Algorithm plug and play" architecture, which "allows customers to become vendor-independent and avoid vendor lock-in as biometric algorithms can be exchanged while the system is running in production" [WCCBrochure2022, p. 7]. The ELISE system can be viewed as a gateway that creates a layer of compatibility between the biometric technologies of different vendors, much like the "plug and play" metaphor suggests.

#### Standardizing gateway's data models

The abovementioned findings are also consistent with @loukissasAllDataAre2019's adage that "all data are local". As Loukissas says, duplicates can be "key to learning about the heterogeneity of data infrastructures" [@loukissasAllDataAre2019]. It is thus difficult to transfer knowledge of why customers may have duplicates in their databases to other organizations because the reasons are so specific to the local data practices of the customers themselves. However, dealing with these particularities and local contexts becomes necessary when systems and infrastructures converge. The following interview excerpt alludes to the requirement for standardization when upgrading legacy systems or when various systems must interoperate:

> "An example that I always give is: when personal data was stored, the hair colour field contained free text. So sometimes brown, sometimes light brown, sometimes 'brn', sometimes as abbreviated or light. (…) So that was free text. To go from there to a pick list, a drop-down list — what they wanted in the new system — that's quite complicated. And that is something you have to do when you connect systems with each other. Now in this case it had to because the [organization] went from a legacy to a new system. But sometimes when you talk about interoperability, the winged words of the EU as well, then you have to be able to compare these kinds of data with each other. For example, a VIS system that contains something and a nationally different system that contains exactly the same data, but the field names are different. Or the notation is just a little different. Even then, it should be possible to match those data. So, there will need to be standard models. The EU is trying to achieve this with UMF, the universal messaging format. America has a number of standards that are also included in our product; NIEM is one of them. And in addition to having standards, you also need to be able to match smartly, and that won't be easy. That's quite complex." (Interview 2021-05-31)

In the interview quote, the informant alludes to two approaches to dealing with the heterogeneity of data infrastructures. One approach is to establish uniformity in the values used. The informant’s first example shows how the values of a free-form field for hair colour in a law enforcement database were standardized to determine categorical values. Another method is to adopt common standards to reduce the various formats used by different systems and organizations. The informants’s second example focuses on the interoperability framework for EU information systems in the area of justice and home affairs. The introduction of a new data standard, the Universal Messaging Format (UMF), that would allow for consistent data exchange is a crucial component to achieving data interoperability, as the relevant EU legislation also specifies:

> "The universal message format (UMF) should serve as a standard for structured, cross-border information exchange between information systems, authorities or organisations in the field of Justice and Home Affairs. The UMF should define a common vocabulary and logical structures for commonly exchanged information with the **objective to facilitate interoperability by enabling the creation and reading of the contents of exchanges in a consistent and semantically equivalent manner.**" [@europeanunionRegulationEU20192019, p. 22, emphasis added]

During the fieldwork, it became clear that WCC was familiar with this UMF data standard due to their previous work with a customer in the law enforcement sector. We could conclude that the UMF format is only partially original. Infrastructures, as discussed in the literature review, are not built from the ground up but rather emerge from the interplay of various pre-existing systems, practices, and communities. Similarly, one interviewee remarked that the UMF data model is similar to the POLE model, which is used by law enforcement agencies all over the world:

> "So POLE, persons objects locations and events. That model was used by police before computers. So, that’s the model that any police forces, anywhere [in the world], use to categorize and classify crimes. So, if there is a crime, there are persons: the victim and the suspect. There are objects like weapon, like, now it’s getting even different because of technology. Now, the objects are getting to be more a means of communication, any means of communication: the Internet, a mobile cellphone. Those are all objects. If there is a car, that’s an object. Licence plate, vehicle, aeroplane, boat, so that’s an object. (…) The location is not only the location of the event. The location can also include the addresses of the people involved in the crime. Or, addresses where they used to go to. Any means of address, or regions, or even a journey. So, a route between two points. For example, there may be a crime where they used a car and they escaped from the crime scene at point A and then they hid in point B, from point A to B. And the event is the offence. What’s the offensive action. That’s the event. (…) So that model is what they are now trying to use in the systems, as a data model. And what the European Union did is to follow this model — but they didn’t announce this anywhere. But following this model they developed a standard for the format and the exchange of the data related to law enforcement. And this standard format is named UMF, Universal Message Format." (Interview 2020-07-30)

UMF appears to have originated at the European Union Agency for Law Enforcement Cooperation (Europol), where the data model served as a gateway for cross-border data exchange. In 2014, Europol published an information sheet on UMF that makes the following statement: "It must be emphasised that UMF is not the internal structure of systems/databases (you are not required to change your national systems, legislation or processes!) but rather an XML-based data format acting as a layer between them to be used whenever structured messages cross national borders." [@europolUniversalMessageFormat2014] The UMF connects systems while keeping their internal database structures intact. In this way, the UMF data model attempts to solve the problems of cross-border standardization and as a response to "gateway problems" [@edwardsUnderstandingInfrastructureDynamics2007].

Indeed, the first page of the Europol UMF brochure similarly describes the problem of law enforcement databases having similar data but in different formats using examples such as plug-and-socket standardization. According to the brochure's definition of UMF Version 1.0, it is "a standard or agreement on what the structure of the most important law enforcement concepts when they are exchanged across borders should be" [@europolUniversalMessageFormat2014, p.3]. Since the UMF acts as a kind of multi-plug adapter between the concepts in various agencies' internal data models and the concepts in the UMF's "reference model" (See also Figure \@ref(fig:umf-mapping)), it is a prime example of a gateway. The result of this mapping is that information is in a standardized format that can be used and understood in data exchanges across international borders. As a gateway, the UMF data model thus enables "the linking of heterogeneous systems into networks" [edwardsUnderstandingInfrastructureDynamics2007, p. 7]. Furthermore, because it provides a single standard to which any number of countries and agencies can map their data model concepts, we can consider UMF a generic gateway technology.

```{r umf-mapping, echo=FALSE, fig.cap="This diagram from Europol   (2013) shows how concepts from national databases are mapped to UMF."}
knitr::include_graphics("figures/europol-umf-mapping.pdf")
```

Crucially, and unlike other data-sharing methods, gateways do not always necessitate changes to "national systems, legislation or processes", as the Europol UMF brochure points out. Instead, gateways can connect systems using different data-exchange protocols and from heterogeneous systems of practice. As such, gateway moments reveal connections between systems and organizations that do not require major adaptations. . However, it would be a mistake to think of gateways as "neutral" hardware, such as adapters for different plug and socket types. On the contrary, gateways can pave the way for more extensive interoperability in the future while, simultaneously spread of expertise, such as about matching identity data, across organizations. <!-- Furthermore, these gateway moments between software and infrastructures demonstrated the spread of knowledge about matching identity data across institutions. TODO: expand this paragraph-->

Indeed, as mentioned above, the UMF is highlighted as an essential aspect of the framework for the interoperability of EU information systems. First, in 2019 WCC presented their solutions at the annual eu-LISA conference. In the company's presentation, WCC suggested their searching and matching solution as a potential application for the Single Search Interface, another gateway-like technology that will use the UMF standard to query EU databases, which have their specific data models, simultaneously. Second, in 2020 WCC once more took part in an eu-LISA organized event, the industry round table. This presentation suggested the software solution as a possibility for the Common Identity Repository and Multiple Identity Repository. The first is a future database containing links for an identity to identity data in other systems using the UMF. The latter is a new component that should detect duplicate identities in the Common Identity Repository. These two presentations thus demonstrate the design flexibility of WCC's software, which can offer gateway-like functionalities for various contexts, such as searching and deduplicating identities across multiple incompatible databases.

The analysis of the gateway moments reveals its inherent value in uncovering aspects that may not be readily apparent. One such element is the understanding of how identity matching capabilities travels across organizations through the integration of the ELISE software into the IND systems and subsequent software updates. This integration enables the transfer of data matching knowledge as organizations adopt and utilize the ELISE software for their identification processes. Particularly revealing is the inclusion of "name databases" within ELISE, which serves as a remarkable insight into the compilation and distribution of data matching expertise. These databases, utilized by numerous organizations and companies for various purposes, highlight the broader network through which data matching knowledge travels and is shared.

By conceptualizing ELISE as a reusable system that effectively transports identification expertise across organizations through its identity- and name-matching features, we gain a deeper comprehension of the knowledge transfer dynamics within the identification ecosystem. This perspective allows us to appreciate the significance of ELISE as a facilitator of knowledge dissemination and utilization. However, challenges arise for software vendors when attempting to customize features for individual customers, such as in the case of deduplication. This highlights the ongoing tension between generic and custom-made solutions, as software vendors must navigate the trade-offs and compatibility issues that arise. Additionally, the issue of proprietary templates in biometrics poses another challenge. Software vendors can employ strategies such as plug-and-play approaches to address the gateway problem of managing multiple formats and compatibility, ensuring that identification systems can effectively communicate and integrate with various data sources. Standardization efforts, such as the Universal Messaging Format, provide an alternative gateway technology by establishing a common standard to which other data can be mapped. These insights shed light on the intricate dynamics involved in the dissemination, adaptation, and utilization of identification technologies and expertise, highlighting the complexities and trade-offs faced by organizations and software vendors.

<!-- ## Imagining gateways and interpretative flexibility -->

<!-- The final empirical example demonstrates how moments of interpretative flexibility and gateways are inextricably linked. Such links are especially noticeable when WCC demonstrates its data matching expertise in sales and pre-sales activities. For instance, WCC representatives must envision the challenges and potential solutions for future data matching solutions and anticipate the implications of connecting to existing systems when presenting their expertise at industry conferences. -->

<!-- For instance, in 2019, I attended the eu-LISA annual conference, where WCC was invited to present their solutions in the context of a new interoperability architecture for the EU systems. The conference presentation focused on data matching and interoperability using the POLE model (People, Object, Location, Event). This POLE model is a standard data model used by law enforcement to represent crimes, people, and objects. WCC's familiarity with the POLE standard comes partly from the company's previous work for another consumer. It is worth noting that the POLE has influenced the Universal Messaging Format (UMF), which will be used in the interoperability framework. The takeaway is the importance of having a single data standard for matching purposes before attempting data matching across multiple (legacy) systems. Near the end of the presentation, WCC suggested their searching and matching solution as a potential application for the Single Search Interface, one of the interoperability components that will use the UMF standard to query all EU databases simultaneously. -->

<!-- TODO: Space for a paragraph to further explain the first example -->

<!-- https://bulletin.cepol.europa.eu/index.php/bulletin/article/view/535 -->

<!-- Finally, in 2020 WCC once more took part in an eu-LISA organized event, the industry round table. While the presentation in 2022 shared many similarities to the one from the previous year (UMF standard and matching on biographic data), there was one crucial difference. Remarkably, in this presentation, the software solution was suggested as a possibility for the Common Identity Repository and Multiple Identity Repository. The first is a future database containing links for an identity to identity data in other systems using the UMF. The latter is a new component that should detect duplicate identities in the CIR. These two presentations show the striking interpretative flexibility of the software package. -->

<!-- TODO: Space for a paragraph to further explain the second example -->

<!-- TODO: Space for a paragraph to summarize the findings section -->

## Conclusion

This chapter began with a discussion of the research methods used to study sociotechnologies of identification and whether researchers need to be more self-aware and explicit about the effects that their methodological decisions will have on the findings. Researchers can employ at least three sampling methods to address the scale of sociotechnologies of identification. First, transversal sampling methods can seek to comprehend broader phenomena by contrasting how technologies are utilized in a limited number of locations or by tracing the same standards in selected settings. Second, perpendicular sampling strategies can seek to explain phenomena by considering all of the human and non-human participants. Third, multi-temporal sampling like those proposed in this chapter can examine various points in a technology's lifecycle. Two heuristics for choosing such points were identified based on the literature on long-term and historical approaches. The first heuristic suggests investigating shifts in the interpretative flexibility of artefacts to spot pivotal transitions in the identification procedures, practices, and technologies. The second heuristic suggests investigating gateway moments in which different systems and communities of practice are linked together into larger infrastructures using gateway-like technologies.

A multi-temporal sampling approach in the field of IT in border and migration value has broadened the analytical focus and made it possible to ask new questions. Based on findings from fieldwork conducted at the supplier of a software package for matching people's identity data, the chapter identified moments of interpretative flexibility and moments where the software acts as a gateway technology. By tracing the interpretive flexibility of the software, it is possible to see how a private company can become enrolled in the logic of security. Furthermore, other shifts in the problems and solutions of identification became visible, such as the shift to the "multi-modal" use of biographic and biometrics data and the introduction of concepts like "multi-cultural name matching". Together, these moments of interpretative flexibility give insight into the work of otherwise rarely featured actors, such as software suppliers and their role in procuring or deploying systems.

The two gateway instances, in which the software aids in tying MS to more extensive EU data infrastructures, show how gateway-like technologies fill in gaps in the systems and practices. ELISE's biographic matching and multi-cultural name matching features provide a common standard to help sidestep problems with identification, thereby acting as a gateway bridging heterogeneous data systems and practices. However, we saw the tension between standardization and generification in this gateway-like technology. As such, we stumbled upon the development of the Universal Messaging Format, a generic gateway technology providing a single standard to which any number of countries and agencies can map their data model concepts. Concerning the research question, the findings provide evidence for how technologies and practices for matching identity data travel and circulate across organizations. It became clear that software vendors such as WCC disseminate and repurpose the software packages, potentially transporting identification expertise. The software features can, in turn, influence identification practices in various domains and settings. Gateway technologies play a critical but underappreciated role in establishing and maintaining networks within larger-scale infrastructures.

The current study is one of the first to investigate an identification system's evolution over time. Although this study is limited in scope by its focus on the history of a single software application, it has some far-reaching implications for future research on identification related to border control and migration management. In particular, the study draws attention to the contributions of less obvious actors, who merit greater attention. The network of international actors that develop, finance, and profit from identification in border security and migration management has a long history and lasting effects.

<!-- TODO: Paragraph for cecognising the limitations of the current study and making recommendations for further research work -->
