# Data matching across organizations and agencies {#ch-dm-across-org}

\chaptermark{Data matching across organizations}

__Abstract__

\noindent

Systems and infrastructures for identifying and registering mobile populations have many facets and long development histories. As such, researchers' partial perspectives will thus shape their understanding of the technologies and practices involved. Furthermore, researchers usually conduct their studies at multiple sites or include human and non-human actors that shape identification encounters to study phenomena with so many moving parts. This paper proposes multi-temporal sampling to understand the long-term development of identification systems and infrastructures. Two heuristics are proposed for selecting such moments in the lifecycle of identification technologies. The first heuristic employs SCOT's concept of "interpretative flexibility" of artefacts to identify significant moments when the meanings of identification practices and technologies are challenged, changed, or closed down. The second heuristic employs the concept of "gateways" from infrastructure studies to highlight moments when software systems and infrastructures intersect. This article makes two contributions to the research agenda of long-term perspectives on identification based on data gathered through fieldwork at an IT vendor of software for matching people's identity data. First, by tracing the interpretive flexibility of the software, it is possible to see how this private company became enrolled in security logics. Second, gateway moments make it possible to see the compromises necessary when adapting globally honed technologies to new settings. For example, when the software was integrated with an EU system, infrastructural compromises were made to allow backward compatibility with MS systems. Together, these findings shed light on the activities of under-the-radar actors, such as software vendors, whose distribution and reuse of software packages have long-term implications on identification practices and infrastructures in various contexts.

---

\vspace*{\fill}
\noindent
<!-- _Possibly insert citation here._ -->
\newpage

## Introduction

What we know about technologies for identifying people is inextricably linked to what we know about the devices and practises involved. [^paraphrase] Every year, for example, authorities identify millions of people crossing the EU's external border, whether migrants to and from the EU, tourists, or asylum seekers. Authorities often link and cross-check personal data from various databases to assist in identifying people. For example, to ensure that people are whom they say they are when applying for a visa, to detect identity theft by linking identities across national and international police systems, or to match flight passengers' personal information against government watch lists. By facilitating or hindering particular forms of mobility and excluding others, these processes contribute to the fragmentation of mobilities [@sparkeNeoliberalNexusEconomy2006]. However, what do we know about identification systems, and how are the technologies known to us?

[^paraphrase]: In this sentence, I paraphrase a more general insight from @pollockSoftwareOrganisationsBiography2009: "What we know about technologies, their innovation processes and outcomes is closely bound up with how we know them." (p. 51)

Researchers can bring to light a variety of moments in the lifecycle of identification technologies, providing unique perspectives on the technologies and their associated practices. Our focus could shift from earlier stages in developing sociotechnical identification systems to present-day system operation and maintenance. For example, at airports, we may see the coordination needed between border agents and equipment to identify travellers using automated border systems and their biometric passports [example: @lisleManyLivesBorder2019]. A shift in emphasis to the back-end infrastructure supporting border management could highlight the mundane tasks of maintaining and operating large information systems, as well as the challenges of dealing with incomplete or inaccurate person identification [see, for example, @bellanovaControllingSchengenInformation2020].

Or, one could follow actors and examine documents to piece together how systems are built, such as for the formulation of broad policies like the European Union's "smart borders" initiative [@bigoJusticeHomeAffairs2012; @jeandesbozSmarteningBorderSecurity2016] to the fine-tuning of specific applications like the European Union's Visa Information System [@glouftsiosDesigningDigitalBorders2019]. Selecting one or more of these moments in the lifecycle of a sociotechnology of identification (e.g., planning, analysis, design, procurement, implementation, operation, and maintenance) and ignoring others will inevitably result in partial descriptions [@pollockEInfrastructuresHowWe2010]. However, chronicling a software package's entire lifecycle would be neither feasible nor desirable. A more practical approach would be to develop analytical criteria for identifying key moments in the many interactions of various actors in the co-construction of sociotechnical problems and the identification of solutions.

Rather than being isolated examples, those moments can reveal connected and contingent aspects of the lifecycle of sociotechnologies of identification. For example, EU Member State fingerprinting software is designed to meet US standards [@pelizzaProcessingAlterityEnacting2019]. However, the way that those EU and US actors mediate the identification encounter may be missed by a research design that concentrates on using biometric identification practices to make people legible. In contrast, studies that begin with standards and technical design guidelines risk missing the subtleties of tailoring technologies to particular sites. It is impossible to account for all possible moments and actors, so researchers must make informed decisions based on their familiarity with the subject and research goals [@pollockEInfrastructuresHowWe2010].

The choices researchers make can have real consequences since it is widely accepted that research methods play an active role in bringing about the phenomena they set out to describe and discover [@latourVisualisationCognitionDrawing1986; @lawEnactingSocial2004]. Methods, in this view, can be thought of as devices that bring bits and pieces of the world together to enact certain realities. As a result, researchers must answer the question of what kind of "ontological politics" (sic.) they participate in and what kinds of realities they contribute to. There are compelling reasons, for example, to use migrants' practises and experiences as a starting point for understanding the forms of discrimination and unpredictability embedded in border technologies. However, this focus can cause one to miss other phenomena, such as the rise to power of a small oligopoly of technology companies and IT consultancy firms in the development of technologies for identifying people [@lemberg-pedersenPoliticalEconomyEntry2020; @jonesFundsFortressEurope2022].

By concentrating on how sociotechnologies of identification move and circulate across organizations, influencing identification problems and solutions, this chapter hopes to contribute to the latter type of analysis.  The chapter retraces the development of a software package used for matching identity data by drawing on concepts and theories from long-term and genealogical analyses of data infrastructures and social constructivist accounts of technology. Consequently, the chapter aims to answer research question 4:

> How do technologies and practices for matching identity data travel and circulate across organizations?

Before providing some heuristics for identifying key moments, this chapter assesses methods well suited to following the circulation of identification practices and technologies. In the next section, we begin by classifying the various research strands that have investigated sociotechnologies of identification according to the types of sampling methods used. First, much ethnographic research has been influenced by debates about expanding the number of research sites and connecting observations to gain insight into more encompassing phenomena, such as the construction of a border identification regime that criminalizes migration. Second, scholars have begun considering the complex web of human and non-human actors that make up border identification encounters. However, neither of these approaches can show how practices and technologies have changed over time. Thus, there is a need to provide detailed accounts that include the multiplicity of sites and actors and the multiplicity of moments in time [@hyysaloMethodMattersSocial2019]. What needs to be clarified are the criteria for selecting moments in the lifecycle of a sociotechnology of identification.

This chapter argues that heuristics are required to identify key moments in the evolution of identification-related sociotechnologies. Long-term and genealogical studies of information systems and infrastructures have convincingly demonstrated that such temporal approaches make it possible to avoid a teleological view of the design of technologies [@edwardsIntroductionAgendaInfrastructure2009; @karastiInfrastructureTimeLongterm2010; @ribesLongNowInfrastructure2009; @williamsMovingSingleSite2012]. Instead, temporal approaches allow the inclusion of often overlooked actors and moments in the development of identification sociotechnologies, such as the numerous interactions between government actors and technology consultants as they work together to develop the problems and solutions for identification. What I will refer to as "multi-temporal sampling" can shed light on the emergence of border and migration identification infrastructure.

## Sampling methods for dealing with the scale of sociotechnologies of identification

Ongoing scholarly debates in Science and Technology Studies (STS) about the intertwining of methods and outcomes of investigations may have implications for border identification technology research. In general, the discussion has drawn attention to a mismatch between research methods and the understanding that technologies are shaped over time, in various contexts, and by various actors [@hyysaloMethodMattersSocial2019; @silvastTheorymethodsPackagesScience2021]. This literature argues that many investigations fail to account for technologies' multiple and contingent lives due to inadequate research designs. This assessment is significant, especially when combined with the realization that methods do not merely describe the world but also play a role in enacting specific realities [@lawEnactingSocial2004].

What we know about sociotechnologies of identification is primarily based on empirical studies that investigate encounters between people and technologies or based on desk research and document analysis. This section demonstrates how methodological choices portray sociotechnologies for identifying people to produce specific research findings. Particular attention will be paid to the growing body of literature exploring the information systems that regulate the entry of TCNs into EU territory. There are two main reasons why these systems were selected. First, there is much research on these EU systems, which are among the largest identification systems in the world. Second, the software I looked into, and will expand on later, is directly connected to one of the systems (the Visa Information System). This chapter section reviews the literature on sociotechnologies of identification and highlights some of the thematic and methodological similarities and differences that affect the reviewed research findings.

This section first examines how research has dealt with the large-scale nature of identification systems. In other words, how research has dealt with the many people, places, and things involved in making, deploying and using these systems. As we will see, it is possible to discern three different sampling methods to address these scale issues. First, researchers can multiply the number of research sites to account for the dispersed nature of the studied phenomenon. Second, researchers can increase the number of actors in their analysis. Third, and most importantly for this chapter, different moments in a technology's lifecycle can be compared and analyzed. Much of the current literature on identification sociotechnologies focuses on the first and second approaches for dealing with scale issues. Section 3 of this chapter will cover the theoretical foundation and empirical value of using a multi-temporal sampling approach to broaden the analytical reach of IT for border and migration control.

### Transverse sampling, or situating and tracing connections across sites

It is common to conceptualize information systems and database technologies that store data about mobile populations as part of larger structures, regimes, systems, infrastructures, and assemblages that bring together border and migration-related practices, rules, and meanings. Researchers are then confronted with the methodological conundrum of localizing and investigating these more comprehensive phenomena. One approach is to multiply the research sites to provide multiple accounts of the connections between sites and unravel distributed phenomena. Such approaches are inspired by multi-sited ethnography [@marcusEthnographyWorldSystem1995]. This theoretical framework improved upon the limitations of earlier ethnographic methods, which relied on researchers physically "being there," or observing and interacting with a specific group of people or community in a bounded field site for an extended period. While these accounts may be rich in empirical data, focusing on particular fieldwork locations was deemed insufficient to comprehend globally overlapping phenomena. When ethnographers trace links between sites, they do more than just put one site in a broader global context. In contrast, an ethnographer can decide to focus, for example, on consumption processes within a capitalist political economy by following connections between sites. If we imagine this sampling strategy as a line that crosses and extends across several research sites, we can refer to it as _transverse sampling_.

For instance, the "Mig@Net-Transnational Digital Networks, Migration and Gender" project [@tsianosTransnationalMigrationEmergence2010] used a multi-sited ethnographic approach to investigate European border policies and practices by bringing together data from sites and contexts across Europe. Triangulating observations from various actors (migrants, policy experts) and locations (Greece, Germany, and Italy) allowed the team of researchers to show how, for instance, classifications of asylum seekers do not only follow a coherent system of governance. In principle, the Eurodac system categorizes asylum seekers into one of three categories based on whether they (1) regularly apply for international protection, (2) are discovered illegally crossing the border, or (3) are discovered illegally present within a Member State. However, based on interviews with officials conducted in 2011, the researchers discovered that there are differences in how these categories are applied and understood on a national and institutional level. A German contact for the Mig@Net project, for example, asked the researcher why Greece primarily employed the second rather than the third category. The German practitioner's apparent ignorance runs counter to the Eurodac system's purported role in the development of a uniform EU asylum policy. Additionally, by emphasizing the inconsistency and unpredictability of border and migration control, this case highlights how approaches that sample and aggregate observations from multiple sites can provide counter-evidence to the view of coherent migration and border control practices and policies.

However, researchers should avoid a priori dichotomizing into groupings such as local sites and macro phenomena. A sense of global phenomena can instead emerge from tracing paths between heterogeneous actors. The problem may arise when researchers begin with a theoretical construct — e.g., the existence of an EU border regime that criminalizes migration — and investigate these phenomena by triangulating results from different sites. Scholarly work, especially that grounded in actor-network theory, has cast doubt on these methods' capacity to demonstrate how scale is accomplished in practice. A central tenet of actor-network theory is that "scale is the actor’s own achievement" [@latourReassemblingSocialIntroduction2005, p.185] and that theoretical divisions between micro and macro should, hence, be dropped. In this way, one valuable methodological insight is to "localiz[e] the global" [@latourReassemblingSocialIntroduction2005, p.173] by following the "connections leading from one local interaction to the other places, times, and agencies through which a local site is made to do something" (173). In other words, researchers can allow any notion of providing actors and locations structure to come from following connections rather than assuming specific orderings.

Following the multiple circulating standards and categories is one effective way of tracking down these links [@latourReassemblingSocialIntroduction2005]. For example, @pelizzaIdentificationTranslationArt2021 has documented how regulations for adopting FBI biometric identification standards in equipment used at the Greek border "create trans-national associations with the EU Commission, corporate contractors and the US security regime" (p. 16). Another example is provided by @donkoMigrationControlLocal2022, who have demonstrated how European migration management technology for identification stretches beyond the external EU borders. The authors describe how border checkpoints between Burkina Faso and Niger are linked to EU agencies such as the European Border and Coast Guard Agency (Frontex) via IOM border management information systems that record people's biographic and biometric data. Furthermore, the EU-funded West Africa Police Information System connects these border checkpoints to all INTERPOL member nations via the global police communication system. In order to see the formation of such novel and distinct "flat 'networky' topographies" [@latourReassemblingSocialIntroduction2005, p. 242] of interactions between players, it is crucial to avoid dichotomies of scale.

So, if we imagine this sampling strategy as a line that crosses and extends across several analytically valuable research sites, we can refer to it as transverse sampling for studying large-scale data infrastructures. Far-reaching relationships might appear by exploring the lines between local encounters and other places. Tracing these connections highlights the different interconnected actors involved in large-scale data infrastructures.

### Perpendicular sampling, or incorporating ecologies of interlinked actors

The challenge of who should in included in research designs runs parallel to the need for solutions to scale up the number of research locations to examine large-scale infrastructures. Scholars have debated the field's proclivity to investigate socially, politically, and geographically marginalized individuals. Contrary to the abundance of research on marginalized individuals, less research has been conducted to "study up" on the wealthy and powerful [@naderAnthropologistPerspectivesGained1972; @gustersonStudyingRevisited1997]. Similarly, many published studies on sociotechnologies of identification focus on the experiences of marginalized persons at the border and the related power imbalances. For instance, a growing body of literature [e.g., @kloppenburgSecuringIdentitiesBiometric2020a; @kusterHowLiquefyBody2016; @olwigBiometricBorderWorld2019] has investigated the contentious processes of collecting migrants' biometric data in border zones. Following the studying up/down analogy, researchers interested in ethnography have tended to concentrate on the perspectives of people who are identified and controlled at the border and less on those responsible for creating and maintaining these large-scale identification infrastructures.

Alternatively, scholars can arrive at more balanced portrayals of the numerous actors involved by concentrating on moments and places where diverse actors interlink and impact one another. Examples in the literature show that researchers can thus include a more diverse set of actors: from interviews with local administrators and officers of international organizations of migration centres [@pollozekInfrastructuringEuropeanMigration2019] to interviews with officials and experts from European and national institutions [@glouftsiosGoverningCirculationTechnology2018; @trauttmansdorffInfrastructuralExperimentationCollective2021], and professionals in the security domain at industrial fairs [@bairdKnowledgePracticeMultisited2017]. Studies influenced by STS and ANT highlight the significance of non-human actors alongside these human ones [e.g., @pelizzaIdentificationTranslationArt2021; @pollozekInfrastructuringEuropeanMigration2019]. Suppose we visualize these approaches as highlighting the multitude of actors whose paths cross at sites. In that case, we can refer to this strategy of increasing the number of actors in studies as _perpendicular sampling_. [^vertical-slice]

[^vertical-slice]: @naderVerticalSliceHierarchies1980 proposed the concept of "vertical slice" to, for instance, map the various actors, government agencies, policies, corporations, and associations to understand power and governance of problems are organized [see also @shorePolicyWorldsAnthropology2011]. In using the term perpendicular sampling, I aimed to avoid implying that some vertical hierarchical organization exists.

The decision of whom to include in the study reflects the questions and politics of the researcher. The "Autonomy of Migration" (AoM) approach, for example, begins with migrants' practices of subverting and appropriating mobility regimes and contrasts them with the Fortress Europe discourse [@scheelAutonomyMigrationAppropriating2019]. It has been argued that depicting migration and border controls as fortifications fosters a narrative that ignores the diversity of experiences on borders and migration [@mezzadraBorderMethodMultiplication2013], thus instilling a paternalistic view of migrants as helpless victims in need of protection. Authors in this body of work seek to destabilize such tropes by centring on the experience of migrants and illustrating how migrants can circumvent and subvert restrictive migration and border control mechanisms. One may wonder, however, if the emphasis on migrants' practices and tactics to demonstrate that migration politics are "a site of struggle" [@strangeIrregularMigrationStruggles2017, p. 243] does not also contribute to a negative image of migrants as subversive of a rules-based international order. In addition, restrictive definitions of migrants run the risk of excluding other "privileged migrants" [@bensonMigrationSearchBetter2009], like professionals living abroad, recipients of golden visas, and retirees who migrate. Therefore, the researcher's findings will again be influenced by how they define and prioritize the actors under investigation.

As Laura Nader stated in her 1972 article on studying up, "we aren’t dealing with an either/or proposition; we need simply to realize when it is useful or crucial in terms of the problem to extend the domain of study up, down, or sideways" [@naderAnthropologistPerspectivesGained1972, p. 8]. Since then, the sites and domains of ethnographically inspired research have expanded in many directions. Ethnographers have expanded their work to include tracing the work of scientists in laboratories [for example, @latourLaboratoryLifeConstruction1986; @gustersonNuclearRitesWeapons1996] and policymakers in governance processes [@shorePolicyWorldsAnthropology2011]. Furthermore, these developments coincided with researchers' recognition of other forms of non-human agency. For example, @glouftsiosGoverningBorderSecurity2020 theorized that the agency of EU information systems, as well as the labour of IT and security professionals to maintain those systems, shapes mobility governance throughout the Schengen area. More researchers are now looking into how devices used in security practices affect political agency due to the realization that technological properties shape practices and are intertwined with effects that shape the world [for example, @amicelleQuestioningSecurityDevices2015].

However, the diversity of places and actors involved and the specialized and closed nature of border and security work create barriers for researchers. A report by the "Advancing Alternative Migration Governance" project, for example, describes how the development of EU information systems "has been engineered in specialized and closed forums, such as expert workshops, task forces, technical studies, pilots, or advisory groups and technological platforms steering not just policies, but also the formulation of research and development priorities of funding programmes" [@jeandesbozFinalReportEntry2020, p.10]. Moreover, according to the report's authors, the influence of less visible actors, such as global actors who build, fund, and thus profit from border infrastructure construction, needs to be studied more.

## Multi-temporal sampling, or tracing the genealogies of data infrastructures

The reach of sociotechnologies of identification lies not only in their ability to operate across numerous sites and bring together diverse actors but also in how the technologies accumulate over time and integrate into infrastructures to have long-lasting effects [see also @ribesLongNowInfrastructure2009; @karastiInfrastructureTimeLongterm2010]. It is possible, though, that researchers mistakenly attribute technologies for identifying people they encounter in their investigations to rational processes of designing and implementing systems by system builders with well-defined goals. For instance, "delays, an escalating budget, political crises, and criticisms of the new system's potential impact on fundamental rights"[@parkinDifficultRoadSchengen2011, p. 1] nearly derailed the development of the well-known second-generation Schengen Information System (SIS II) (see also Figure @ref(fig:sisii)). Surprisingly, the "instability of system requirements" [@ecaLessonsEuropeanCommission2014, p. 13], which includes the European Commission's ignorance of how Member States' end-users use the system, was frequently cited as a cause, according to a report by the European Court of Auditors (ECA). Hence, research on the sociotechnologies of identification should correct the idea that designing information systems is a purely rational process. In the information systems literature, it is well known that such systems should be understood as the result of negotiations, adaptations over multiple timescales, and interactions between actors from different organizations [@gassonGenealogicalStudyBoundaryspanning2006; @pollockSoftwareOrganisationsBiography2009].

```{r sisii, echo=FALSE, fig.cap="Chronology of the SISII (ECA 2014).", out.width="80%", fig.asp=.75, fig.align="center"}
knitr::include_graphics("figures/sisii.pdf")
```

Tracing how knowledge and technologies matching identities emerged and circulated thus necessitates uncovering choices and contingencies in the design of information systems over time. Social constructivists have long emphasized that the growth and spread of new technologies do not adhere to any simple linear models [e.g., @pinchSocialConstructionFacts1984; @hughesNetworksPowerElectrification1983]. Instead, various factors influence the trajectory, resulting in distinct sociotechnologies for identification. There are likely many forking paths that could have produced different technological outcomes. Unfortunately, only some guidelines or criteria are found in the literature to help detect such seminal moments in a software's lifecycle. For instance, @hyysaloMethodMattersSocial2019 suggest identifying "moments and sites in which the various focal actors in the ecology interlink and affect each other and the evolving technology." More generally, STS has long pointed to technological breakdowns and controversies as entry points for understanding the workings of technology and the meanings attributed by various actors.

How can we systematically collect observations from various stages in the sociotechnical development of identification software? This chapter proposes two heuristics for detecting pivotal moments in time. First, tracing the moments when the meanings of technologies are challenged, changed, or closed down can help to explain the emergence and establishment of standardized software packages. Second, tracing the moments when technology connects to other systems can help us understand the unfolding of large-scale identification infrastructures.

### Interpretative flexibility and the making of a standard software

Social constructivist accounts of technological innovation provide the theoretical foundation for the first analytical heuristic for identifying pivotal moments in developing standardized identification software packages. Constructivist approaches such as the Social Construction of Technology (SCOT) and Social Shaping of Technology (SST) have criticized linear and deterministic models of innovation and technological development. Instead, these scholarships have shown how technological development is a long-term and open-ended process in which change can be disorderly and protracted.

A fundamental premise of constructivist approaches is that technologies aim to solve difficult-to-solve problems with multiple solutions due to competing demands and requirements. Thus, the adaptability of technological designs shapes and is shaped by the interpretation and interest of specific (groups of) actors. To demonstrate this "interpretative flexibility" of artefacts, the basic template for conducting a SCOT analysis calls for first identifying "relevant social groups" [@pinchSocialConstructionFacts1984]. Since a SCOT analysis is grounded in methodological relativism, relevant social groups are not pre-determined. Instead, social groups are empirically introduced when a group of actors assigns a particular interpretation or meaning to a technological artefact. As such, a SCOT analysis is interested in how different interpretations of social groups give different problems to be solved by technology. The second step is to analyze how interpretative flexibility decreases through the process of "closure", in which the number of alternative solutions narrows and "stabilizes" into one or more artefacts (sic.). It is important to note that the original SCOT approach has some issues due to its ambivalence to structural contexts and that power imbalances can render some actors invisible [@kleinSocialConstructionTechnology2002]. However, analyzing changes in the interpretative flexibility of artefacts can be a valuable heuristic for identifying pivotal moments of change in the lifecycle of identification sociotechnologies.

The shifts in meaning enable us to discover the "biography" of software packages and their standardization by considering tensions between "local" and "global" aspects of software packages, as well as the links between technical and organizational changes. [@pollockSoftwareOrganisationsBiography2009]. For example, one of the recommendations related to the SISII system problems mentioned above was that the Commission "ensure that there is effective global coordination when a project requires the development of different but dependent systems by different stakeholders" [@ecaLessonsEuropeanCommission2014, p. 07]. [^gpmb] Therefore, the report's recommendation for how to deal with the issue of end-user's differing perspectives was to establish a new organizational structure that could align the various actors, from Member States to international contracting firms.

Another excellent example can be found in the work of @soysurenEuropeanInstrumentsDeportation2022, who used a comparative approach to compare the application of the Dublin III regulation (for the Eurodac system) between a founding EU member (France) and an associated country (Switzerland). The researchers found that France took a more sceptical and decentralized approach to use the Dublin system for deportation. On the other hand, Switzerland eagerly adopted the Dublin system and implemented it in a highly centralized manner. In this way, their comparison demonstrates how even a single European instrument can have different meanings and be applied differently. The spatial and temporal reach of sociotechnologies of identification does not imply that these technologies have necessarily stabilized; instead, the systems may still be implemented in various ways depending on the context.

[^gpmb]: The report further notes that for the development of SISII a "Global Project Management Board" was established at a late stage in the project to "to draw more fully on the experience of end-users in member countries" [@ecaLessonsEuropeanCommission2014, p. 37].

### Gateways to infrastructures of identification

Scholars have argued that infrastructures can only "grow" and build up from pre-existing systems, practices, and communities rather than being purposefully constructed. Information infrastructures, in this way, assemble "a combination of standard and custom technology components from different suppliers, selected and adapted to the user’s context and purposes" [@pollockSoftwareOrganisationsBiography2009, p. 286]. Consequently, @starStepsEcologyInfrastructure1996 rethought the baffling concept of infrastructure by asking, "When is an Infrastructure?" One crucial moment is when arrangements are made to connect previously separate systems. This process is referred to as "gateway phases" by @edwardsIntroductionAgendaInfrastructure2009. They metaphorically represent sociotechnical arrangements that "permit multiple systems to be used as if they were a single integrated system" (p. 367) by using the gateway, a hardware element that connects telecommunications networks that use multiple protocols. This chapter proposes operationalising the gateway concept as a second heuristic for identifying moments where software packages and infrastructures intersect. Gateway moments can reveal structural constraints that must be reconciled to connect new components in the emergence of identification infrastructures.[^failure] 

[^failure]: Of course, these junctions between new components and existing infrastructures are also prone to failure [@edwardsIntroductionAgendaInfrastructure2009]. Such failures have, for instance, been well documented in e-government and information systems literature more broadly, where failures have been a long-standing concern due to their high stakes and use of public money [@pelizzaBirthFailureConsequences2018].

In contrast to the many historical technologies studied, information systems are typically (re)designed to be "generic" and "travel" across organizational contexts [@pollockSoftwareOrganisationsBiography2009]. Using the metaphor of a biography, @pollockSoftwareOrganisationsBiography2009 demonstrated how software suppliers must balance requirements as the software matures and accumulates functionalities through its history. For instance, they found that software may be more adjusted to particular user requirements early in the development process. Later, when vendors want to transfer their software to new clients, they must identify overlaps between the sites' needs and the procedures; this is known as "process alignment work" by the authors (p. 174). However, power disparities between the supplier and large/small customers may skew the requirements in particular directions, eventually giving shape to best practices and standards. One weakness of this argument is that it may emphasize interactions between suppliers and customers. Instead, other actors like industry analysts [@pollockHowIndustryAnalysts2016] and government agencies [@trauttmansdorffInfrastructuralExperimentationCollective2021] shaping current and future technological developments should be included in an analysis of these moments where new meanings may emerge.

Scholars have also noted that systems storing personal identity data can easily be used for new and derived purposes aside from their original objectives [@monahanEmergingPoliticsDHS2009]. For example, this type of "function creep" has been referred to for the Eurodac biometric identification system. The system's original purpose was to assist the Dublin system in preventing people from requesting asylum in multiple Member States. However, and as an attestation of the connection between migration and crime control (also known as "crimmigration"), this scope has gradually expanded by allowing police authorities to query the database [@broedersEuropeanBorderSurveillance2011; @amelungCrimmigrationControlBorders2021]. As a result, it is essential to consider how diverse organizations and their systems are interconnected and how this results in the emergence of new and contentious meanings.

The following section uses these theoretical concepts as heuristics to identify points in the lifecycle of a software package for matching people's identity data that can provide insight into the evolution of practices and technologies for identifying people in the context of migration, borders, and security.

## Tracing fields of identification through the evolution of software for matching data

### Methodology

This Section draws on fieldwork conducted at a software package vendor for matching people's identity data in the context of border security and migration management. This chapter, in particular, analyses software deployments in EU and Member State identification systems to examine various stages in developing and using the software package. As a result, the study sheds new light on the diverse set of actors involved in practices of identifying and circulating data about people on the move at the European border [@pelizzaProcessingAlterityEnacting2019]. As detailed in Chapter 3, I joined the company "WCC Group" (WCC) to investigate the design, use, and evolution of a software product dealing with data matching and deduplication. Since I was a temporary member of the ID team, I could visit the company's headquarters in Utrecht (The Netherlands), review all necessary paperwork, conduct one-on-one interviews with relevant company and client personnel, and sit in on some of the team's group meetings.

Building on Chapter 5's introduction to WCC's "ELISE ID platform" (ELISE), this chapter analyzes the software's history of deployments in systems like the Netherlands' Immigration and Naturalization Service system (INDiGO) and the EU Visa Information System's central system (VIS). In Chapter 5, we already saw how the software enhances standard database searches and comparisons with sophisticated matching functionalities. The technology does this through probabilistic algorithms that calculate the degree of identity data similarity. Consequently, the program incorporates complex features for matching identity data, such as dealing with incomplete data or different languages, alphabets, and cultural contexts. Hence, this chapter delves into the history and development of WCC's software and its various features.

We can gain insight into identification in national and international settings by contrasting the EU and MS systems' adoption of the ELISE software. As we saw in Chapter 5, the Netherlands' migration and asylum government agency uses the ELISE software to search for applicants' data in their case management system, INDiGO, and to deal with data quality issues such as duplicate records. In addition, the ELISE software is used by the EU Visa Information System to facilitate alphabetical searches from Member State systems into the central EU database, which is essential for exchanging visa data between EU and Member State systems. For instance, border guards may confirm that a person has a visa, or local authorities may check a visa applicant's data in the context of a Schengen visa application. The analysis of the evolution of the ELISE package in these systems is based on (1) field notes, document analysis on the package for data matching, (2) interviews with vendors' developers, sales, and consultants, and (3) participant observations at industry events.

### Interpretative flexibility 1: Searching and matching data in the dot-com era

The history of the ELISE software package, from its inception to its deployment in the VIS and INDiGO systems, provides a fascinating insight into the development of software and practices used in identifying individuals. The company's origins also bring to light the difficulties in translating and "generifying" the software package so that it can be used in different contexts and by different organizations, paving the way for it to play a notable role in international identification systems. Strangely enough, the history of this data matching software developed by a relatively small IT company provides a window into the complex web of heterogeneous actors linked to international identification systems.

The company's early days reveal a surprising amount of interpretative flexibility regarding what the software should accomplish and for whom it should be helpful. When the company was first conceived in 1996, its founders saw the software primarily as a generic database technology for matching various "things". One of the founders is said to have conceived the initial idea after having unsatisfactory results while looking for a home. In addition, he was aware of other friends who had encountered comparable difficulties in their online job searches. The core issue was that their search results plummeted whenever they used exceedingly specific criteria. Thus, the founders realized there needed to be more advanced search engines that would match users' expectations. There is palpable excitement for the Internet and cutting-edge IT technologies in the company's early stages. There were high expectations at the time for how a technology-driven boom could reorganize various industries and consumer behaviour, such as how people would search for new homes and jobs or plan vacations [@benjaminElectronicMarketsVirtual1995]. As a result, new technologies such as cutting-edge search engines were thought to change or replace the functions of once-essential intermediaries like travel agents and real estate agencies [@wigandWhateverHappenedDisintermediation2020].

The idea and execution for what would become the company's fuzzy searching and matching software were twofold. First, the creators opined that a different search and matching engine was required, which would always produce a result, even when the search criteria were imprecise. To achieve this, the traditional methods of expressing data matching problems, which sought to satisfy Boolean expressions, had to be reworked. For example, a typical search query might read something like this: "SURNAME='Smith' AND NATIONALITY='Canadian. Running this query would return a list of records that meet these criteria. What if you also want to include alternate spellings like "Smit" and "Smithson" in the final result? A boolean expression can only distinguish between true and false, so the company's software would use a different probabilistic data matching method. Hence, search results would be ranked based on the likelihood that two data objects will match. For example, the software would examine the sequence of characters in those name variations of "Smith" and conclude that these strings of characters are X% similar and, thus, possibly the same. In addition, as explained on a page from the company's website in 2007, different search criteria may be given more or less weight when determining this similarity score:

> "In traditional searching, all given criteria must be met exactly in order to be part of the results set. In matching, a search may take into consideration multiple criteria, each with a different level of importance. Some criteria may require an absolute match, where a “no” answer rules out a search result altogether.  Other criteria may be less important, so that 'close enough' results combined with exact matches to the most important criteria bring back a list of extremely strong matches."[^search]

[^search]: https://web.archive.org/web/20070301063433/http://www.wcc-group.com/page.aspx?page=pagecontent&id=4171069

Second, the founders believed advanced search would solve a general issue that various real-world industries and businesses were experiencing. Data matching, in this view, offers a _generic_ solution that a wide range of organizations in various settings and industries can adopt with an improved search that would always return results. In line with this, the following snippet from the company's 2009 website promoted the software as a solution that could address a wide range of search issues:

> "[N]o matter how flawed or incomplete the search criteria, ELISE is always able to return a match. Looking for an email that was sent in October about a team meeting? ELISE will find it, even if you got it wrong and the email was sent in September. Searching for a candidate in a 40 km/mile range? ELISE will find the candidate that best fits the criteria, even if they happen to be 48 km/miles away. Even if you have limited knowledge of what you are actually looking for, just a few simple steps with ELISE will provide all of the information you need."[^results]

[^results]: <https://web.archive.org/web/20090704023143/http://www.wcc-group.com/page.aspx?menu=solutions&page=solutions>

Considering the WCC company's founders as a first social group, we can understand that they interpreted the problems organizations faced at the time as follows. For goods and services to be discoverable by clients and customers in an emerging Internet-based economy, organizations needed to implement sophisticated search mechanisms. Technically, this problem definition led to designing the search and match software as a generic solution that would be applied regardless of domain.

Although customers were quite diverse at the time, we could consider them another social group. As such, it would seem that customers accepted WCC's problem definition and the technical solution. For example, based on interview data and the customers mentioned in old WCC marketing materials, we can understand that WCC had customers in diverse domains. At the time, WCC customers used the software solution to match house seekers with suitable houses, job seekers with relevant jobs, wine lovers with wines that suit their tastes, and tourists with their ideal holiday booking. However, despite the variety of industries and sectors served by WCC, not all amounted to a sizable market. Therefore, as the following interviewee recalls, WCC would gradually reduce the software solution's interpretative flexibility and redefine the problem by focusing on fewer, more commercially successful domains, such as public employment:

> "So, yes, we [WCC Group] were very broad. The first customer, a major customer, was the Dutch employment agency UVV. And that made us think. Because all those other customers were small amounts. And the UVV was a significant customer, and that convinced management at the time that it was a great match. And the main reason for that was—and we are still uniquely ourselves in that regard even compared to the open source competition you see now—the bidirectional matching. So ELISE can not only include your own search criteria, but also what the other party wants. [...] So what the job needs and what the employee is looking for does matter. And that is then matched with each other and that is what ELISE can do very well. So, that’s the reason we entered the labour market. And that has now been completely expanded into much more than just matching wishes with supply, and we are now also solving all kinds of preconditions." (Interview 2021-05-31)

Consequently, WCC started to build domain-specific solutions based on the foundation of the original ELISE data matching software. For instance, WCC presently services public and private employment customers through its "Employment Platform". However, as the previous interview excerpt suggests, not all customers, as relevant social groups, shared the same problem definitions for searching and matching. Public and private employment services are the authorities and organizations that connect job seekers with employers to find the right job for a person or the right person for the job. Hence, these customers must match job seekers' skills, competencies, experiences, and preferences with requirements established for an open position, and vice-versa. This problem definition in the employment services domain was solved using "bidirectional matching". Technically, bidirectional matching uses the same fuzzy matching algorithms to determine compatibility between two sets of data records, in this case, job descriptions and job applicants' preferences.

In contrast, in border security and migration management, searching and matching identity data typically only occurs in one direction. A person's identity record typically has no requirements to match, unlike a job announcement in a database that specifies who should apply. Still, the company could solve both problems using the same data matching engine, despite different problem definitions between the employment services domain and the identity and security domain. Contrary to what one might expect from a SCOT analysis, the company could translate and reconcile such data matching across contexts even with these different problem definitions of the clients as relevant social groups. Because unidirectional search is just a streamlined version of bidirectional matching, there is no technical incompatibility between the two designs. As the software functions without the user being aware of this difference, the software design has reached a point of closure as clients perceive their problems to be resolved. The company now creates dedicated, context-specific platforms that build on the search and match engine to address the unique needs of the two contexts. At the same time, the number of contexts that the searching and matching tool can be used has decreased.

The following section delves deeper into the links between the company's customers in employment services and deployments of the software for identification in security contexts. In particular, examining the history of how WCC joined the consortium that created the EU Visa Information System can shed light on how identity matching has evolved over time.

### Interpretative flexibility 2: Counterterrorism and the transnational professional networks standardizing sociotechnologies of identification

The company's involvement in the security and identity markets has some unexpected connections with WCC's data matching for employment services. In the early 2000s, for instance, WCC worked with the international IT services and consulting firm Accenture to roll out a data matching system for a new platform for the German public employment service [@betlemUtrechtseDatatechnologieMoet2011]. The WCC's then-CEO, Peter Went, discussed this project's success in an interview published in "Database Magazine" in 2006. In the following passage from that interview, Mr Went describes how the two businesses would work together once more to use WCC's search and match software, such as for a US Customs and Border Protection management system: 

> "WCC entered that world [identity matching] through a successful trajectory with Accenture at the Employment Service in Germany. The response was so positive that Accenture decided to hire WCC for a huge project that the consultancy won in 2004 with US Visit, the US border security company. 'They search there, as every traveller to America knows, by face and fingerprint. Ideally suited for our ranking technology, because there are no perfect Boolean-true matches with biometric data.' [Peter Went]" [@rippenSterkePositieHR2006, p. 37, translated from Dutch]

Following the success of the data matching solution for the German employment service, Accenture invited WCC to participate in the contract given to the consulting firm for the United States Visitor and Immigrant Status Indicator Technology (US-VISIT) system. For a long time, the United States has employed this US-VISIT system to track the entry and exit of foreign nationals. True, WCC may have played only a supporting role in providing the data matching technology. However, the participation of WCC in such comprehensive border control information systems exemplifies the growing importance of private actors in developing government biographic and biometric identification systems for controlling mobility [@amooreBiometricBordersGoverning2006]. Overall, WCC executives were keen to participate in large-scale US government systems for identity management and were cognizant of the new opportunities for businesses in the field of security and biometrics in the wake of the September 11 terrorist attacks [@betlemUtrechtseDatatechnologieMoet2011].

The evolution of WCC and its data matching software thus sheds light on the changes to identification systems after the September 11 terrorist attacks. Essentially, increased use of new and latent surveillance technologies, such as biometrics and risk profiling, for controlling borders and governing mobility to prevent terrorism marks this post-9-11 period [@amooreBiometricBordersGoverning2006; @lyonSurveillanceSeptember112003; @lyonSurveillanceSeptember112003]. Post-9/11 measures can be seen in the US-VISIT system, which checks the fingerprints of visa applicants and temporary visitors against national security databases and terrorist watchlists. Tellingly, one news report even mentioned WCC's transition to biometrics and the fact that "it was Accenture that set WCC on the trail of biometric analysis." Thus, it is evident that there was a change in how various social groups interpreted problems of identification, emphasising the use of latent biometric data technologies. Additionally, we can see how industry and government actors united to redefine identification as a combination of biographic and biometric data. In 2009, WCC published a white paper titled "Homeland Security Presidential Directive 24 (HSPD-24)" that exemplifies this kind of parallel problem-solving. This report describes how the ELISE software could be used to comply with HSPD-24, a framework for interagency cooperation and interoperability of biographic and biometric data as part of US counterterrorism efforts and screening processes against terrorism watchlists. The following excerpt from a press release on the whitepaper demonstrates how parties like WCC try to comprehend the US government's problem definitions and position WCC's software as a possible solution:

> "The recently [2009] issued presidential directive mandates a new level of interagency cooperation and interoperability to enhance the terrorist screening process and, at the same time, specifies the use of a layered approach to identification utilizing biometric, biographic, and related contextual data. WCC‘s new white paper explores the ramifications of HSPD-24 and explores its implications for the matching software that supports these processes, with a close look at how WCC‘s ELISE ID supports the layered approach." [@wccHSPD24WhitePaper2009]

This white paper is just one example of exchanges between private companies and public agencies. Executives from WCC also attended events like the annual meetings of the National Defense Industrial Association and international biometric conferences. Using the words of @bairdKnowledgePracticeMultisited2017 and @stockmarrSecurityFairs2015, such "security fairs" are gathering places for government and industry actors to exchange information about security best practices and technologies. According to Baird, talks given at such gatherings can (re)align the issues of current and future policy problems with the solutions offered by the industry. These examples illustrate how governments and international industries co-construct and standardize problems and solutions of identification through transnational networks.

Notable in both the white paper "Meeting the Challenges of HSDP-24: A Layered Approach to Accurate Real-Time Identification" [@wccMeetingChallengesHSDP242009] and the directive "NSPD-59 and HSPD-24 on Biometrics for Identification and Screening to Enhance National Security" [@bushNSPD59HSPD24Biometrics2008] is the emphasis on the use of multiple biometric identifiers. Furthermore, WCC's proposed solution primarily addresses the organizational challenges associated with establishing a common standard for biometric data. Overall, these problem definitions and solutions may serve as good indicators of how identification and screening were problematized more broadly at that time. First, the documents identify the need to standardize and share agencies' biographic and biometric data to screen individuals against "known and suspected terrorists" watchlists. Second, they conclude that matching based solely on either biographic or biometric data will be insufficient and that a "layered approach" [@bushNSPD59HSPD24Biometrics2008] is required, which makes use of multiple methods and data to identify and screen individuals effectively.

The WCC's proposed solution builds on the "multi-modal fusion" concept they developed in previous years. As such, identification trends based on biographic and biometric data led WCC to develop these new features in the ELISE software solution. For example, the company would start to offer a new "vendor neutral and future proof" software architecture that allows new biometric standards to be plugged in "as soon as they are ratified and deployed" [@wccMeetingChallengesHSDP242009, p. 7]. The possibility of new social groups forming and reinstating interpretive flexibility means the closure and reduction of design flexibility are temporary. Accordingly, the issues and solutions of identification were once again open to interpretive flexibility in the post-9/11 era. The United States government, as a social group, re-problematized policy problems of identifying people in the context of security as a problem to be solved with new technical solutions. Hence, biometrics' growing significance and data interoperability between various agencies as technological solutions to identify potential threats. However, as these examples of exchanges between government and business actors have demonstrated, this is not a unidirectional process. Instead, the interactions between public and private actors demonstrate the joint production of these identification issues and their solutions.

WCC's participation in a curious event called "the MITRE challenge" exemplifies the growing popularity and widespread adoption of data matching and the efforts made to stabilize the challenges and potential solutions of data matching based on biographic/alphanumeric data. In 2011, the MITRE Corporation, a non-profit research organization for the US federal government, held an international competition for individuals, companies, and researchers to come up with the best solution for "multicultural name matching" [@millerInternationalMulticulturalName2012]. The idea was sparked by the now well-known "Netflix prize", a competition that the then-rising streaming service held in which participants were invited to submit technical solutions that could outperform Netflix's movie recommendation system based on a set of provided data. Similarly, the MITRE group supplied two data sets with names (given name, surname)  which included name variations from various cultures and languages that would realistically be found in population registers.

The basic idea was to match names from a smaller "query list" with names from a more extensive "index list" (both in Latin script). Therefore, to compete in the MITRE challenge, participants used these data sets to develop technical solutions to find name matches between the query and index lists. Participants then uploaded their results to a special online platform where they could see their position on a public leaderboard. Therefore, the competition could be seen as a closure mechanism to reduce design flexibility and set a benchmark against which data matching solutions can be evaluated.

The creation of these multi-cultural naming data sets uncovers otherwise unseen ideas and concepts that developers use when creating and testing identification technologies, such as how data matching systems conceptualize the relationships between various naming conventions. For instance, the MITRE challenge's data drew on prior work from MITRE researchers [@millerInfrastructureToolsMethodology2008] to develop a taxonomy and data set of person name variants, including "transliteration variation[s], database fielding errors, segmentation differences, incomplete names, titles, initials, abbreviations, nicknames, typos, OCR errors, and truncated data" [@arehartGroundTruthDataset2008, p. 1136]. In order to represent a group of "culturally diverse Romanized person names" the 2008 set of about 70,000 names drew on both public and commercial data sources and tools. The MITRE researchers used publicly available sources like the Death Master File[^dmf], which includes the names of people who had Social Security numbers in the United States, and the "Mémoire des hommes"[^mhd], which includes the names of deceased French soldiers. All data are situated, so it is crucial to inquire about the circumstances surrounding data collection [@dignazioDataFeminism2020]. However, the resulting MITRE data sets obscure these intricate processes that went into collecting and transforming data for testing and developing data matching technologies. <Add name example>

[^dmf]: <https://www.ssab.gov/research/social-security-and-the-death-master-file/>

[^mhd]: <https://www.memoiredeshommes.sga.defense.gouv.fr/fr/arkotheque/navigation_facette/index.php?f=opendata>

The 2008 MITRE data sets were carefully curated to be "large, multicultural, and realistic" [@arehartGroundTruthDataset2008, p. 1136]. Surprisingly, however, none of the papers' authors raises ethical concerns about the data practices of using the names of deceased persons or other problematic choices. For example, the papers devote much attention to examples and peculiarities of Arabic names. Thus, we can learn how Arabic names are technically handled in the research: "With the exception of Arabic names, we used one set of adjudication guidelines that represents a middle-of-the-road view of what should match" [@arehartGroundTruthDataset2008, p.1138]. The adjudication process described here involves human annotators looking through data sets and indicating how they believe the names should match (see Figure @ref(fig:adjudication)).[^adjudication] But a different process was used for Arabic names "due to the relatively larger range of name variations and possible judgments [of abjudicators]" (p. 1138).

As a result, a person with an Arabic name may be profiled as suspicious due to what Ruha Benjamin [-@benjaminRaceTechnologyAbolitionist2019] has called "coded inequity". In this case, the prevalence of scientific practices to deal with the technicalities of Arabic names simultaneously reifies preexisting profiling categories that unevenly target certain people.[^profiling] By participating in the challenge and finding solutions to the name-matching puzzles, actors can adopt the same logic. As we will see later, this may explain why one of ELISE's new features was matching based on Arabic and Asian name transcriptions and transliterations.

[^adjudication]: Unfortunately, not much is known about who these adjudicators might have been.

[^profiling]: See also @moghulUnapologeticRacialProfiling2016, quoted in @dignazioDataFeminism2020.

```{r adjudication, echo=FALSE, fig.cap="An example of the adjudication process from the paper."}
knitr::include_graphics("figures/adjudication.pdf")
```

In the end, WCC participated in the MITRE challenge because they knew that if they won the challenge, they would be invited to present their software solution ELISE to "the three letter agencies in America" (Interview 2021-05-31). These agencies include the Federal Bureau of Investigation (FBI), the Drug Enforcement Agency (DEA) and the Central Intelligence Agency (CIA). WCC did exceptionally well on the test, earning a spot as one of only three "Top Tier Vendors" [@WCCWinsTop2011] and securing the meeting. Although these positive results did not contribute to direct contracts with US government systems, the achievement may have strengthened the Accenture partnership. In 2012, for example, the European Commission chose a consortium of companies, including Accenture, Morpho, and HP, to maintain the EU Visa Information and Biometric Matching Systems, with WCC serving as a subcontractor to provide the search engine for alphanumeric data (see also Figure @ref(fig:floorplan)). Furthermore, WCC remained a subcontractor of Accenture's, continuing to supply the solution for searching and matching biographical and biometric data in the UNHCR's Identity Management System.

```{r floorplan, echo=FALSE, fig.cap="This picture of a biometrics conference's floorplan from 2011 shows how close the VIS consortium members were to each other at the conference."}
knitr::include_graphics("figures/floorplan/floorplan-biometrics-2011-highlight.png")
```

Problem definitions and design solutions for identifying people have evolved, as evidenced by these two subsections on the interpretive flexibility of the ELISE software. WCC initially intended the software to be a domain-agnostic tool for matching data records. Later, the company shifted to focus on serving two distinct data matching markets: the first, which serves public and private employment service providers, and the second, which serves the identity and security industry. By retracing these changes, we gained an understanding of the moments and places where governmental and private actors interact to co-produce and define the issues and solutions related to matching identity data. By tracking changes in interpretative flexibility and mechanisms of closure of design flexibility, it was also possible to understand how actors may end up caught up in the "logic of security" [@balzacqContestingSecurityStrategies2015], such as the use of biometrics and the coded inequalities in name matching. These interpretive ambiguities also demonstrate how actors respond to the ongoing standardization of techniques and technologies for matching identity data, such as the plug-in architecture for biometric standards and the MITRE challenge for best practices for name matching.

### Gateway moment 1: connecting central EU and Member States systems

Let us fast forward in time for a moment to appreciate the scope of the Visa Information System. According to the "Report on the technical function of the Visa Information System (VIS)" [@eu-lisaReportTechnicalFunction2020], the central VIS system received 17 million visa applications in 2019 alone, which thus resulted in the registration of dozens of non-EU citizens' personal information. The report notes 25 million alphanumeric searches on this data, making them an essential form of identification for the VIS. However, these alphanumeric searches can correspond to sometimes ambiguous searches based on 'biographic data,' such as name, date of birth, and nationality, as well as other data, such as passport numbers. Researchers tend to focus on biometric identification, but alphanumeric data are also essential for identification. In addition to the need for flexible searching on alphanumeric data, the report explains two other concerns of end-users of large-scale identification systems: system availability and response times. [^response-time]. These alphanumeric search queries for the VIS are carried out using the ELISE search and match program.

[^response-time]: Indicative of the search volume is the emphasis placed on the median response time for queries (less than 0.8 seconds). The report adds that this response time excludes retrieval of a visa application by its number or visa-sticker number, which has a marginally quicker response time. Additionally, the amount of data stored keeps growing, and by the middle of 2018, the ELISE engine's license had been upgraded to accommodate these increases [@eu-lisaReportTechnicalFunction2020].

The initial iterations of the Visa Information System were gradually introduced throughout various parts of the world. In addition, increased system usage necessitated the launch of the "VIS Evolutions" project to expand the system's capabilities. This project's objectives included changing "the search engine to improve its performance." Technical Functioning Report 2013, p. 8] The then newly established European Agency for the Operational Management of Large Scale IT Systems in the Area of Freedom, Security, and Justice (eu-LISA) was in charge of overseeing the development of a new VIS system through the consortium. A "completely new VIS system in terms of infrastructure, software versions, and search engine" [@eu-lisaVISReportPursuant2016, p. 8] was provided in 2014. As a result, WCC ELISE was provided as a component to search and match for searches based on alphanumeric data. These details of the VIS's history are frequently disregarded. However, understanding how this system has grown and been increasingly deployed in countries worldwide requires understanding the technical difficulties and solutions.

Therefore, the fact that the VIS Evolutions project improved upon a pre-existing system is significant. The system upgrade would entail a complicated process for Member States connecting to the central VIS system, during which everything would need to operate normally. Moreover, system technical specifications for searching as defined for the original VIS system had to be adhered to in order for the upgrade to be compatible with the existing integrations between the central EU VIS system and the VIS systems of the Member States (i.e., sometimes known as "backwards compatibility"). As a result, the EU-VIS system could not take advantage of ELISE's powerful matching capabilities. Nevertheless, the system essentially improved the processing power and performance of the VIS, which reveals the substantial increases in usage, such as by Member States' consular posts [@eu-lisaVISReportPursuant2016]. When ELISE is viewed as a gateway between the EU-VIS and MS-VIS, it becomes clear that there were path dependencies in developing the new and improved VIS that prevented the addition of complex identity matching.

### Gateway moment 2: INDiGO and the travelling data matching knowledge

The deployment of the ELISE software in the IND systems demonstrates how identification knowledge spreads across organizations. At the IND, a completely new system (INDiGO) was created, enabling much more fine-grained configuration for the IND's context. As an illustration, meeting minutes described the deliberations between WCC and IND in setting the appropriate weights for various search criteria. For instace, to configure how importance of matching on last name in calculating match scores. Yet, what is remarkable is how new identity matching capabilities move between organizations to become integrated into the IND systems with subsequent ELISE software updates.

For instance, one presentation listed new software upgrade features that became available to the IND search. Interestingly, this upgrade described new features that were previously specific to the EU-VIS and new name matching functionalities created during the MITRE challenge. Therefore, in 2013, following the MITRE challenge and the EU-VIS implementation, the IND could use new and improved identity matching features. Upgrades included transcription and transliteration matching for Arabic and Asian names, matching based on _also known as_ information provided by third parties (e.g., "'Ahmed the Tall' should match with 'Sheikh Ahmed Salim Swedan' and vice versa), and even an "EU-VIS specific feature" named "partial" (e.g., "Wij should match Wijfels"). The ELISE software upgrades in the IND's systems, including new name matching capabilities developed in the context of the MITRE challenge, show how name- and identity-matching expertise moves between organizations.

These moments also show tensions between the generification of the identification software package and the development of new customer-specific data matching functionalities [compare with @pollockGenerificationStrategyHow2016]. Over time, the ELISE "core", as it is known at WCC, has grown to include a wide variety of features. This metaphor depicts how domain-specific solutions, such as those for security or public employment services, are "wrapped around" the ELISE core, which serves as a central and generic data matching system component. As a result, ELISE has become a reusable system that "transports" identification expertise across organizations through its identity- and name-matching functionalities. This is accomplished by accumulating knowledge about data matching that can be applied in various settings. Of course, when WCC releases a new ELISE version, customers have the option to upgrade their current software version and gain access to new features.

On the other hand, software vendors face difficulties when customized features for individual customers are difficult or impossible to port over. I came across such an example while discussing with WCC staff the development of a new version of a component to query and resolve duplicates (also known as deduplication) in the IND databases. The company weighed the benefits of a custom-made solution for the IND deduplication tool versus a standardized solution for deduplication that other customers could use in the future. However, it became clear that defining what constitutes a duplicate identity is complex. As @loukissasAllDataAre2019 points out, duplicates can be "key to learning about the heterogeneity of data infrastructures" [@loukissasAllDataAre2019]. Why customers may have duplicates in their databases is thus closely related to their local data practices, knowledge of which is not easily transferable to other organizations.

The integration of ELISE in systems as gateway moments provides insights into the structural constraints that must be reconciled to connect new components. Also, in these points of interaction between software and infrastructures, we can observe the dissemination of matching identity data knowledge across institutions.

### Imagining gateways and interpretative flexibility

The final empirical example demonstrates how moments of interpretative flexibility and gateways are inextricably linked. Such links are especially noticeable when WCC demonstrates its data matching expertise in sales and pre-sales activities. For instance, WCC representatives must envision the challenges and potential solutions for future data matching solutions and anticipate the implications of connecting to existing systems when presenting their expertise at industry conferences.

For instance, in 2019, I attended the eu-LISA annual conference, where WCC was invited to present their solutions in the context of a new interoperability architecture for the EU systems. The conference presentation focused on data matching and interoperability using the POLE model (People, Object, Location, Event). This POLE model is a standard data model used by law enforcement to represent crimes, people, and objects. WCC's familiarity with the POLE standard comes partly from the company's previous work for another consumer. It is worth noting that the POLE has influenced the Universal Messaging Format (UMF), which will be used in the interoperability framework. The takeaway is the importance of having a single data standard for matching purposes before attempting data matching across multiple (legacy) systems. Near the end of the presentation, WCC suggested their searching and matching solution as a potential application for the Single Search Interface, one of the interoperability components that will use the UMF standard to query all EU databases simultaneously.

<!-- Space for a paragraph to further explain the first example -->

Finally, in 2020 WCC once more took part in an eu-LISA organized event, the industry round table. While the presentation in 2022 shared many similarities to the one from the previous year (UMF standard and matching on biographic data), there was one crucial difference. Remarkably, in this presentation, the software solution was suggested as a possibility for the Common Identity Repository and Multiple Identity Repository. The first is a future database containing links for an identity to identity data in other systems using the UMF. The latter is a new component that should detect duplicate identities in the CIR. These two presentations show the striking interpretative flexibility of the software package.

<!-- Space for a paragraph to further explain the second example -->

<!-- Space for a paragraph to summarize the findings section -->

## Conclusion

This chapter began with a discussion of the research methods used to study sociotechnologies of identification and whether researchers need to be more self-aware and explicit about the effects that their methodological decisions will have on the findings. Researchers can employ at least three sampling methods to address the scale of sociotechnologies of identification. First, transversal sampling methods can seek to comprehend broader phenomena by contrasting how technologies are utilized in a limited number of locations or by tracing the same standards in selected settings. Second, perpendicular sampling strategies can seek to explain phenomena by considering all of the human and non-human participants. Third, multi-temporal sampling can examine various points in a technology's lifecycle. Two heuristics for choosing such points were identified based on the literature on long-term and historical approaches. The first heuristic suggests investigating shifts in the interpretative flexibility of artefacts to spot pivotal transitions in the identification procedures, practices, and technologies. The second heuristic suggests investigating gateway moments at which software packages and infrastructures intersect.

A multi-temporal sampling approach in the field of IT in border and migration value can broaden the analytical focus and ask new questions. Based on findings from fieldwork conducted at the supplier of a software package for matching people's identity data, the chapter identified moments of interpretative flexibility and moments where the software acts as a gateway. By tracing the interpretive flexibility of the software, it is possible to see how a private company can become enrolled in the logic of security. Furthermore, these moments of interpretative flexibility give insight into the work of otherwise rarely featured actors, such as software suppliers and their role in procuring or deploying systems. The two gateway moments, where the software connects to EU and MS data infrastructures, demonstrate how systems and identification practices are rarely self-contained. Software vendors disseminate and repurpose the software packages, which in turn influences identification practices in a wide variety of domains and locations.

<!-- Paragraph for cecognising the limitations of the current study and making recommendations for further research work -->
