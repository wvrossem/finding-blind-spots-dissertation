# Data matching across organizations and agencies {#ch-dm-across-org}

\chaptermark{Data matching across organizations}

__Abstract__

\noindent

_Enter abstract here_

---

\vspace*{\fill}
\noindent
_Possibly insert citation here._
\newpage

## Introduction

What we know about technologies for identifying people is strongly related to how we know the devices and practices involved.[^paraphrase] For example, every year authorities identify millions of people crossing the EU external border; people migrating to and from the EU, visiting as tourists, or seeking asylum. To do this identification, authorities link and cross-check personal data from various databases to help identify people. For instance, to verify identities for visa applications, to detect identity fraud by linking identities across national and international policing systems, or to match flight passengers’ personal data against government watch lists. These processes play a role in furcating mobilities — speeding up, slowing down, excluding some forms of mobility [@sparkeNeoliberalNexusEconomy2006]. But, what do we know about the systems that progressively carry more weight in establishing someone’s identity and shaping their mobility, and how do we know the technologies?

[^paraphrase]: In this sentence I paraphrase a more general insight of @pollockSoftwareOrganisationsBiography2009: ‘What we know about technologies, their innovation processes and outcomes is closely bound up with how we know them.’ (p. 51)

Researchers can bring in to focus a number of moments in the lifecycle of technologies of identification thus giving specific perspectives on the technologies and their related practices. Moving backwards in time we might focus on different moments and locales: from operation and maintenance of sociotechnical systems of identification to initial phases of their conception. At airports, for instance, we might observe the choreography required between border agents and devices to identify travellers with automated borders systems using their biometric passports [e.g., @lisleManyLivesBorder2019]. Shifting attention to back office systems supporting border management might foreground everyday activities to maintain and operate large information systems and difficulties of dealing with incomplete or inaccurate to identify persons [e.g., @bellanovaControllingSchengenInformation2020]. Or, the processes of developing systems could be traced by following actors and reviewing proposals: from initial steps policymaking in establishment of policy such as the EU ‘smart borders’ package [@bigoJusticeHomeAffairs2012; @jeandesbozSmarteningBorderSecurity2016] to the final design stages of multi-level systems like the EU Visa Information System [@glouftsiosDesigningDigitalBorders2019]. Selecting between one or more of these moments in the lifecycle of a sociotechnology of identification (e.g., planning, analysis, design, procurement, implementation, operation, maintenance) and disregarding others will unavoidably lead to partial descriptions [@pollockEInfrastructuresHowWe2010]. Yet, chronicling the whole lifecycle of a software would not be feasible, nor desirable. A more sensible approach could be to formulate analytical criteria to identify significant moments such as the interactions of diverse actors in the co-construction of sociotechnical problems and solutions of identification.

Those moments are not self-contained, but rather contain interconnected and contingent aspects of the lifecycle of sociotechnologies of identification. For example, research design focusing on practices of biometric identification might miss how various actors mediate and intertwine with identification, just as EU Member State fingerprinting software are made to comply with US standards [@pelizzaProcessingAlterityEnacting2019]. Alternatively, taking proposals and technical design specifications as a starting point of inquiry might miss the intricacies of adopting technologies to particular locales. Taking into account all moments and actors is of course impossible, so researchers need to weigh and make explicit their choices through an understanding of the field and the research objectives.

The choices researchers make can have real effects since it is by now widely accepted that research methods play an active role in bringing into being phenomena they set out to describe and discover [@lawEnactingSocial2004]. Methods can, according to this view, be thought of as devices that bring together bits and pieces of the world to enact certain realities. The question therefore has become in which ‘ontological politics’ (sic.) researchers are involved in and what realities they help make. There are, for instance, important reasons to make migrants’ practices and experience a starting point to understand forms of discrimination and unpredictability ingrained in sociotechnologies of identification at the border. Yet, such focus also might make one oblivious to other phenomena, such as how a small oligopoly of technology companies and IT consultancy companies have come to dominate the development of technologies for identifying people [@lemberg-pedersenPoliticalEconomyEntry2020; @jonesFundsFortressEurope2022].

With this chapter I aim to complement the latter type of analysis by developing a focus on how sociotechnologies of identification travel and circulate across organizations so shaping the problems and solutions of identification. I draw upon concepts and theories from long-term and genealogical analyses of data infrastructures, as well as social constructivists accounts of technology, to analyse the infrastructures for identifying mobile populations, through the evolution of a software package used for matching identity data. Accordingly, I address the following research question:

> How do technologies and practices for matching identity data travel and circulate across organizations?

To begin, this chapter aims to identify and assess methods suited to trace the circulation of practices and technologies for identification, and proposes some heuristics for identifying important moments. In the following Section, I characterize strands of research that have scrutinized sociotechnologies of identification based on their different sampling methods. First, much of the ethnographically inspired researched has been influenced by debates to multiply the number of sites and link observations to gain insight into wider phenomena, such as the construction of a border identification regime that criminalizes migration. Second, researchers have increasingly started to incorporate the various human and non-human actors that, together, give shape to identification encounters at the border. However, both cases do not necessarily show subtle changes over time in the practices and technologies. There is thus a need to give detailed accounts not only by multiplying the venues or actors, but also by including the multiplicity of moments in time [@hyysaloMethodMattersSocial2019]. What is less clear is what the criteria would be for selecting such moments in the lifecycle of a sociotechnology of identification.

For this reason, I argue that, to answer my research questions, there is a need for heuristic techniques for sampling observations from different moments in time of the lifecycle of sociotechnologies of identification. Long-term and genealogical studies of information systems and infrastructures have convincingly shown that such temporal approaches make it possible to avoid a teleological view of the design of technologies [@edwardsIntroductionAgendaInfrastructure2009; @karastiInfrastructureTimeLongterm2010; @ribesLongNowInfrastructure2009;@williamsMovingSingleSite2012]. Instead, temporal approaches make it possible to include otherwise less visible actors and moments in the lifecycle of sociotechnologies of identification, such as the various moments of interactions in time between technology consultants and government actors in co-constructing the problems and solutions of identification. Such a multi-temporal sampling can serve as a way to gain insight into the emergence of border and migration infrastructures of identification.

## Sampling methods for dealing with the scale of sociotechnologies of identification

Ongoing scholarly debates in the field of Science and Technology Studies (STS) about the intertwining of methods and outcome of investigations may have implications for research conducted on technologies at the border. Broadly speaking, the debate has called attention to a discrepancy between research designs and the knowledge that technologies are shaped, over time, in multiple context and by various actors [@hyysaloMethodMattersSocial2019; @silvastTheorymethodsPackagesScience2021]. On the whole, this scholarship asserts that many inquiries omit these insights on the multiple and contingent life of technologies by limited research designs, and provides recommendations for improving research. This assessment is not insignificant, especially when combined with the insight that methods do not merely describe the world but are play a role in enacting particular realities [@lawEnactingSocial2004].

What we know about sociotechnologies of identification is, to a great extent, based upon empirical studies that investigate encounters between people and technologies, or based on document analyses of the descriptive standards. In this section, I aim to show how such methodological choices portray sociotechnologies for identifying people to produce specific research findings. I will focus, in particular, on a large and growing body of literature investigating the information systems used to govern the entry of third country nationals (TCNs) to the territories of the Member States of the European Union. This choice of systems is warranted for two reasons. First, these EU systems are among the largest identification systems and have been widely studied. Second, the software which I have investigated, and will discuss more later, is directly tied up with one of the systems (the Visa Information System). The review of the related literature on sociotechnologies of identification shows a few thematic and methodological similarities and differences that affects their findings.

In particular, in this section I aim to make an inventory of how research has dealt with the large-scale nature of the sociotechnical systems. That is, how the development, deployment, use of these systems extends over many spaces, actors, and time. As such, I will propose to distinguish three kinds of sampling methods to deal with these aspects of scale. First, researchers can multiply the number of sites of research to account for the distributed character of the phenomena under investigation. Second, a more nuanced view is thought to be possible by increasing the number of actors included in the research. Third, and this constitutes this chapter’s specific contribution, different moments in the lifecycle of a technology can be compared and analyzed. Much of the current literature on sociotechnologies of identification pays particular attention to the first and second approaches to deal with issues of scale. Section 3 of this chapter will thus further detail the theoretical background and empirical usefulness of taking multi-temporal sampling approach to expand the analytical scope of IT for border and migration management.

### Transverse sampling, or situating and tracing connections across sites

Information systems and database technologies storing information about mobile populations are often conceptualized as part of a larger structure, regime, system, infrastructure, assemblage in which unites meanings practice, rules, meanings of borders and migration. Methodologically, researchers are then presented with the puzzle of how to localize and study these wider phenomena. One approach is through multiplying the sites of research to give multiple accounts of the connections between sites and unravel distributed phenomena. Such approaches take cues from what is known as multi-sited ethnography [@marcusEthnographyWorldSystem1995]. This methodological concept revised deficiencies of the earlier methodological practices of ethnographers of ‘being there’, i.e., by observing and participating with a people, a community in a bounded field site for an extended period of time. Although such accounts can be empirically rich, the attention to distinct fieldwork sites was thought to be inadequate to understand globally overlapping phenomena. By tracing connections between sites the ethnographer is also not merely situating a local site within a global context. Instead, the ethnographer’s choices often express political and ethical stances to, for instance, take as object of study processes of consumption within a capitalist political economy. As a visual metaphor, we can think of this sampling method of situating and extending across multiple sites of research as _transverse sampling_.

For example, we can see this multi-sited ethnographic approach in the ‘Mig@Net-Transnational Digital Networks, Migration and Gender’ project which set out to investigate European border policies and practices by combining sites and contexts in Europe [@tsianosTransnationalMigrationEmergence2010]. The team of this two-year transdisciplinary research project were able to triangulate observations from different locales (Greece, Germany, Italy) and from various actors (migrants, policy experts). By combining observations regarding the Eurodac system from different sites they could show how categorizations of asylum seekers are not simply congruent with a harmonious system of governance. Broadly speaking, the Eurodac system assigns one of three categories to asylum seekers based on if the person (1) applies regularly for internation protection, (2) is found crossing the border illegally, or (3) is found illegally present within a Member State. However, based on interviews with officials in 2011, they found that there exists national and institutional disparities in how these categories are used and interpreted. For instance, at the time, one of the Mig@Net project’s interlocutors in Germany expressed confusion to the researcher about why Greece was mainly using the second but not the third category. The lack of knowledge of the practitioner seems counter to the Eurodac system's supposed role in the pursuit of a common EU asylum policy. This example thus shows how methods that sample and combine observations from multiple sites can have a politics of counteracting views of impassable borders and instead highlight inconsistencies and unpredictability of border and migration control.

However, researchers should avoid a priori dichotomizing into groupings such as local sites and macro phenomena. A sense of global phenomena can instead emerge from tracing paths between heterogeneous actors. The problem may arise when researchers start with a theoretical construct — e.g., the existence of an EU border regime that criminalizes migration — and investigate this phenomena by triangulating results from different sites. Scholarship such as actor-network theory has questioned the ability of such approaches to show how scale is practically achieved. Actor-network theory convincingly argues that theoretical distinctions between micro and macro should be avoided and posit that ‘scale is the actor’s own achievement’ [@latourReassemblingSocialIntroduction2005, p. 185]. A methodological insight is therefore to ‘localiz[e] the global’ [@latourReassemblingSocialIntroduction2005, p.173] by tracing the ’connections leading from one local interaction to the other places, times, and agencies through which a local site is made to do something’ (173). In short, researchers can avoid presuming a specific ordering and let any sense of giving structure to actors and places emerge from following connections.

One useful method to track down these connections is to follow the many circulating standards and classifications [@latourReassemblingSocialIntroduction2005]. For instance, @pelizzaIdentificationTranslationArt2021 has traced how requirements for incorporating FBI standards for biometric identification in devices used at the Hellenic border ‘create trans-national associations with the EU Commission, corporate contractors and the US security regime’ (p. 16). In another case, @donkoMigrationControlLocal2022 have shown how European migration management technologies for identification moves beyond the external borders of the EU. Through an ethnographic account, the authors explain how border posts at the border between Burkina Faso and Niger are connected to EU agencies — such as European Border and Coast Guard Agency (Frontex) — through border management information systems of the IOM that capture people’s biographic and biometric data. Furthermore, the EU-funded West Africa Police Information System connects these border posts to all member countries of INTERPOL global policing communication system. Avoiding dichotomies of scale therefore makes it possible to observe the emergence of new and different ‘flat “networky” topographies’ [@latourReassemblingSocialIntroduction2005, p. 242] of relations between actors.

Therefore, what I describe as a transverse sampling method for investigating large-scale information systems or infrastructures can, on one hand, involve selecting and comparing from a larger number of analytically useful sites to trace wider phenomena. Alternatively, far-reaching connections can emerge from following along the paths between local interactions and other places. Tracing these connections furthermore highlight the importance of tracing the various interlinked actors involved in large-scale infrastructures.

### Perpendicular sampling, or incorporating ecologies of interlinked actors

Next to responses to account for the problems of scale in studying large-scale infrastructures multiplying the sites of research, there is also the question of whom to include in research designs. This question has been debated by ethnographers who have questioned the field’s tendency to study people who are socially, politically, and/or geographically excluded. This abundance of studies on people at the margin of society was compared to the paucity of work on ‘studying up’ on the wealthy and powerful [@naderAnthropologistPerspectivesGained1972; @gustersonStudyingRevisited1997]. In the case of research on sociotechnologies of identification, there is a large number of published studies that focus on marginalized people’s experiences at the border and the inherent power imbalances. For example, a large and growing body of literature has investigated the contentious processes of capturing migrant's biometric data at border zones [e.g., @kloppenburgSecuringIdentitiesBiometric2020a; @kusterHowLiquefyBody2016; @olwigBiometricBorderWorld2019]. Following the studying up/down analogy, ethnographical inspired researchers have been predisposed to focus more on the experiences of those subjected to identification and controls at the border and less on those responsible for building and operating the large-scale infrastructures of identification.

As an alternative, researchers can arrive at more balanced accounts of different actors involved by focusing on moments and sites in which various actors interlink and affect each other. Examples in literature show that researchers can thus include a more diverse set of actors: from interviews with local administrators and officers of international organizations of migration centres [@pollozekInfrastructuringEuropeanMigration2019], to interviews with officials and experts from European and national institutions [@glouftsiosGoverningCirculationTechnology2018; @trauttmansdorffInfrastructuralExperimentationCollective2021], and professionals in the security domain at industrial fairs [@bairdKnowledgePracticeMultisited2017a]. Besides these various human actors, studies drawing inspiration from STS and ANT also stress the role of non-humans [@pelizzaIdentificationTranslationArt2021; @pollozekInfrastructuringEuropeanMigration2019]. I will refer to this sampling method of multiplying the amount of actors in studies as _perpendicular sampling_ (Figure \@ref()) as another metaphor and heuristic to understand how methods can show the multitude of actors that intersect at sites.[^vertical-slice]

[^vertical-slice]: @naderVerticalSliceHierarchies1980 proposed the concept of ‘vertical slice’ to, for instance, map the various actors such government agencies, policies, corporations, associations to understand power and governance of problems is organized [see also @shorePolicyWorldsAnthropology2011]. In my use of the term perpendicular sampling I aimed to avoid implying that there exists some vertical hierarchical organization.

The choice of whom to include in research also entails careful alteration of research questions and politics. The ‘Autonomy of Migration’ (AoM) approach, for instance, takes migrants’ practices of subverting and appropriating mobility regimes as a starting point and in opposition to the Fortress Europe discourse [@scheelAutonomyMigrationAppropriating2019]. This image of migration and border controls as fortifications has been criticized as favouring a narrative that excludes the multiplicity of experiences of borders and migration [@mezzadraBorderMethodMultiplication2013], and favour a paternalistic image towards migrants as powerless and in need of aid. By focusing on migrants' experience, authors in this scholarship aim to destabilize these tropes by showing migrants’ capacities to subvert and get round restrictive mechanisms of migration and border control.  At the same time, I wonder if this focus on migrant’s practices and tactics to show how the politics of migration are ‘a site of struggle’ [@strangeIrregularMigrationStruggles2017, p. 243] does not also inadvertently contribute to a negative figure of migrants as subversive of a rules-based international order. Furthermore, narrow definitions of migrants risk ignoring other ‘privileged migrants’ [@bensonMigrationSearchBetter2009] such as professional expatriates, golden visa schemes, retirement migration. The various choices researchers make to define and give prominence to the actors investigated through the research will thus again shape their findings.

As Laura Nader in her 1972 article on studying up already remarked: ‘we aren't dealing with an either/or proposition; we need simply to realize when it is useful or crucial in terms of the problem to extend the domain of study up, down, or sideways’ [@naderAnthropologistPerspectivesGained1972, p. 8]. Since then, the sites and domains of ethnographic inspired investigations has expanded in many directions. Ethnographers have expanded into tracing the work of scientists in laboratories [e.g., @latourLaboratoryLifeConstruction1986; @gustersonNuclearRitesWeapons1996] to policymakers' work in processes of governance [@shorePolicyWorldsAnthropology2011]. Furthermore, these expansions coincided with researchers also acknowledging other forms of non-human agency. @glouftsiosGoverningBorderSecurity2020, for instance, has theorized that the agency of EU information systems and the labour of IT and security professionals to maintain those systems shapes the governing of mobility throughout the Schengen area. The acknowledgement that properties of technologies give form to practices and are entwined in world making effects has thus prompted more researchers to examine how devices in security practices also shapes political agency [e.g., @amicelleQuestioningSecurityDevices2015].

However, the multiplicity of places and actors involved, as well as the specialized and closed nature of the work in borders and security does unfortunately create barriers for researchers. For instance, a report by the ‘Advancing Alternative Migration Governance’ project explains how the development of the EU information systems ‘has been engineered in specialized and closed forums, such as expert workshops, task forces, technical studies, pilots, or advisory groups and technological platforms steering not just policies, but also the formulation of research and development priorities of funding programmes’ [@jeandesbozFinalReportEntry2020, p.10]. As the authors of the report remark, the influence of less visible actors such as global actors that build, fund, and hence profit from the construction of border infrastructures has remained under researched.

## Multi-temporal sampling, or tracing the genealogies of data infrastructures

The reach of sociotechnologies of identification is not only in their capacity to operate at many sites and assemble large sets of heterogeneous actors, but also how the technologies’ build up over time and become part of infrastructures to have long-running effects [see also @ribesLongNowInfrastructure2009; @karastiInfrastructureTimeLongterm2010]. However, there is a risk that when researchers encounter technologies for identifying people in their investigations, they assume that these technologies are the outcome of a rational processes of designing and implementing systems by system builders with clear-cut objectives. For instance, the development of the well-known second generation Schengen Information System (SIS II) actually ran into substantial difficulties and almost failed (Figure \@ref(fig:sisii)) due to ‘delays, an escalating budget, political crises and criticisms of the new system's potential impact on fundamental rights’[@parkinDifficultRoadSchengen2011, p. 1]. According to report by the European Court of Auditors (ECA), a frequently mentioned reason was the ‘instability of system requirements’ [@ecaLessonsEuropeanCommission2014, p. 13], such as a lack of knowledge of the European Commission of how Member States’ end-users actually use the system. Instead of a rational process then, scholars have started to correct those rationalistic views and have brought to the fore the contingent aspects of information systems’ design that include negotiations, adaptations over multiple timescales and between various cross-organizational actors[@gassonGenealogicalStudyBoundaryspanning2006; @pollockSoftwareOrganisationsBiography2009].

```{r sisii, echo=FALSE, fig.cap="Chronology of the SISII (ECA 2014).", out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("figures/sisii.pdf")
```

To trace how knowledge and technologies matching identities emerged and circulate therefore requires unearthing choices and contingencies in information systems’ designs in time. The importance of understanding the trajectories of technological innovations has also been essential to social constructivists who stress that the development and diffusion of new technologies do not follow any simplistic linear models [e.g., @pinchSocialConstructionFacts1984; @hughesNetworksPowerElectrification1983]. Many factors will have influenced the trajectory that leads to distinct sociotechnologies for identification, with many forking paths that could have led to different technological outcomes. To date, only a limited number of guidelines or criteria have been identified to detect relevant points in the lifecycle of a software. @hyysaloMethodMattersSocial2019, for instance, only recommend, rather self-evidently, identifying the ‘moments and sites in which the various focal actors in the ecology interlink and affect each other and the evolving technology’. More generally, STS has long pointed at moments of breakdowns of a technology and controversies as entry points to understand the workings of technology and the meanings given by various actors.

There is therefore a need to systematize methods that sample observations from different moments in time of the lifecycle are therefore necessary to answer the research questions. As heuristics to detect important points in time, I will propose to look at two different kinds of moments. First, tracing moments where the meanings of the technologies are challenged, changed, and closed down can make it possible to trace the emergence and establishing of standardized software packages. Second, tracing moments when a technology connects to other systems can make it possible to understand the development of wider infrastructures.

### Interpretative flexibility and the making of a standard software

The first analytical heuristic for detecting important moments in the emergence and stabilization to standardized software packages for identification draws from concepts developed by social constructivist accounts of technological innovation. Approaches such as the Social Construction of Technology (SCOT) and Social Shaping of Technology (SST) have criticized linear and deterministic models of innovation and technological development. Instead, these scholarships have emphasized how the development of technologies is a long-term and open-ended where the process of change can be disorderly and protracted.

A basic insight from such approaches is that technologies aim to solve some hard to resolve problems which has multiple solutions due to conflicting demands and requirements. The flexibility of designs thus constitutes the interpretation and interest of particular (groups of) actors. For instance, the basic template to conduct a SCOT analysis is to first identify ‘relevant social groups’ to show the ‘interpretative flexibility’ of artefacts [@pinchSocialConstructionFacts1984]. Based on the methodological relativism of this approach, the relevant social groups are identified empirically if they attribute some interpretation or meaning to a technological artefact. Essentially, this approach is interested in how different interpretations give different problems to be solved by the technology. The second step in this approach is then how such interpretative flexibility diminishes through the process of ‘closure’ in which the number of alternative solutions narrows down and ‘stabilizes’ into one or more artefacts (sic.). The original SCOT approach has some problems arising from its ambivalence to structural contexts and power imbalances can make invisible some actors [@kleinSocialConstructionTechnology2002]. However, analysing moments of changes in the interpretative flexibility of artefacts can serve as a useful heuristic to discover important moments of change in the lifecycle of sociotechnologies of identification.

The changes in meaning make it possible to trace the ‘biography’ of software packages and their standardization by highlighting tensions between ‘local’ and ‘global’ aspects of a software packages and the linkages between technical and organizational changes [@pollockSoftwareOrganisationsBiography2009]. For instance, one of the recommendations related to the problems with the SISII system mentioned above was the Commission should ‘ensure that there is effective global coordination when a project requires the development of different but dependent systems by different stakeholders’ [@ecaLessonsEuropeanCommission2014, p. 07].[^gpmb] Hence, the recommendation from the report to the problem of diverging views of end-users is to create new organizational structure to bring into line the diverse actors, from member countries to global contracting companies. Another example can be found in the work of @soysurenEuropeanInstrumentsDeportation2022, who took a comparative approach to compare the implementation of the Dublin III regulation (for the Eurodac system) between a founding EU member (France) and an associated country (Switzerland). They found that France took a more sceptical and decentralized approach in the use of the Dublin system for deporting people, whereas Switzerland eagerly took up the system and deployed in a highly centralized manner. In this way, their comparison shows how even a single European instrument can have different meanings and be put differently into practice. The spatial and temporal reach of sociotechnologies of identification does not mean that these technologies have necessarily stabilized, but rather the systems may still be enacted differently across contexts.

[^gpmb]: The report further notes that for the development of SISII a ‘Global Project Management Board’ was established at a late stage in the project to ‘to draw more fully on the experience of end-users in member countries’ (p. 37).

### Gateways to infrastructures of identification

Scholars have argued that infrastructures cannot be deliberately built according to some pre-determined plan, but ‘grow’ and build up from already established systems, practices, communities [@edwardsUnderstandingInfrastructureDynamics2007; @edwardsIntroductionAgendaInfrastructure2009]. Information infrastructures, in this way, assemble ’a combination of standard and custom technology components from different suppliers, selected and adapted to the user’s context and purposes’ [@pollockSoftwareOrganisationsBiography2009, p. 286]. Accordingly, @starStepsEcologyInfrastructure1996 reframed the perplexing of what an infrastructure is and asked: ‘When is an Infrastructure?’ One crucial moment is when arrangements are made to connect previously separate systems. @edwardsIntroductionAgendaInfrastructure2009 have dubbed this process as ‘gateway phases’. In this way they use the gateway, a hardware component to connect telecommunications networks that use multiple protocols, as a metaphor for sociotechnical arrangements that ‘permit multiple systems to be used as if they were a single integrated system’ (p. 367). I thus propose to operationalize this concept of the gateway as a second heuristic that can help identify moments where software packages and infrastructures meet. Gateway moments can give insights into the constraints of structural arrangements that will need to be reconciled to connect new components.[^failure]

[^failure]: Of course, these junctions between of new components and existing infrastructures are also moments that are prone to failure[@edwardsIntroductionAgendaInfrastructure2009]. Such failures have, for instance, been well documented in e-government and information systems literature more broadly, where failures have been long-standing concern due its high stakes and use of public money [@pelizzaBirthFailureConsequences2018].

These junctions between of new components and existing infrastructures allows including also moments when new meanings arise. In contrast to the many analyses of historical technologies, information systems are usually specifically (re)designed to be ‘generic’ and ‘travel’ across organizational contexts [@pollockSoftwareOrganisationsBiography2009]. Having said that, @pollockSoftwareOrganisationsBiography2009 have shown, using the metaphor of a biography, how software suppliers need to balance requirements as the software matures and accumulates functionalities. They identified that in the initial stages of development a software might be more attuned to specific user requirements. Later, when suppliers want to move their software to new customers the suppliers would need to find correspondences between sites‘ requirements and process, what the authors called ‘process alignment work’ (p. 174). Yet, power imbalances in the interactions between the supplier and large/small customers may tilt the requirements in specific directions to eventually give shape to conceptions of best practices. A weakness with this argument, however, is that it may put too much emphasis on interactions between suppliers and customers. Alternatively, an analysis of these moments where new meanings may arise should also include other key actors such as industry analysts [@pollockHowIndustryAnalysts2016] and government agencies[@trauttmansdorffInfrastructuralExperimentationCollective2021] shaping the current and future technological developments.

In the case of systems storing personal identity data, scholars have also noted that such systems can easily be used for new and derived uses apart from their original objectives [@monahanEmergingPoliticsDHS2009]. This kind of ‘function creep’ has, for instance, been referred to for the Eurodac biometric identification system. The original purpose of the system was to support the Dublin system for preventing people from making asylum requests in multiple Member States. This scope has gradually been increased by allowing police authorities to query the database, and put forward as an attestation of the linking of migration and crime control (sometimes termed ‘crimmigration’) [@broedersEuropeanBorderSurveillance2011; @amelungCrimmigrationControlBorders2021]. One question that needs to be asked, therefore, is how new and contested meanings are brought about by interconnecting diverse organizations and their systems.

In the following section I will use this conceptual schema and heuristics to identify moments in the lifecycle of a software package for matching people's identity data that can give insight into the evolution of the practices and technologies for identifying people in the context of migration, borders, and security.

## Tracing field of identification through the evolution of software package for matching data

### Methodology

In this Section, I will draw on findings from fieldwork I conducted at the supplier of a software package for matching people’s identity data in the context of migration, borders, and security. More specifically, I traced different episodes in the development and deployments of the software package, and compared deployments at EU and Member State systems. As such, the research was thought to provide insights into the heterogeneous set of actors involved in practices of identifying and circulating data about people on the move at the European border [@pelizzaProcessingAlterityEnacting2019]. Practically, I joined the company named ‘WCC Group’ (WCC) to investigate the design, use, and evolution of a software product dealing with data matching and data deduplication. As a temporary member of the team, I visited the company’s headquarters in Utrecht (The Netherlands), had access to relevant documentation, carried out individual interviews with people from the company and the customers, and attended some of the team's joint meetings.

My main focus is on the evolution of WCC’s software package/system called the ‘ELISE ID platform’ (ELISE), deployed at the immigration and naturalization service of the Netherlands (Immigratie- en Naturalisatiedienst, hereafter IND) and the central system of the EU Visa Information System (VIS). Briefly stated, the software provides for additional advanced searching and matching of information on existing databases. The novelty of the technology comes from using various kinds of fuzzy logic algorithms to search structured and unstructured data — taking into account that this data may be incomplete or inaccurate. Here, fuzzy logic refers to a form of logic in mathematics where truth variables are calculated in probabilities instead of only being ‘true’ and ‘false’. Consequently, search results are returned as values indicating the probability that two sets of identity data can be considered identical — i.e., match. In the case of matching identity, the functionalities of the software make it possible to handle difficulties of matching between personal data such as from different locales, scripts, cultural contexts, and so forth.

Comparing the inital deployment and longer-term use of this ELISE software in the IND and VIS systems was thought to give insights into national and transnational problematics of identification. The IND is responsible for, among others, processing the applications from people who want to stay in The Netherlands or who want to become Dutch nationals. The government agency uses the ELISE software to facilitate searching for applicants’ data against the alphanumeric information in their case management system called INDiGO, and to deal with data quality issues such as duplicate records. The VIS allows visa data exchange (including personal data and biometrics) to support a common EU visa policy. The ELISE supports the alphanumeric searches for VIS from Member State system to a central EU database. For instance, authorities may check a visa applicant’s data in the context of Schengen visa application or border guards may verify that a person holds a visa. For analysis the evolution of the ELISE package in these systems, I draw on data which include the (1) field notes, document analysis on the package for data matching, (2) interviews with developers, sales, consultants at the supplier, and (3) participant observations at industry events.

### Interpretative flexibility 1: Searching and matching data in the dot-com era

The genealogy of the ELISE software package through deployments in the VIS and INDIGO systems gives remarkable insight into the changing field of software and practices for identifying people. These origins also highlight the tensions in the interpretative flexibility of the software and the translation and generification work needed to transport the software package across contexts and organizations, to eventually play an important role in transnational infrastructures of identification. Peculiarly enough, the story of a modest and relatively small Dutch data matching company’s software allows peeking into the web of heterogeneous actors interconnected with global infrastructure of identification.

The early days of the company certainly show a surprising amount of interpretative flexibility of what the software should accomplish and for whom it should be useful. When the company was founded in 1996, the founders thought of the software primarily as a generic database for matching different ‘things’. The founding legend told to me was that one of the founders came up with the idea after getting unsatisfactory results while looking for a house. He apparently also knew of other friends having similar issues while searching for a job. Fundamentally, the problem they experienced was that when they specified too strict criteria for a search, the number of results plummeted. In my view, these early stages of the company resonate with the enthusiasm for the Internet and new IT technologies. At the time, there were great hopes that a technology-inspired boom could reorganize how, for instance, customers would be able to search for their own new homes or plan their own holidays in ‘electronic markets’ [@benjaminElectronicMarketsVirtual1995]. As a result, the roles of previously important intermediaries such as travel agents and real estate agencies would change as their functions could, to a certain extent, be replaced by new technologies [@wigandWhateverHappenedDisintermediation2020], such as state-of-the-art search engines.

The idea and execution for what would become the fuzzy searching and matching proposed by the company was twofold. First, the founders hypothesized that there was a need for a searching and matching engine that would always return a result, even with imprecise search criteria. This ability was realized primarily by reworking the conventional ways of expressing the data matching problem which aimed to satisfy Boolean propositions. Typically, one might formulate a search query to search on rather exact criteria, such as ‘SURNAME=“Smith” AND NATIONALITY=“Canadian”’. It follows that the result of running this query would be list of records for which these criteria are true. So what if you would also like to see variations such as ‘Smit’ or ‘Smithson‘ in the list of results? In the company’s software, data matching would therefore rely on fuzzy logic to a find and rank results based on the probability that two data objects would match. If a boolean expression evaluates to two possible values, either 1 (true) or 0 (false). Similarly, a probability can be expressed more finely-grained as a value in a range between 0 and 1 (i.e., a percentage). For instance, the software would look at the sequence of characters in those name variations of ‘Smith’ and conclude that these strings of characters are X per cent similar and therefore possibly the same. Furthermore, as a page on the company’s 2007 website describes, the different search criteria may be more or less important in calculating this similarity score:

> ‘In traditional searching, all given criteria must be met exactly in order to be part of the results set. In matching, a search may take into consideration multiple criteria, each with a different level of importance. Some criteria may require an absolute match, where a “no” answer rules out a search result altogether.  Other criteria may be less important, so that “close enough” results combined with exact matches to the most important criteria bring back a list of extremely strong matches.’[^search]

[^search]: https://web.archive.org/web/20070301063433/http://www.wcc-group.com/page.aspx?page=pagecontent&id=4171069

Second, the founders anticipated that there would be a need for such software in many more domains besides the founder’s trouble of finding a house. According to this rationale, data matching provides a generic solution that can be put to use by many organizations operating in various contexts and domains. Correspondingly, the following quote from the company’s 2009 website advertised the software as a software solution which could solve varied search problems:

> ‘[N]o matter how flawed or incomplete the search criteria, ELISE is always able to return a match. Looking for an email that was sent in October about a team meeting? ELISE will find it, even if you got it wrong and the email was sent in September. Searching for a candidate in a 40 km/mile range? ELISE will find the candidate that best fits the criteria, even if they happen to be 48 km/miles away. Even if you have limited knowledge of what you are actually looking for, just a few simple steps with ELISE will provide all of the information you need.’ [^results]

[^results]: <https://web.archive.org/web/20090704023143/http://www.wcc-group.com/page.aspx?menu=solutions&page=solutions>

From the company’s perspective, we can understand that they understood the problem companies faced as follows. In a then emerging Internet-based economy, companies would need to provide advanced search to make their products and services accessible to end-users. They defined this search as a generic problem to which the solution, i.e., new data matching technology, could also thus be applied independently of the domain.

From the customers’ perspective this problem definition and solution seems to have, to some extent, been taken on. For instance, based on what was told to me and the customers mentioned in old promotion materials, we can see that WCC, indeed, had customers in a broad range of domains: from matching people with houses, people with jobs, wine enthusiasts to wines matching their tastes, people with their ideal holiday booking. Later, as the following interviewee recalls, there was a gradual closing down of this design flexibility to focus on a few specific domains such as public employment:

> ‘So, yes, we [WCC Group] were very broad. The first customer, a major customer, was the Dutch employment agency UVV. And that made us think. Because all those other customers were small amounts. And the UVV was a significant customer, and that convinced management at the time that it was a great match. And the main reason for that was—and we are still uniquely ourselves in that regard even compared to the open source competition you see now—the bidirectional matching. So ELISE can not only include your own search criteria, but also what the other party wants. [...] So what the job needs and what the employee is looking for does matter. And that is then matched with each other and that is what ELISE can do very well. So, that’s the reason we entered the labour market. And that has now been completely expanded into much more than just matching wishes with supply, and we are now also solving all kinds of preconditions.’ (Interview 2021-05-31)

By now, public and private employment services account for the majority of business for WCC via an ‘Employment Platform’ based on the original ELISE data matching software. Within the different contexts, the customers as relevant social groups did, however, not have all share the same problem definitions for the searching and matching. In the context of employment services, the clients required a simultaneous, two-way matching of a jobseeker’s skills, competencies, experiences, and preferences with criteria specified for an available position. This problem definition in the employment services domain was technically solved through ‘bidirectional matching’. Using the above-mentioned fuzzy matching technologies, the bidirectional matching can calculate the degree of similarity between criteria specified for the jobseeker and all job descriptions available in the database.

Searching and matching identity data for security contexts, in contrast, is generally only in one direction. That is, compared to a job announcement in a database that specifies who should apply, a person’s identity record generally has no demanded requirements to match. Yet, it was possible for the company to solve these two problem definitions using the same data matching engine even though some meanings and interpretations in the public employment services differed from those in the identity and security domain. Contrary to what one might expect in an SCOT analysis, even with these different problem definitions of the clients as relevant social groups, it was possible for the company to translate and reconcile such data matching across contexts. That being so, it could be argued there is a kind of closure of the design of the software as clients see their respective problems as solved. At the same time, the numbers of contexts that the searching and matching tool has been reduced, and, although out of scope for our discussion here, the company now develops dedicated, context-specific platforms that build on the searching and matching engine to deal with particular needs of the two main contexts.

Next, we can trace the connections between the company’s customers in employment services to deployments of the software for identification in the context of security. In particular, tracing the connections of how WCC became a subcontractor in the consortium that developed the EU Visa Information System makes it possible to unravel changes in the practices and technologies for matching identity data.

### Interpretative flexibility 2: Counterterrorism and the transnational professional networks standardizing sociotechnologies of identification

There are, in fact, links between these domains of data matching for employment services and the involvement of WCC in the security and identity markets. In the early 2000s, WCC collaborated with the multinational information technology services and consulting company Accenture to deploy their data matching system for a new platform for the German public employment service [@betlemUtrechtseDatatechnologieMoet2011]. In an 2006 interview with Peter Went in ‘Database Magazine’, the WCC CEO at the time, mentioned that this collaboration was considered successful and led to the deployment of the software other projects and further developments of the software to match biometric data:

> ‘WCC entered that world [identity matching] through a successful trajectory with Accenture at the Employment Service in Germany. The response was so positive that Accenture decided to hire WCC for a huge project that the consultancy won in 2004 with US Visit, the US border security company. “They search there, as every traveller to America knows, by face and fingerprint. Ideally suited for our ranking technology, because there are no perfect Boolean-true matches with biometric data.” [Peter Went]’ [@rippenSterkePositieHR2006, p. 37, own translation from Dutch]

Therefore, based on the positive experience of the fast bidirectional matching for employment, Accenture invited WCC to get involved in the contract awarded to the consultancy company for the United States Visitor and Immigrant Status Indicator Technology (US-VISIT) system. The US-VISIT program is a well-known system used to identify non-US citizens and record their arrival and departure to the United States. The role of WCC in this project might be considered only minimal (providing only the data matching technology). Yet, the involvement of WCC in such a large-scale information systems for border control can be seen as an example of the changing role of private actors in the establishment of new government biographic and biometric identification systems for the governing of mobilities [@amooreBiometricBordersGoverning2006]. Overall, WCC executives showed a keen interest to get involved in large-scale US government systems for identity management and were well aware of the new opportunities for firms in field of security and biometrics after the September 11 terrorist attacks [@betlemUtrechtseDatatechnologieMoet2011].

These developments of WCC and the ELISE software give insights into the evolutions of identification systems in the post-9/11 period. Essentially, this period is marked by an expansion of new and latent surveillance technologies such as biometrics and risk profiling for controlling borders and governing mobilities in the name of preventing terrorism [@amooreBiometricBordersGoverning2006; @lyonSurveillanceSeptember112003]. On one hand, the problem definitions of identification started to turn more to latent biometric data technologies. One news article even mentioned that ‘it was Accenture that set WCC on the trail of biometric analysis’. During such times of change we can start to see how industry and government actors jointly started to redefine the identification by combining biographic and biometric data. Such concurrent problem-solving can also be seen in a 2009 white paper published by WCC on the topic of ‘Homeland Security Presidential Directive 24 (HSPD-24)’. This report details how the ELISE software could be used to comply with HSPD-24, a framework for interagency cooperation and interoperability of biographic and biometric data as part of United States counterterrorism efforts and screening processes against terrorism watchlists. The following quote from a press release on the whitepaper shows how actors such as WCC attempt to understand the US governments problem definitions and how WCC‘s software could work as a solution:

> ‘The recently [2009] issued presidential directive mandates a new level of interagency cooperation and interoperability to enhance the terrorist screening process and, at the same time, specifies the use of a layered approach to identification utilizing biometric, biographic, and related contextual data. WCC‘s new white paper explores the ramifications of HSPD-24 and explores its implications for the matching software that supports these processes, with a close look at how WCC‘s ELISE ID supports the layered approach.’ [@wccHSPD24WhitePaper2009]

The whitepaper is just one example of the kinds of interactions between industry and governments. WCC executives were furthermore present at various events such as the yearly international biometric conferences and the annual meetings of the National Defence Industrial Association. Such ‘security fairs’ are important places where governmental and industry actors to meet and circulate knowledge of security ‘best practices’ and technologies [@bairdKnowledgePracticeMultisited2017]. As Baird (sic.) further remarked, presentations at these events can be useful to (re)align the problems of current and forthcoming policy problems and solutions provided by industry. These examples show how governmental and international industry actors co-construct and standardize problems and solutions of identification through transnational networks.

Based on my reading of the white paper ‘Meeting the challenges of HSDP-24: a layered approach to accurate real time identification’ [@wccMeetingChallengesHSDP242009] and the directive ‘NSPD-59 / HSPD-24 on biometrics for identification and screening to enhance national security’ [@bushNSPD59HSPD24Biometrics2008], the main problematization for which WCC proposes a solution, relate to organizational difficulties in standardizing the use of biometric data. In my view, these problem definitions and solutions can be representative of the wider problematization of identification and screening at that moment in time. First, the documents identify a need to standardize and share biographic and biometric data of various agencies so that they are mutually compatible to screen individuals against ‘known and suspected terrorists‘ watchlists. Second, they conclude that matching only on biographic or biometric data will not be sufficient and that there needs to be a ’layered approach’ [@bushNSPD59HSPD24Biometrics2008] that uses multiple methods and data to effectively identify and screen individuals.

The proposed solution by WCC builds on a concept they had been developing in the years before dubbed ‘multi-modal fusion’. WCC thus started to implement new features in the ELISE software solution that were influenced by these newer trends in identification based on both biographic and biometric data. For instance, by providing a ‘vendor neutral and future proof’ software architecture that would allow new biometric standards to be plugged-in ‘as soon as they are ratified and deployed’ [@wccMeetingChallengesHSDP242009, p. 7]. The post-9/11 period thus reintroduced interpretative flexibility to the problems and solution of identification. The US government, as a relevant social group, put forward policy problems and a redefinition of the problem identifying people in the context of security that needed to be solved technically. These problems included the increased importance of biometrics and data interoperability between different agencies. At the same, as the different interactions between government and industry actors has shown, this is not a one-way process. On the contrary, the interactions showed the co-production of these problems and solutions of identification.

Besides the increased use and standardization of biometric data, an effort to stabilize the problem and solutions of data matching based on biographic/alphanumeric data can be observed when WCC participated in the MITRE challenge. In 2011, the MITRE Corporation, a non-profit research organization for the US federal government, launched a challenge for individuals, companies, and researchers to develop and benchmark the best possible solution for ‘multicultural name matching’ [@millerInternationalMulticulturalName2012]. The whole concept was inspired by the famous ‘Netflix prize’, a competition launched by the then up-and-coming streaming company for anyone to submit solutions to improve the accuracy of their recommendation system based on a data set made available by the company. Similarly, in the MITRE challenge, the MITRE organization provided a data set of names (given name, surname) and variations which could be realistically be found in databases. Participants (academic or government research institutions, commercial companies, individuals) could then compete by using their solution to identify matching names and upload these results on a platform.

The creation of this data set of names is itself revealing of otherwise less visible develoer’s imaginaries and concepts, such as how different naming practices correspond, that are put into practice when developing identification technologies. For instance, the data set included various name variations such as ‘transliteration variation[s], database fielding errors, segmentation differences, incomplete names, titles, initials, abbreviations, nicknames, typos, OCR errors, and truncated data’ [@arehartGroundTruthDataset2008, p. 1136]. This set of approximately 70,000 names drew from both public and commercial data sets and tools to represent a set of ‘culturally diverse Romanized person names’ (sic.). For the publicly available sources, the MITRE researchers, in fact, used lists of names of deceased people such as the Death Master File[^dmf], which contains the names of persons who had Social Security number in the US, and the ‘Mémoire des hommes’[^mhd], which lists the names of French deceased soldiers. [@dignazioDataFeminism2020]. However, in the final data set, these complex histories of how these data sets were collected into data sets for testing and developing (non)commercial software become invisible. Yet, the choices made in creating this data sets shows the imaginaries and concepts used by developers to respond to the problems of  

[^dmf]: <https://www.ssab.gov/research/social-security-and-the-death-master-file/>

[^mhd]: <https://www.memoiredeshommes.sga.defense.gouv.fr/fr/arkotheque/navigation_facette/index.php?f=opendata>

What is remarkable is that in developing the MITRE challenge data set, which was carefully curated to be ‘large, multicultural, and realistic’ [@arehartGroundTruthDataset2008, p. 1136], the authors of that paper don't raise any ethical questions on the data practices of using the names of deceased persons, nor for other problematic choices. For instance, a disproportionate amount of attention the papers give to examples and particularities of Arabic names. In the paper we can read how Arabic names are treated as a technical exception: ‘With the exception of Arabic names, we used one set of adjudication guidelines that represents a middle-of-the-road view of what should match, based on the variation taxonomy presented earlier.’ [@arehartGroundTruthDataset2008, p.1138] Here, the adjudication refers to a process where human annotators go through the data sets and indicate how they think the names should match (see also Figure \@ref(fig:adjudication)).[^adjudication] These technoscientific practices insert ‘coded inequity’ [@benjaminRaceTechnologyAbolitionist2019] and reify profiling categories that result in someone being becoming suspicious simply by having an Arabic name.[^profiling] By competing in this challenge and developing solutions to solve these name matching puzzles, actors can become enrolled into these same logics. Hence, and as we will see again later, one of the new features added to ELISE in this time was the addition of matching based on the transcription and transliteration of Arabic and Asian names.

[^adjudication]: Unfortunately not much is known about who these adjudicators might have been.

[^profiling]: See also @moghulUnapologeticRacialProfiling2016, quoted in @dignazioDataFeminism2020.

```{r adjudication, echo=FALSE, fig.cap="An example of the adjudication process from the paper."}
knitr::include_graphics("figures/adjudication.pdf")
```

Eventually, WCC competed in the challenge with incentive that, if they would be successful in the MITRE challenge, they would be invited to present their software solution ELISE to ‘the three letter agencies in America’ (Interview 2021-05-31): the Federal Bureau of Investigation (FBI), Drug Enforcement Agency (DEA), Central Intelligence Agency (CIA). WCC did in fact score well and achieved one of the three ‘Top Tier Vendors‘ [@WCCWinsTop2011] and had that meeting. Although these favourable results do not seem to have contributed to any direct contracts with US government systems, it might have strengthened the partnership with Accenture. In 2012 the European Commission selected a consortium of companies, Accenture, Morpho, and HP to maintain the EU Visa Information and Biometric Matching Systems which included WCC as a subcontractor to provide the searching engine for alphanumeric data (see also Figure \@ref(fig:floorplan)). After this collaboration, WCC later continued also as a subcontractor with Accenture to provide the solution for searching and matching biographical and biometric data in the UNHCR's Identity Management System.

```{r floorplan, echo=FALSE, fig.cap="This image of a floorplan of a 2011 conference on biometrics highlights the physical proximimity at the conference of the actors of the VIS consortium."}
knitr::include_graphics("figures/floorplan/floorplan-biometrics-2011-highlight.png")
```

In these two Subsections on identifying moments of interpretative flexibility of ELISE it becomes clear that the company has quite repositioned the software over time. From its initial beginnings as a rather domain-independent way to match data records, to its repositioning as primarily a solution for data matching for employment services and for the identity and security markets. In the case of identification, we have glimpsed at the moments and places where governmental, private actors liaise to co-produce and define the problems and solutions of matching identity data. By tracing these developments it is also possible to actually see how actors can become entangled in the ‘logic of security’[@balzacqContestingSecurityStrategies2015] (such as the use of biometrics and the coded inequalities in name matching). At the same time, these moments of interpretative flexibility show how actors deal with the continuing standardization of practices and technologies for matching identity data (such as the plug-in architecture for biometric standards and the MITRE challenge for best practices for name matching).

### Gateway moment 1: connecting central EU and Member States systems

Let’s jump forward in time for a moment to better understand the scale of the Visa Information System. As opposed to the previously mentioned tendency of researchers to focus on biometric identification, identification based on alphanumeric data such as a person’s name, date of birth, or nationality should not be underestimated. From the ‘Report on the technical function of the Visa Information System (VIS)’ [@eu-lisaReportTechnicalFunction2020] we can learn that in 2019, for instance, 17 million visa applications (and hence the data of non-EU citizens) were registered in the central system. Alphanumeric searches on this data are a significant factor of identification for the VIS; the report notes 25 million alphanumeric searches which are thus executed through the ELISE search and match software. These alphanumeric searches can correspond to sometimes ambiguous searches based on ‘biographic data‘ — e.g., name, date of birth, nationality — as well as other data such as passport numbers. Next to the need for flexible searching on alphanumeric data, two other important concerns of end-users of large-scale identification systems can thus be understood from the report: to have high availability of the systems and to have fast response times.[^response-time].

[^response-time]: The high frequency of searches is visible in the importance given to average response time of searches (less than 0.8 seconds). The report further notes that this response time does not include retrieval of a visa application by its number or visa-sticker number, which has a slightly faster response time. Furthermore, the amount of data stored keeps increasing and by mid-2018 the licence of the ELISE engine was upgraded to deal with these increases. [@eu-lisaReportTechnicalFunction2020]

The first versions of the Visa Information System were progressively deployed in diverse regions of the world. As the system gained more use, the capacity of the system needed to increase and a ‘VIS Evolutions’ project was started. One of the goals of this project was ‘to change the search engine to improve its performance.’ [@eu-lisaReportTechnicalFunctioning2013, p.8] A new version of the VIS system was therefore developed through the consortium and under the management of the freshly established European Agency for the Operational Management of Large Scale IT Systems in the Area of Freedom, Security and Justice (eu-LISA). In 2014 a ‘completely new VIS system in terms of infrastructure, software versions and search engine’ [@eu-lisaVISReportPursuant2016, p. 8] was provided, and hence with WCC ELISE as a component to search and match for searches based on alphanumeric data. These aspects of the evolution of the VIS are often overlooked, but the technical difficulties and solutions are actually important to understand how this system has grown and been increasingly deployed in countries all over the world.

It is thus important that this VIS Evolutions project was an upgrade of a previously existing system. This detail meant that this project would entail a complex migration for Member States connecting to the central VIS system where everything would need to keep working as usual. In practice then, the configuration ELISE of the central EU-VIS would not be able to use many of the advanced matching features. The interfacing specifications for searching defined for the initial VIS system would need to be followed in order to not break the integrations between the central and Member State systems. Conceptualizing ELISE a gateway element connecting the EU-VIS and MS-VIS systems shows path dependencies in the development of the upgraded VIS that prohibited the company to add any sophisticated identity matching. Compared to the previous discussion on the search engine, the system mainly provided a remarkable improvement processing power and performance of the VIS, which is revealing of the substantial increases of the usage of the system, such as by Member States’ consular posts [@eu-lisaVISReportPursuant2016].

### Gateway moment 2: INDiGO and the travelling data matching knowledge

The focus on the performance of the search engine in EU-VIS stands in contrast with the deployment of ELISE in the IND system. In this case, an entirely new system (INDiGO) was developed which allowed for much more customization and discussions between the supplier and the customer to set up a suitable implementation. For instance, in meeting minutes I had access to I could see ongoing discussions about what could be the rights weights for different search criteria. Later on, when the ELISE software was updated this meant that newly developed features could be added. For instance, new name matching functionalities developed during the MITRE challenge and features specific to the EU-VIS would show up in the list of new features of the software upgrades. Some of the features available for the IND from 2013 — after the MITRE challenge and the EU-VIS — included: matching based on the transcription and transliteration of Arabic and Asian names, taking into account 3rd party AKA information (e.g., ‘“Ahmed the Tall” should match with “Sheikh Ahmed Salim Swedan” and vice versa), and even a ‘EU-VIS specific feature‘ named ‘partial’: ‘Wij should match Wijfels’. The deployment of software in the systems of the IND shows how identity and name matching knowledge moves across organizations, such as the addition of name matching functionalities following the MITRE challenge.

These moments also show tensions between the generification of the software package of identification and the development of new data matching possibilities specific to certain customers. Over time, these new functionalities were added to the ELISE ‘core’ which, as a re-usable system, could ‘transport’ new knowledge and features about identity data matching across organizations. Of course, customers may still have a choice to upgrade their current version of the package to receive such new features. Alternatively, software suppliers also face issues when some customer-specific features cannot easily be transported. I encountered such an example during a discussion about the development of new version of a component to query and resolve duplicates (also known as deduplication) in the databases of the IND. Essentially, the question that arose was if, for upgrade to the IND deduplication tool, a custom-made solution should be built, or if WCC could develop a more standardized solution for deduplication be deployed at other customers. However, it became clear that the difficulty of defining what can be considered a duplicate identity is not easily. As @loukissasAllDataAre2019 notes, duplicates can be ‘key to learning about the heterogeneity of data infrastructures’ [@loukissasAllDataAre2019]. Why customer might have duplicates in their databases is thus closely linked to their local data practices for which knowledge is not easily transferable to other organizations.

Conceptualizing the integration of ELISE in systems as gateway moments can thus give insights into the constraints of structural arrangements that will need to be reconciled to connect new components. In addition, these moments where the software and infarstructures meet makes it possible to see how knowledge of matching identity data can travel between agencies and organizations.

<!-- ### Imagining gateways and interpretative flexibility -->

<!-- As a last empirical example, I want to emphasize that moments of interpretative flexibility and gateways are of course closely linked together. Such connected moments can be seen when WCC displays their data matching expertise in the sales and pre-sales activities. For instance, when performing their expertise in data matching at industry conferences, WCC representatives needs to imagine the expected problems and solutions for future data matching solutions and anticipate the implications to connect to already existing systems. -->

<!-- For instance, in 2019 I attended the eu-LISA annual conference where WCC was invited to present at their solutions in the context of a new interoperability architecture for the EU systems. The presentation at the conference focus heavily on the use of the POLE model (People, Object, Location, Event) for data matching and interoperability. This POLE model is a standard data model used by police for representing crimes, people and objects involved. The experience of WCC with the POLE standard can actually be traced back to their work with another customer. Important to note is that the POLE has influenced the Univeral Messaging Format which will be used in the interoperability framework. In essence, the lesson is that a prerequisite for data matching across different (legacy) systems is to have one standard to match to. Near the end of the presentation, WCC posited their searching and matching solution as a potential use for the Single Search Interface, one of the interoperability components that will use the UMF standard to query all EU databases at once. -->

<!-- In 2020, WCC once more took part in an eu-LISA organized event, the industry round table. While the presentation in 2022 shared many similarities to the one from the previous year (UMF standard and matching on biographic data), there was one crucial difference. Remarkably, in this presentation the software solution was suggested as a possibility for the Common Identity Repository and Mutltiple Identity Repository. The first is a future database that will contain links for an identity to identity data in other systems and using the UMF. The latter is new component should detect duplicate identities in the CIR. These two presentations show the striking interpretative flexibility of the software package. -->

## Conclusion

In this article I started with the question of what methods researchers have used to investigate sociotechnologies of identification and if researchers need to be more reflective and explicit about the kinds of results their methodological choices will have on the findings. Such research can be understood to make use of at least three kinds of sampling methods to deal especially with the scale of sociotechnologies of identification. First, transversal sampling methods may use attempt to understand wider phenomena by comparing the use of technologies at a select number of places or following standards across locales. Second, perpendicular sampling methods may instead try to account for phenomena through the various human and non-human actors involved. Third, multi-temporal sampling can look at different moments in time of the lifecycle of a technology. Based on the literature on long-term and genealogical approaches, I identified two heuristics to select such moments. The first is to analyse moments of change in the interpretative flexibility of artefacts to discover important moments of change in the practices and technologies of identification. The second heuristic is to analyze gateway moments, i.e., where software packages and infrastructures meet.

I argue that a multi-temporal sampling approach can be useful for the field of IT in border and migration value to expanding the analytical focus and by asking new question. Based on findings from fieldwork I conducted at the supplier of a software package for matching people’s identity data I identified a couple of moments of interpretative flexibility and where the software acts as a kind of gateway. By tracing the interpretive flexibility of the software it is possible to see how a private company can become enrolled in the logics of security. Furthermore, these moments of interpretative flexibility give insight into the work of actors that are otherwise rarely featured, such as software suppliers and their role in processes of procuring or deploying systems. The two gateway moments, where the software becomes connected to EU and MS data infrastructures shows how systems and practices of identification are rarely self-contained. Software suppliers circulate and re-use the software packages, influencing practices of identification across many domains and locales.

<!-- * Reflection on my own research has a deficit of not including the perspective of those affected by the technology. Links to PC project and multi-sited fieldwork -->
<!-- * Highlight implications for research. -->

<!-- * Temporal approach can open up new questions -->

<!-- * Show the control available in shaping technologies. -->
